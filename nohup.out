Initializing navigation with fixed parameters
__________________________________________________
Choose to use gpu...
__________________________________________________
__________________________________________________
||                  Arguments                   ||
__________________________________________________
|| algorithm_name: rmappo                       ||
|| project_name: informarl                      ||
|| experiment_name: informarl                   ||
|| seed: 100                                    ||
|| cuda: True                                   ||
|| cuda_deterministic: True                     ||
|| n_training_threads: 1                        ||
|| n_rollout_threads: 64                        ||
|| n_eval_rollout_threads: 1                    ||
|| n_render_rollout_threads: 1                  ||
|| num_env_steps: 4000000                       ||
|| user_name: marl                              ||
|| use_wandb: True                              ||
|| env_name: GraphMPE                           ||
|| use_obs_instead_of_state: False              ||
|| world_size: 2                                ||
|| num_scripted_agents: 0                       ||
|| obs_type: global                             ||
|| max_edge_dist: 1                             ||
|| num_nbd_entities: 3                          ||
|| use_comm: False                              ||
|| episode_length: 25                           ||
|| share_policy: True                           ||
|| use_centralized_V: True                      ||
|| stacked_frames: 1                            ||
|| use_stacked_frames: False                    ||
|| hidden_size: 64                              ||
|| layer_N: 1                                   ||
|| use_ReLU: False                              ||
|| use_popart: True                             ||
|| use_valuenorm: False                         ||
|| use_feature_normalization: True              ||
|| use_orthogonal: True                         ||
|| gain: 0.01                                   ||
|| split_batch: False                           ||
|| max_batch_size: 32                           ||
|| use_naive_recurrent_policy: False            ||
|| use_recurrent_policy: True                   ||
|| recurrent_N: 1                               ||
|| data_chunk_length: 10                        ||
|| lr: 0.0007                                   ||
|| critic_lr: 0.0007                            ||
|| opti_eps: 1e-05                              ||
|| weight_decay: 0                              ||
|| ppo_epoch: 10                                ||
|| use_clipped_value_loss: True                 ||
|| clip_param: 0.2                              ||
|| num_mini_batch: 1                            ||
|| entropy_coef: 0.01                           ||
|| value_loss_coef: 1                           ||
|| use_max_grad_norm: True                      ||
|| max_grad_norm: 10.0                          ||
|| use_gae: True                                ||
|| gamma: 0.99                                  ||
|| gae_lambda: 0.95                             ||
|| use_proper_time_limits: False                ||
|| use_huber_loss: True                         ||
|| use_value_active_masks: True                 ||
|| use_policy_active_masks: True                ||
|| huber_delta: 10.0                            ||
|| use_linear_lr_decay: False                   ||
|| save_interval: 1                             ||
|| log_interval: 5                              ||
|| use_eval: False                              ||
|| eval_interval: 25                            ||
|| eval_episodes: 32                            ||
|| save_gifs: False                             ||
|| use_render: False                            ||
|| render_episodes: 5                           ||
|| ifi: 0.1                                     ||
|| render_eval: False                           ||
|| model_dir: None                              ||
|| verbose: True                                ||
|| scenario_name: navigation_graph              ||
|| num_landmarks: 3                             ||
|| num_agents: 7                                ||
|| num_obstacles: 3                             ||
|| reward_sparsity: 1                           ||
|| collaborative: True                          ||
|| max_speed: 2                                 ||
|| collision_rew: 5.0                           ||
|| goal_rew: 5                                  ||
|| min_dist_thresh: 0.05                        ||
|| use_dones: False                             ||
|| num_embeddings: 3                            ||
|| embedding_size: 2                            ||
|| embed_hidden_size: 16                        ||
|| embed_layer_N: 1                             ||
|| embed_use_ReLU: True                         ||
|| embed_add_self_loop: False                   ||
|| gnn_hidden_size: 16                          ||
|| gnn_num_heads: 3                             ||
|| gnn_concat_heads: False                      ||
|| gnn_layer_N: 2                               ||
|| gnn_use_ReLU: True                           ||
|| graph_feat_type: global                      ||
|| actor_graph_aggr: node                       ||
|| critic_graph_aggr: global                    ||
|| global_aggr_type: mean                       ||
|| use_cent_obs: False                          ||
|| auto_mini_batch_size: False                  ||
|| target_mini_batch_size: 32                   ||
__________________________________________________
__________________________________________________
Creating wandboard...
__________________________________________________
wandb: Currently logged in as: sharlinu (golde). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240801_171359-0kt6imc6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rmappo_informarl_seed100
wandb: â­ï¸ View project at https://wandb.ai/golde/enemy
wandb: ðŸš€ View run at https://wandb.ai/golde/enemy/runs/0kt6imc6
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
<unknown>:105: DeprecationWarning: invalid escape sequence \i
Overriding Observation dimension
Overriding Observation dimension
________________________________________________________________________________
Actor Network
________________________________________________________________________________
________________________________________________________________________________
GR_Actor(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0-1): 2 x TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((22,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=22, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (act): ACTLayer(
    (action_out): Categorical(
      (linear): Linear(in_features=64, out_features=5, bias=True)
    )
  )
)
________________________________________________________________________________
________________________________________________________________________________
Critic Network
________________________________________________________________________________
________________________________________________________________________________
GR_Critic(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0-1): 2 x TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (v_out): PopArt()
)
________________________________________________________________________________
Traceback (most recent call last):
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/scripts/train_mpe.py", line 318, in <module>
    main(sys.argv[1:])
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/scripts/train_mpe.py", line 303, in main
    runner.run()
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/runner/shared/graph_mpe_runner.py", line 48, in run
    ) = self.collect(step)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/runner/shared/graph_mpe_runner.py", line 149, in collect
    ) = self.trainer.policy.get_actions(
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/graph_MAPPOPolicy.py", line 163, in get_actions
    actions, action_log_probs, rnn_states_actor = self.actor.forward(
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/graph_actor_critic.py", line 170, in forward
    nbd_features = self.gnn_base(node_obs, adj, agent_id)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/utils/gnn.py", line 488, in forward
    x = self.gnn(node_obs, adj, agent_id)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/utils/gnn.py", line 278, in forward
    x = self.active_func(self.gnn1(x, edge_index, edge_attr))
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch_geometric/nn/conv/transformer_conv.py", line 229, in forward
    out = self.propagate(edge_index, query=query, key=key, value=value,
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch_geometric/nn/conv/transformer_conv.py", line 266, in message
    key_j = key_j + edge_attr
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 3.31 MiB is free. Process 8640 has 9.20 GiB memory in use. Including non-PyTorch memory, this process has 388.00 MiB memory in use. Of the allocated memory 67.11 MiB is allocated by PyTorch, and 18.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.005 MB of 0.005 MB uploadedwandb: - 0.005 MB of 0.005 MB uploadedwandb: ðŸš€ View run rmappo_informarl_seed100 at: https://wandb.ai/golde/enemy/runs/0kt6imc6
wandb: ï¸âš¡ View job at https://wandb.ai/golde/enemy/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2MTU4MTI2MA==/version_details/v17
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240801_171359-0kt6imc6/logs
Initializing navigation with fixed parameters
__________________________________________________
Choose to use gpu...
__________________________________________________
__________________________________________________
||                  Arguments                   ||
__________________________________________________
|| algorithm_name: rmappo                       ||
|| project_name: informarl                      ||
|| experiment_name: informarl                   ||
|| seed: 100                                    ||
|| cuda: True                                   ||
|| cuda_deterministic: True                     ||
|| n_training_threads: 1                        ||
|| n_rollout_threads: 64                        ||
|| n_eval_rollout_threads: 1                    ||
|| n_render_rollout_threads: 1                  ||
|| num_env_steps: 4000000                       ||
|| user_name: marl                              ||
|| use_wandb: True                              ||
|| env_name: GraphMPE                           ||
|| use_obs_instead_of_state: False              ||
|| world_size: 2                                ||
|| num_scripted_agents: 0                       ||
|| obs_type: global                             ||
|| max_edge_dist: 1                             ||
|| num_nbd_entities: 3                          ||
|| use_comm: False                              ||
|| episode_length: 25                           ||
|| share_policy: True                           ||
|| use_centralized_V: True                      ||
|| stacked_frames: 1                            ||
|| use_stacked_frames: False                    ||
|| hidden_size: 64                              ||
|| layer_N: 1                                   ||
|| use_ReLU: False                              ||
|| use_popart: True                             ||
|| use_valuenorm: False                         ||
|| use_feature_normalization: True              ||
|| use_orthogonal: True                         ||
|| gain: 0.01                                   ||
|| split_batch: False                           ||
|| max_batch_size: 32                           ||
|| use_naive_recurrent_policy: False            ||
|| use_recurrent_policy: True                   ||
|| recurrent_N: 1                               ||
|| data_chunk_length: 10                        ||
|| lr: 0.0007                                   ||
|| critic_lr: 0.0007                            ||
|| opti_eps: 1e-05                              ||
|| weight_decay: 0                              ||
|| ppo_epoch: 10                                ||
|| use_clipped_value_loss: True                 ||
|| clip_param: 0.2                              ||
|| num_mini_batch: 1                            ||
|| entropy_coef: 0.01                           ||
|| value_loss_coef: 1                           ||
|| use_max_grad_norm: True                      ||
|| max_grad_norm: 10.0                          ||
|| use_gae: True                                ||
|| gamma: 0.99                                  ||
|| gae_lambda: 0.95                             ||
|| use_proper_time_limits: False                ||
|| use_huber_loss: True                         ||
|| use_value_active_masks: True                 ||
|| use_policy_active_masks: True                ||
|| huber_delta: 10.0                            ||
|| use_linear_lr_decay: False                   ||
|| save_interval: 1                             ||
|| log_interval: 5                              ||
|| use_eval: False                              ||
|| eval_interval: 25                            ||
|| eval_episodes: 32                            ||
|| save_gifs: False                             ||
|| use_render: False                            ||
|| render_episodes: 5                           ||
|| ifi: 0.1                                     ||
|| render_eval: False                           ||
|| model_dir: None                              ||
|| verbose: True                                ||
|| scenario_name: navigation_graph              ||
|| num_landmarks: 3                             ||
|| num_agents: 7                                ||
|| num_obstacles: 3                             ||
|| reward_sparsity: 1                           ||
|| collaborative: True                          ||
|| max_speed: 2                                 ||
|| collision_rew: 5.0                           ||
|| goal_rew: 5                                  ||
|| min_dist_thresh: 0.05                        ||
|| use_dones: False                             ||
|| num_embeddings: 3                            ||
|| embedding_size: 2                            ||
|| embed_hidden_size: 16                        ||
|| embed_layer_N: 1                             ||
|| embed_use_ReLU: True                         ||
|| embed_add_self_loop: False                   ||
|| gnn_hidden_size: 16                          ||
|| gnn_num_heads: 3                             ||
|| gnn_concat_heads: False                      ||
|| gnn_layer_N: 2                               ||
|| gnn_use_ReLU: True                           ||
|| graph_feat_type: global                      ||
|| actor_graph_aggr: node                       ||
|| critic_graph_aggr: global                    ||
|| global_aggr_type: mean                       ||
|| use_cent_obs: False                          ||
|| auto_mini_batch_size: False                  ||
|| target_mini_batch_size: 32                   ||
__________________________________________________
__________________________________________________
Creating wandboard...
__________________________________________________
wandb: Currently logged in as: sharlinu (golde). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240801_172358-tyquzltc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rmappo_informarl_seed100
wandb: â­ï¸ View project at https://wandb.ai/golde/enemy
wandb: ðŸš€ View run at https://wandb.ai/golde/enemy/runs/tyquzltc
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
created scenarioReward sparsity of 1

Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
created scenario
Initialising graph navigation
Reward sparsity of 1
<unknown>:105: DeprecationWarning: invalid escape sequence \i
Overriding Observation dimension
Overriding Observation dimension
________________________________________________________________________________
Actor Network
________________________________________________________________________________
________________________________________________________________________________
GR_Actor(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0-1): 2 x TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((22,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=22, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (act): ACTLayer(
    (action_out): Categorical(
      (linear): Linear(in_features=64, out_features=5, bias=True)
    )
  )
)
________________________________________________________________________________
________________________________________________________________________________
Critic Network
________________________________________________________________________________
________________________________________________________________________________
GR_Critic(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0-1): 2 x TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (v_out): PopArt()
)
________________________________________________________________________________
Average episode rewards is -212.743 	Total timesteps: 1600 	 Percentage complete 0.040
/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/wandb/util.py:449: DeprecationWarning: 
Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),
(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)
but was not found to be installed on your system.
If this would cause problems for you,
please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466
        
  import pandas as pd
Average episode rewards is -206.610 	Total timesteps: 9600 	 Percentage complete 0.240
Average episode rewards is -213.650 	Total timesteps: 17600 	 Percentage complete 0.440
Average episode rewards is -209.316 	Total timesteps: 25600 	 Percentage complete 0.640
Average episode rewards is -180.971 	Total timesteps: 33600 	 Percentage complete 0.840
Average episode rewards is -175.655 	Total timesteps: 41600 	 Percentage complete 1.040
Average episode rewards is -160.357 	Total timesteps: 49600 	 Percentage complete 1.240
Average episode rewards is -157.939 	Total timesteps: 57600 	 Percentage complete 1.440
Traceback (most recent call last):
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/scripts/train_mpe.py", line 318, in <module>
    main(sys.argv[1:])
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/scripts/train_mpe.py", line 303, in main
    runner.run()
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/runner/shared/graph_mpe_runner.py", line 76, in run
    train_infos = self.train()
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/runner/shared/base_runner.py", line 168, in train
    train_infos = self.trainer.train(self.buffer)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/graph_mappo.py", line 329, in train
    ) = self.ppo_update(sample, update_actor)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/graph_mappo.py", line 170, in ppo_update
    values, action_log_probs, dist_entropy = self.policy.evaluate_actions(
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/graph_MAPPOPolicy.py", line 267, in evaluate_actions
    values, _ = self.critic.forward(
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/graph_actor_critic.py", line 399, in forward
    nbd_features = self.gnn_base(
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/utils/gnn.py", line 488, in forward
    x = self.gnn(node_obs, adj, agent_id)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/utils/gnn.py", line 282, in forward
    x = self.active_func(self.gnn2[i](x, edge_index, edge_attr))
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch_geometric/nn/conv/transformer_conv.py", line 229, in forward
    out = self.propagate(edge_index, query=query, key=key, value=value,
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py", line 547, in propagate
    out = self.message(**msg_kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch_geometric/nn/conv/transformer_conv.py", line 277, in message
    out = out * alpha.view(-1, self.heads, 1)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 352.00 MiB. GPU 0 has a total capacity of 9.67 GiB of which 60.44 MiB is free. Including non-PyTorch memory, this process has 9.53 GiB memory in use. Of the allocated memory 8.90 GiB is allocated by PyTorch, and 318.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: 
wandb: Run history:
wandb:                actor_grad_norm â–‚â–‚â–â–ƒâ–„â–…â–ˆâ–†
wandb:            agent0/dist_to_goal â–‡â–†â–ˆâ–…â–„â–‚â–â–
wandb:      agent0/individual_rewards â–„â–„â–â–…â–‡â–‡â–ˆâ–ˆ
wandb:        agent0/min_time_to_goal â–ˆâ–â–†â–ƒâ–ƒâ–…â–â–‚
wandb:    agent0/num_agent_collisions â–â–‚â–…â–â–…â–„â–ˆâ–†
wandb: agent0/num_obstacle_collisions â–ˆâ–‚â–â–…â–â–„â–ˆâ–‡
wandb:            agent0/time_to_goal â–ˆâ–ˆâ–†â–„â–ˆâ–…â–â–…
wandb:            agent1/dist_to_goal â–‡â–ˆâ–‡â–‡â–„â–ƒâ–‚â–
wandb:      agent1/individual_rewards â–ƒâ–ƒâ–‚â–â–…â–†â–ˆâ–‡
wandb:        agent1/min_time_to_goal â–†â–ƒâ–‚â–ˆâ–â–†â–…â–‚
wandb:    agent1/num_agent_collisions â–â–„â–ƒâ–ƒâ–ƒâ–ˆâ–†â–„
wandb: agent1/num_obstacle_collisions â–â–„â–ˆâ–‡â–…â–…â–ƒâ–
wandb:            agent1/time_to_goal â–ˆâ–…â–…â–ˆâ–…â–ˆâ–†â–
wandb:            agent2/dist_to_goal â–ˆâ–ˆâ–‡â–†â–…â–ƒâ–ƒâ–
wandb:      agent2/individual_rewards â–â–â–‚â–„â–„â–…â–ˆâ–ˆ
wandb:        agent2/min_time_to_goal â–…â–„â–…â–„â–â–†â–„â–ˆ
wandb:    agent2/num_agent_collisions â–‚â–ƒâ–â–„â–ƒâ–ˆâ–ƒâ–„
wandb: agent2/num_obstacle_collisions â–ƒâ–„â–â–‡â–‚â–ƒâ–ƒâ–ˆ
wandb:            agent2/time_to_goal â–ˆâ–â–„â–ˆâ–‚â–†â–„â–‚
wandb:            agent3/dist_to_goal â–†â–†â–ˆâ–ˆâ–…â–â–‚â–
wandb:      agent3/individual_rewards â–…â–…â–ƒâ–â–…â–‡â–ˆâ–ˆ
wandb:        agent3/min_time_to_goal â–â–„â–â–…â–„â–„â–‡â–ˆ
wandb:    agent3/num_agent_collisions â–â–ƒâ–‚â–ƒâ–„â–‡â–ˆâ–„
wandb: agent3/num_obstacle_collisions â–ƒâ–ˆâ–„â–â–‚â–ˆâ–â–ˆ
wandb:            agent3/time_to_goal â–‡â–†â–†â–ˆâ–‡â–†â–ƒâ–
wandb:            agent4/dist_to_goal â–ˆâ–‡â–ˆâ–†â–„â–‚â–‚â–
wandb:      agent4/individual_rewards â–‚â–‚â–â–‚â–„â–…â–†â–ˆ
wandb:        agent4/min_time_to_goal â–ˆâ–ƒâ–…â–ƒâ–â–†â–‡â–†
wandb:    agent4/num_agent_collisions â–â–ƒâ–‚â–â–‚â–ˆâ–ƒâ–„
wandb: agent4/num_obstacle_collisions â–…â–â–…â–‡â–ˆâ–ˆâ–ƒâ–ˆ
wandb:            agent4/time_to_goal â–ˆâ–ˆâ–‡â–†â–‡â–„â–†â–
wandb:            agent5/dist_to_goal â–‡â–ˆâ–‡â–‡â–„â–ƒâ–‚â–
wandb:      agent5/individual_rewards â–â–â–‚â–â–…â–„â–ˆâ–†
wandb:        agent5/min_time_to_goal â–ˆâ–ƒâ–ƒâ–â–â–ƒâ–†â–„
wandb:    agent5/num_agent_collisions â–â–ƒâ–â–‚â–…â–ˆâ–ƒâ–„
wandb: agent5/num_obstacle_collisions â–†â–‡â–„â–â–‡â–†â–†â–ˆ
wandb:            agent5/time_to_goal â–ˆâ–â–ˆâ–‡â–†â–…â–…â–†
wandb:            agent6/dist_to_goal â–†â–†â–ˆâ–‡â–„â–‚â–â–
wandb:      agent6/individual_rewards â–â–„â–â–‚â–…â–„â–ˆâ–‡
wandb:        agent6/min_time_to_goal â–ƒâ–â–‡â–ˆâ–‡â–„â–„â–ˆ
wandb:    agent6/num_agent_collisions â–„â–„â–‚â–ƒâ–â–ˆâ–„â–ˆ
wandb: agent6/num_obstacle_collisions â–ƒâ–‚â–‚â–â–‚â–„â–‚â–ˆ
wandb:            agent6/time_to_goal â–†â–„â–ˆâ–†â–ˆâ–‚â–â–ƒ
wandb:        average_episode_rewards â–â–‚â–â–‚â–…â–†â–ˆâ–ˆ
wandb:               critic_grad_norm â–ˆâ–â–â–‚â–‚â–â–‚â–‚
wandb:                   dist_entropy â–ˆâ–‡â–‡â–†â–†â–ƒâ–â–
wandb:                    policy_loss â–‡â–ˆâ–ˆâ–†â–…â–„â–â–‚
wandb:                          ratio â–ˆâ–ƒâ–…â–…â–â–…â–…â–„
wandb:                     value_loss â–ˆâ–‚â–‚â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                actor_grad_norm 0.12188
wandb:            agent0/dist_to_goal 0.56652
wandb:      agent0/individual_rewards -0.8009
wandb:        agent0/min_time_to_goal 0.46522
wandb:    agent0/num_agent_collisions 0.71875
wandb: agent0/num_obstacle_collisions 0.45312
wandb:            agent0/time_to_goal 2.45781
wandb:            agent1/dist_to_goal 0.52976
wandb:      agent1/individual_rewards -0.68548
wandb:        agent1/min_time_to_goal 0.44947
wandb:    agent1/num_agent_collisions 0.59375
wandb: agent1/num_obstacle_collisions 0.15625
wandb:            agent1/time_to_goal 2.44062
wandb:            agent2/dist_to_goal 0.46194
wandb:      agent2/individual_rewards -0.69491
wandb:        agent2/min_time_to_goal 0.49223
wandb:    agent2/num_agent_collisions 0.60938
wandb: agent2/num_obstacle_collisions 0.71875
wandb:            agent2/time_to_goal 2.44219
wandb:            agent3/dist_to_goal 0.59287
wandb:      agent3/individual_rewards -0.82724
wandb:        agent3/min_time_to_goal 0.55067
wandb:    agent3/num_agent_collisions 0.6875
wandb: agent3/num_obstacle_collisions 0.5
wandb:            agent3/time_to_goal 2.40156
wandb:            agent4/dist_to_goal 0.51311
wandb:      agent4/individual_rewards -0.43445
wandb:        agent4/min_time_to_goal 0.47317
wandb:    agent4/num_agent_collisions 0.65625
wandb: agent4/num_obstacle_collisions 0.45312
wandb:            agent4/time_to_goal 2.325
wandb:            agent5/dist_to_goal 0.61244
wandb:      agent5/individual_rewards -0.84614
wandb:        agent5/min_time_to_goal 0.48894
wandb:    agent5/num_agent_collisions 0.64062
wandb: agent5/num_obstacle_collisions 0.375
wandb:            agent5/time_to_goal 2.45938
wandb:            agent6/dist_to_goal 0.57346
wandb:      agent6/individual_rewards -0.72971
wandb:        agent6/min_time_to_goal 0.48914
wandb:    agent6/num_agent_collisions 0.875
wandb: agent6/num_obstacle_collisions 0.92188
wandb:            agent6/time_to_goal 2.42188
wandb:        average_episode_rewards -157.9389
wandb:               critic_grad_norm 3.19894
wandb:                   dist_entropy 1.36777
wandb:                    policy_loss -0.00566
wandb:                          ratio 0.99995
wandb:                     value_loss 0.06851
wandb: 
wandb: ðŸš€ View run rmappo_informarl_seed100 at: https://wandb.ai/golde/enemy/runs/tyquzltc
wandb: ï¸âš¡ View job at https://wandb.ai/golde/enemy/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2MTU4MTI2MA==/version_details/v17
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 3 other file(s)
wandb: Find logs at: ./onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240801_172358-tyquzltc/logs
Initializing navigation with fixed parameters
__________________________________________________
Choose to use gpu...
__________________________________________________
__________________________________________________
||                  Arguments                   ||
__________________________________________________
|| algorithm_name: rmappo                       ||
|| project_name: informarl                      ||
|| experiment_name: informarl                   ||
|| seed: 100                                    ||
|| cuda: True                                   ||
|| cuda_deterministic: True                     ||
|| n_training_threads: 1                        ||
|| n_rollout_threads: 128                       ||
|| n_eval_rollout_threads: 1                    ||
|| n_render_rollout_threads: 1                  ||
|| num_env_steps: 4000000                       ||
|| user_name: marl                              ||
|| use_wandb: True                              ||
|| env_name: GraphMPE                           ||
|| use_obs_instead_of_state: False              ||
|| world_size: 2                                ||
|| num_scripted_agents: 0                       ||
|| obs_type: global                             ||
|| max_edge_dist: 1                             ||
|| num_nbd_entities: 3                          ||
|| use_comm: False                              ||
|| episode_length: 25                           ||
|| share_policy: True                           ||
|| use_centralized_V: True                      ||
|| stacked_frames: 1                            ||
|| use_stacked_frames: False                    ||
|| hidden_size: 64                              ||
|| layer_N: 1                                   ||
|| use_ReLU: False                              ||
|| use_popart: True                             ||
|| use_valuenorm: False                         ||
|| use_feature_normalization: True              ||
|| use_orthogonal: True                         ||
|| gain: 0.01                                   ||
|| split_batch: False                           ||
|| max_batch_size: 32                           ||
|| use_naive_recurrent_policy: False            ||
|| use_recurrent_policy: True                   ||
|| recurrent_N: 1                               ||
|| data_chunk_length: 10                        ||
|| lr: 0.0007                                   ||
|| critic_lr: 0.0007                            ||
|| opti_eps: 1e-05                              ||
|| weight_decay: 0                              ||
|| ppo_epoch: 10                                ||
|| use_clipped_value_loss: True                 ||
|| clip_param: 0.2                              ||
|| num_mini_batch: 1                            ||
|| entropy_coef: 0.01                           ||
|| value_loss_coef: 1                           ||
|| use_max_grad_norm: True                      ||
|| max_grad_norm: 10.0                          ||
|| use_gae: True                                ||
|| gamma: 0.99                                  ||
|| gae_lambda: 0.95                             ||
|| use_proper_time_limits: False                ||
|| use_huber_loss: True                         ||
|| use_value_active_masks: True                 ||
|| use_policy_active_masks: True                ||
|| huber_delta: 10.0                            ||
|| use_linear_lr_decay: False                   ||
|| save_interval: 1                             ||
|| log_interval: 5                              ||
|| use_eval: False                              ||
|| eval_interval: 25                            ||
|| eval_episodes: 32                            ||
|| save_gifs: False                             ||
|| use_render: False                            ||
|| render_episodes: 5                           ||
|| ifi: 0.1                                     ||
|| render_eval: False                           ||
|| model_dir: None                              ||
|| verbose: True                                ||
|| scenario_name: navigation_graph              ||
|| num_landmarks: 3                             ||
|| num_agents: 7                                ||
|| num_obstacles: 3                             ||
|| reward_sparsity: 1                           ||
|| collaborative: True                          ||
|| max_speed: 2                                 ||
|| collision_rew: 5.0                           ||
|| goal_rew: 5                                  ||
|| min_dist_thresh: 0.05                        ||
|| use_dones: False                             ||
|| num_embeddings: 3                            ||
|| embedding_size: 2                            ||
|| embed_hidden_size: 16                        ||
|| embed_layer_N: 1                             ||
|| embed_use_ReLU: True                         ||
|| embed_add_self_loop: False                   ||
|| gnn_hidden_size: 16                          ||
|| gnn_num_heads: 3                             ||
|| gnn_concat_heads: False                      ||
|| gnn_layer_N: 2                               ||
|| gnn_use_ReLU: True                           ||
|| graph_feat_type: global                      ||
|| actor_graph_aggr: node                       ||
|| critic_graph_aggr: global                    ||
|| global_aggr_type: mean                       ||
|| use_cent_obs: False                          ||
|| auto_mini_batch_size: False                  ||
|| target_mini_batch_size: 32                   ||
__________________________________________________
__________________________________________________
Creating wandboard...
__________________________________________________
wandb: Currently logged in as: sharlinu (golde). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.11
wandb: Run data is saved locally in /home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240802_092014-pqnvgapx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rmappo_informarl_seed100
wandb: â­ï¸ View project at https://wandb.ai/golde/enemy
wandb: ðŸš€ View run at https://wandb.ai/golde/enemy/runs/pqnvgapx
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenariocreated scenario
Initialising graph navigation

Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenariocreated scenario
Initialising graph navigation

Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenariocreated scenario
Initialising graph navigation

Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
Overriding Observation dimension
created scenariocreated scenario

Initialising graph navigationInitialising graph navigation

created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenariocreated scenario

Initialising graph navigationInitialising graph navigation

created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
Overriding Observation dimension
________________________________________________________________________________
Actor Network
________________________________________________________________________________
________________________________________________________________________________
GR_Actor(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0): TransformerConv(16, 16, heads=3)
        (1): TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((22,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=22, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (act): ACTLayer(
    (action_out): Categorical(
      (linear): Linear(in_features=64, out_features=5, bias=True)
    )
  )
)
________________________________________________________________________________
________________________________________________________________________________
Critic Network
________________________________________________________________________________
________________________________________________________________________________
GR_Critic(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0): TransformerConv(16, 16, heads=3)
        (1): TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (v_out): PopArt()
)
________________________________________________________________________________
Traceback (most recent call last):
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py", line 239, in __lift__
    return src.index_select(self.node_dim, index)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 594.00 MiB (GPU 0; 44.55 GiB total capacity; 1.75 GiB already allocated; 543.38 MiB free; 1.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/scripts/train_mpe.py", line 318, in <module>
    main(sys.argv[1:])
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/scripts/train_mpe.py", line 303, in main
    runner.run()
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/runner/shared/graph_mpe_runner.py", line 76, in run
    train_infos = self.train()
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/runner/shared/base_runner.py", line 168, in train
    train_infos = self.trainer.train(self.buffer)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/graph_mappo.py", line 329, in train
    ) = self.ppo_update(sample, update_actor)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/graph_mappo.py", line 170, in ppo_update
    values, action_log_probs, dist_entropy = self.policy.evaluate_actions(
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/graph_MAPPOPolicy.py", line 255, in evaluate_actions
    action_log_probs, dist_entropy = self.actor.evaluate_actions(
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/graph_actor_critic.py", line 253, in evaluate_actions
    nbd_features = self.gnn_base(node_obs, adj, agent_id)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/utils/gnn.py", line 488, in forward
    x = self.gnn(node_obs, adj, agent_id)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/utils/gnn.py", line 278, in forward
    x = self.active_func(self.gnn1(x, edge_index, edge_attr))
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch_geometric/nn/conv/transformer_conv.py", line 176, in forward
    out = self.propagate(edge_index, query=query, key=key, value=value,
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py", line 429, in propagate
    coll_dict = self.__collect__(self.__user_args__, edge_index,
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py", line 301, in __collect__
    data = self.__lift__(data, edge_index, dim)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py", line 242, in __lift__
    raise ValueError(
ValueError: Encountered a CUDA error. Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 380800) in your node feature matrix and try again.
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: ðŸš€ View run rmappo_informarl_seed100 at: https://wandb.ai/golde/enemy/runs/pqnvgapx
wandb: ï¸âš¡ View job at https://wandb.ai/golde/enemy/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjM2MjUwMjIwOQ==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240802_092014-pqnvgapx/logs
Initializing navigation with fixed parameters
__________________________________________________
Choose to use gpu...
__________________________________________________
__________________________________________________
||                  Arguments                   ||
__________________________________________________
|| algorithm_name: rmappo                       ||
|| project_name: informarl                      ||
|| experiment_name: informarl                   ||
|| seed: 100                                    ||
|| cuda: True                                   ||
|| cuda_deterministic: True                     ||
|| n_training_threads: 1                        ||
|| n_rollout_threads: 64                        ||
|| n_eval_rollout_threads: 1                    ||
|| n_render_rollout_threads: 1                  ||
|| num_env_steps: 4000000                       ||
|| user_name: marl                              ||
|| use_wandb: True                              ||
|| env_name: GraphMPE                           ||
|| use_obs_instead_of_state: False              ||
|| world_size: 2                                ||
|| num_scripted_agents: 0                       ||
|| obs_type: global                             ||
|| max_edge_dist: 1                             ||
|| num_nbd_entities: 3                          ||
|| use_comm: False                              ||
|| episode_length: 25                           ||
|| share_policy: True                           ||
|| use_centralized_V: True                      ||
|| stacked_frames: 1                            ||
|| use_stacked_frames: False                    ||
|| hidden_size: 64                              ||
|| layer_N: 1                                   ||
|| use_ReLU: False                              ||
|| use_popart: True                             ||
|| use_valuenorm: False                         ||
|| use_feature_normalization: True              ||
|| use_orthogonal: True                         ||
|| gain: 0.01                                   ||
|| split_batch: False                           ||
|| max_batch_size: 32                           ||
|| use_naive_recurrent_policy: False            ||
|| use_recurrent_policy: True                   ||
|| recurrent_N: 1                               ||
|| data_chunk_length: 10                        ||
|| lr: 0.0007                                   ||
|| critic_lr: 0.0007                            ||
|| opti_eps: 1e-05                              ||
|| weight_decay: 0                              ||
|| ppo_epoch: 10                                ||
|| use_clipped_value_loss: True                 ||
|| clip_param: 0.2                              ||
|| num_mini_batch: 1                            ||
|| entropy_coef: 0.01                           ||
|| value_loss_coef: 1                           ||
|| use_max_grad_norm: True                      ||
|| max_grad_norm: 10.0                          ||
|| use_gae: True                                ||
|| gamma: 0.99                                  ||
|| gae_lambda: 0.95                             ||
|| use_proper_time_limits: False                ||
|| use_huber_loss: True                         ||
|| use_value_active_masks: True                 ||
|| use_policy_active_masks: True                ||
|| huber_delta: 10.0                            ||
|| use_linear_lr_decay: False                   ||
|| save_interval: 1                             ||
|| log_interval: 5                              ||
|| use_eval: False                              ||
|| eval_interval: 25                            ||
|| eval_episodes: 32                            ||
|| save_gifs: False                             ||
|| use_render: False                            ||
|| render_episodes: 5                           ||
|| ifi: 0.1                                     ||
|| render_eval: False                           ||
|| model_dir: None                              ||
|| verbose: True                                ||
|| scenario_name: navigation_graph              ||
|| num_landmarks: 3                             ||
|| num_agents: 7                                ||
|| num_obstacles: 3                             ||
|| reward_sparsity: 1                           ||
|| collaborative: True                          ||
|| max_speed: 2                                 ||
|| collision_rew: 5.0                           ||
|| goal_rew: 5                                  ||
|| min_dist_thresh: 0.05                        ||
|| use_dones: False                             ||
|| num_embeddings: 3                            ||
|| embedding_size: 2                            ||
|| embed_hidden_size: 16                        ||
|| embed_layer_N: 1                             ||
|| embed_use_ReLU: True                         ||
|| embed_add_self_loop: False                   ||
|| gnn_hidden_size: 16                          ||
|| gnn_num_heads: 3                             ||
|| gnn_concat_heads: False                      ||
|| gnn_layer_N: 2                               ||
|| gnn_use_ReLU: True                           ||
|| graph_feat_type: global                      ||
|| actor_graph_aggr: node                       ||
|| critic_graph_aggr: global                    ||
|| global_aggr_type: mean                       ||
|| use_cent_obs: False                          ||
|| auto_mini_batch_size: False                  ||
|| target_mini_batch_size: 32                   ||
__________________________________________________
__________________________________________________
Creating wandboard...
__________________________________________________
wandb: Currently logged in as: sharlinu (golde). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.11
wandb: Run data is saved locally in /home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240802_092251-4qplrtxj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rmappo_informarl_seed100
wandb: â­ï¸ View project at https://wandb.ai/golde/enemy
wandb: ðŸš€ View run at https://wandb.ai/golde/enemy/runs/4qplrtxj
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
Overriding Observation dimension
Overriding Observation dimension
________________________________________________________________________________
Actor Network
________________________________________________________________________________
________________________________________________________________________________
GR_Actor(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0): TransformerConv(16, 16, heads=3)
        (1): TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((22,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=22, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (act): ACTLayer(
    (action_out): Categorical(
      (linear): Linear(in_features=64, out_features=5, bias=True)
    )
  )
)
________________________________________________________________________________
________________________________________________________________________________
Critic Network
________________________________________________________________________________
________________________________________________________________________________
GR_Critic(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0): TransformerConv(16, 16, heads=3)
        (1): TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (v_out): PopArt()
)
________________________________________________________________________________
Traceback (most recent call last):
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/scripts/train_mpe.py", line 318, in <module>
    main(sys.argv[1:])
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/scripts/train_mpe.py", line 303, in main
    runner.run()
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/runner/shared/graph_mpe_runner.py", line 76, in run
    train_infos = self.train()
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/runner/shared/base_runner.py", line 168, in train
    train_infos = self.trainer.train(self.buffer)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/graph_mappo.py", line 329, in train
    ) = self.ppo_update(sample, update_actor)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/graph_mappo.py", line 170, in ppo_update
    values, action_log_probs, dist_entropy = self.policy.evaluate_actions(
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/graph_MAPPOPolicy.py", line 255, in evaluate_actions
    action_log_probs, dist_entropy = self.actor.evaluate_actions(
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/graph_actor_critic.py", line 253, in evaluate_actions
    nbd_features = self.gnn_base(node_obs, adj, agent_id)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/utils/gnn.py", line 488, in forward
    x = self.gnn(node_obs, adj, agent_id)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/algorithms/utils/gnn.py", line 278, in forward
    x = self.active_func(self.gnn1(x, edge_index, edge_attr))
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch_geometric/nn/conv/transformer_conv.py", line 176, in forward
    out = self.propagate(edge_index, query=query, key=key, value=value,
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py", line 437, in propagate
    out = self.message(**msg_kwargs)
  File "/home/utke_s@WMGDS.WMG.WARWICK.AC.UK/miniconda3/envs/MADRL_py39/lib/python3.9/site-packages/torch_geometric/nn/conv/transformer_conv.py", line 222, in message
    out = out + edge_attr
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 296.00 MiB (GPU 0; 44.55 GiB total capacity; 2.08 GiB already allocated; 7.38 MiB free; 2.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: ðŸš€ View run rmappo_informarl_seed100 at: https://wandb.ai/golde/enemy/runs/4qplrtxj
wandb: ï¸âš¡ View job at https://wandb.ai/golde/enemy/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjM2MjUwMjIwOQ==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240802_092251-4qplrtxj/logs
Initializing navigation with fixed parameters
__________________________________________________
Choose to use gpu...
__________________________________________________
__________________________________________________
||                  Arguments                   ||
__________________________________________________
|| algorithm_name: rmappo                       ||
|| project_name: informarl                      ||
|| experiment_name: informarl                   ||
|| seed: 100                                    ||
|| cuda: True                                   ||
|| cuda_deterministic: True                     ||
|| n_training_threads: 1                        ||
|| n_rollout_threads: 64                        ||
|| n_eval_rollout_threads: 1                    ||
|| n_render_rollout_threads: 1                  ||
|| num_env_steps: 4000000                       ||
|| user_name: marl                              ||
|| use_wandb: True                              ||
|| env_name: GraphMPE                           ||
|| use_obs_instead_of_state: False              ||
|| world_size: 2                                ||
|| num_scripted_agents: 0                       ||
|| obs_type: global                             ||
|| max_edge_dist: 1                             ||
|| num_nbd_entities: 3                          ||
|| use_comm: False                              ||
|| episode_length: 25                           ||
|| share_policy: True                           ||
|| use_centralized_V: True                      ||
|| stacked_frames: 1                            ||
|| use_stacked_frames: False                    ||
|| hidden_size: 64                              ||
|| layer_N: 1                                   ||
|| use_ReLU: False                              ||
|| use_popart: True                             ||
|| use_valuenorm: False                         ||
|| use_feature_normalization: True              ||
|| use_orthogonal: True                         ||
|| gain: 0.01                                   ||
|| split_batch: False                           ||
|| max_batch_size: 32                           ||
|| use_naive_recurrent_policy: False            ||
|| use_recurrent_policy: True                   ||
|| recurrent_N: 1                               ||
|| data_chunk_length: 10                        ||
|| lr: 0.0007                                   ||
|| critic_lr: 0.0007                            ||
|| opti_eps: 1e-05                              ||
|| weight_decay: 0                              ||
|| ppo_epoch: 10                                ||
|| use_clipped_value_loss: True                 ||
|| clip_param: 0.2                              ||
|| num_mini_batch: 1                            ||
|| entropy_coef: 0.01                           ||
|| value_loss_coef: 1                           ||
|| use_max_grad_norm: True                      ||
|| max_grad_norm: 10.0                          ||
|| use_gae: True                                ||
|| gamma: 0.99                                  ||
|| gae_lambda: 0.95                             ||
|| use_proper_time_limits: False                ||
|| use_huber_loss: True                         ||
|| use_value_active_masks: True                 ||
|| use_policy_active_masks: True                ||
|| huber_delta: 10.0                            ||
|| use_linear_lr_decay: False                   ||
|| save_interval: 1                             ||
|| log_interval: 5                              ||
|| use_eval: False                              ||
|| eval_interval: 25                            ||
|| eval_episodes: 32                            ||
|| save_gifs: False                             ||
|| use_render: False                            ||
|| render_episodes: 5                           ||
|| ifi: 0.1                                     ||
|| render_eval: False                           ||
|| model_dir: None                              ||
|| verbose: True                                ||
|| scenario_name: navigation_graph              ||
|| num_landmarks: 3                             ||
|| num_agents: 7                                ||
|| num_obstacles: 3                             ||
|| reward_sparsity: 1                           ||
|| collaborative: True                          ||
|| max_speed: 2                                 ||
|| collision_rew: 5.0                           ||
|| goal_rew: 5                                  ||
|| min_dist_thresh: 0.05                        ||
|| use_dones: False                             ||
|| num_embeddings: 3                            ||
|| embedding_size: 2                            ||
|| embed_hidden_size: 16                        ||
|| embed_layer_N: 1                             ||
|| embed_use_ReLU: True                         ||
|| embed_add_self_loop: False                   ||
|| gnn_hidden_size: 16                          ||
|| gnn_num_heads: 3                             ||
|| gnn_concat_heads: False                      ||
|| gnn_layer_N: 2                               ||
|| gnn_use_ReLU: True                           ||
|| graph_feat_type: global                      ||
|| actor_graph_aggr: node                       ||
|| critic_graph_aggr: global                    ||
|| global_aggr_type: mean                       ||
|| use_cent_obs: False                          ||
|| auto_mini_batch_size: False                  ||
|| target_mini_batch_size: 32                   ||
__________________________________________________
__________________________________________________
Creating wandboard...
__________________________________________________
wandb: Currently logged in as: sharlinu (golde). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.11
wandb: Run data is saved locally in /home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240804_151934-50rjjkxp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rmappo_informarl_seed100
wandb: â­ï¸ View project at https://wandb.ai/golde/enemy
wandb: ðŸš€ View run at https://wandb.ai/golde/enemy/runs/50rjjkxp
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
Overriding Observation dimension
Overriding Observation dimension
________________________________________________________________________________
Actor Network
________________________________________________________________________________
________________________________________________________________________________
GR_Actor(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0): TransformerConv(16, 16, heads=3)
        (1): TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((22,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=22, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (act): ACTLayer(
    (action_out): Categorical(
      (linear): Linear(in_features=64, out_features=5, bias=True)
    )
  )
)
________________________________________________________________________________
________________________________________________________________________________
Critic Network
________________________________________________________________________________
________________________________________________________________________________
GR_Critic(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0): TransformerConv(16, 16, heads=3)
        (1): TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (v_out): PopArt()
)
________________________________________________________________________________
Average episode rewards is -213.193 	Total timesteps: 1600 	 Percentage complete 0.040
Average episode rewards is -182.901 	Total timesteps: 9600 	 Percentage complete 0.240
Average episode rewards is -176.204 	Total timesteps: 17600 	 Percentage complete 0.440
Average episode rewards is -179.761 	Total timesteps: 25600 	 Percentage complete 0.640
Average episode rewards is -161.312 	Total timesteps: 33600 	 Percentage complete 0.840
Average episode rewards is -145.119 	Total timesteps: 41600 	 Percentage complete 1.040
Average episode rewards is -139.706 	Total timesteps: 49600 	 Percentage complete 1.240
Average episode rewards is -127.632 	Total timesteps: 57600 	 Percentage complete 1.440
Average episode rewards is -117.995 	Total timesteps: 65600 	 Percentage complete 1.640
Average episode rewards is -109.549 	Total timesteps: 73600 	 Percentage complete 1.840
Average episode rewards is -105.263 	Total timesteps: 81600 	 Percentage complete 2.040
Average episode rewards is -89.035 	Total timesteps: 89600 	 Percentage complete 2.240
Average episode rewards is -79.484 	Total timesteps: 97600 	 Percentage complete 2.440
Average episode rewards is -58.639 	Total timesteps: 105600 	 Percentage complete 2.640
Average episode rewards is -38.490 	Total timesteps: 113600 	 Percentage complete 2.840
Average episode rewards is -31.482 	Total timesteps: 121600 	 Percentage complete 3.040
Average episode rewards is -23.548 	Total timesteps: 129600 	 Percentage complete 3.240
Average episode rewards is -25.172 	Total timesteps: 137600 	 Percentage complete 3.440
Average episode rewards is -5.468 	Total timesteps: 145600 	 Percentage complete 3.640
Average episode rewards is -8.908 	Total timesteps: 153600 	 Percentage complete 3.840
Average episode rewards is -0.697 	Total timesteps: 161600 	 Percentage complete 4.040
Average episode rewards is 2.292 	Total timesteps: 169600 	 Percentage complete 4.240
Average episode rewards is 11.180 	Total timesteps: 177600 	 Percentage complete 4.440
Average episode rewards is 18.092 	Total timesteps: 185600 	 Percentage complete 4.640
Average episode rewards is 27.164 	Total timesteps: 193600 	 Percentage complete 4.840
Average episode rewards is 11.404 	Total timesteps: 201600 	 Percentage complete 5.040
Average episode rewards is 21.637 	Total timesteps: 209600 	 Percentage complete 5.240
Average episode rewards is 46.871 	Total timesteps: 217600 	 Percentage complete 5.440
Average episode rewards is 6.504 	Total timesteps: 225600 	 Percentage complete 5.640
Average episode rewards is 1.668 	Total timesteps: 233600 	 Percentage complete 5.840
Average episode rewards is -30.626 	Total timesteps: 241600 	 Percentage complete 6.040
Average episode rewards is -4.110 	Total timesteps: 249600 	 Percentage complete 6.240
Average episode rewards is 6.819 	Total timesteps: 257600 	 Percentage complete 6.440
Average episode rewards is 11.679 	Total timesteps: 265600 	 Percentage complete 6.640
Average episode rewards is -25.639 	Total timesteps: 273600 	 Percentage complete 6.840
Average episode rewards is 3.184 	Total timesteps: 281600 	 Percentage complete 7.040
Average episode rewards is -0.174 	Total timesteps: 289600 	 Percentage complete 7.240
Average episode rewards is -18.581 	Total timesteps: 297600 	 Percentage complete 7.440
Average episode rewards is 2.213 	Total timesteps: 305600 	 Percentage complete 7.640
Average episode rewards is -18.437 	Total timesteps: 313600 	 Percentage complete 7.840
Average episode rewards is 8.303 	Total timesteps: 321600 	 Percentage complete 8.040
Average episode rewards is -4.248 	Total timesteps: 329600 	 Percentage complete 8.240
Average episode rewards is -11.523 	Total timesteps: 337600 	 Percentage complete 8.440
Average episode rewards is -31.057 	Total timesteps: 345600 	 Percentage complete 8.640
Average episode rewards is -24.455 	Total timesteps: 353600 	 Percentage complete 8.840
Average episode rewards is 29.221 	Total timesteps: 361600 	 Percentage complete 9.040
Average episode rewards is 20.249 	Total timesteps: 369600 	 Percentage complete 9.240
Average episode rewards is -27.896 	Total timesteps: 377600 	 Percentage complete 9.440
Average episode rewards is 0.771 	Total timesteps: 385600 	 Percentage complete 9.640
Average episode rewards is 16.460 	Total timesteps: 393600 	 Percentage complete 9.840
Average episode rewards is -5.618 	Total timesteps: 401600 	 Percentage complete 10.040
Average episode rewards is -35.432 	Total timesteps: 409600 	 Percentage complete 10.240
Average episode rewards is 9.402 	Total timesteps: 417600 	 Percentage complete 10.440
Average episode rewards is -38.680 	Total timesteps: 425600 	 Percentage complete 10.640
Average episode rewards is -25.649 	Total timesteps: 433600 	 Percentage complete 10.840
Average episode rewards is -13.419 	Total timesteps: 441600 	 Percentage complete 11.040
Average episode rewards is -2.371 	Total timesteps: 449600 	 Percentage complete 11.240
Average episode rewards is -0.035 	Total timesteps: 457600 	 Percentage complete 11.440
Average episode rewards is -0.520 	Total timesteps: 465600 	 Percentage complete 11.640
Average episode rewards is 42.595 	Total timesteps: 473600 	 Percentage complete 11.840
Average episode rewards is -47.233 	Total timesteps: 481600 	 Percentage complete 12.040
Average episode rewards is 12.735 	Total timesteps: 489600 	 Percentage complete 12.240
Average episode rewards is 17.375 	Total timesteps: 497600 	 Percentage complete 12.440
Average episode rewards is -21.062 	Total timesteps: 505600 	 Percentage complete 12.640
Average episode rewards is 1.688 	Total timesteps: 513600 	 Percentage complete 12.840
Average episode rewards is 9.856 	Total timesteps: 521600 	 Percentage complete 13.040
Average episode rewards is 44.172 	Total timesteps: 529600 	 Percentage complete 13.240
Average episode rewards is -9.974 	Total timesteps: 537600 	 Percentage complete 13.440
Average episode rewards is -10.893 	Total timesteps: 545600 	 Percentage complete 13.640
Average episode rewards is 13.773 	Total timesteps: 553600 	 Percentage complete 13.840
Average episode rewards is -3.348 	Total timesteps: 561600 	 Percentage complete 14.040
Average episode rewards is -3.963 	Total timesteps: 569600 	 Percentage complete 14.240
Average episode rewards is -23.595 	Total timesteps: 577600 	 Percentage complete 14.440
Average episode rewards is 14.431 	Total timesteps: 585600 	 Percentage complete 14.640
Average episode rewards is -26.811 	Total timesteps: 593600 	 Percentage complete 14.840
Average episode rewards is -4.166 	Total timesteps: 601600 	 Percentage complete 15.040
Average episode rewards is 42.976 	Total timesteps: 609600 	 Percentage complete 15.240
Average episode rewards is -11.295 	Total timesteps: 617600 	 Percentage complete 15.440
Average episode rewards is 19.756 	Total timesteps: 625600 	 Percentage complete 15.640
Average episode rewards is -2.803 	Total timesteps: 633600 	 Percentage complete 15.840
Average episode rewards is 20.065 	Total timesteps: 641600 	 Percentage complete 16.040
Average episode rewards is 49.362 	Total timesteps: 649600 	 Percentage complete 16.240
Average episode rewards is 1.938 	Total timesteps: 657600 	 Percentage complete 16.440
Average episode rewards is 21.039 	Total timesteps: 665600 	 Percentage complete 16.640
Average episode rewards is 3.906 	Total timesteps: 673600 	 Percentage complete 16.840
Average episode rewards is 55.402 	Total timesteps: 681600 	 Percentage complete 17.040
Average episode rewards is 64.135 	Total timesteps: 689600 	 Percentage complete 17.240
Average episode rewards is 31.805 	Total timesteps: 697600 	 Percentage complete 17.440
Average episode rewards is 13.379 	Total timesteps: 705600 	 Percentage complete 17.640
Average episode rewards is 35.647 	Total timesteps: 713600 	 Percentage complete 17.840
Average episode rewards is 22.759 	Total timesteps: 721600 	 Percentage complete 18.040
Average episode rewards is 52.285 	Total timesteps: 729600 	 Percentage complete 18.240
Average episode rewards is 9.901 	Total timesteps: 737600 	 Percentage complete 18.440
Average episode rewards is 70.504 	Total timesteps: 745600 	 Percentage complete 18.640
Average episode rewards is 41.011 	Total timesteps: 753600 	 Percentage complete 18.840
Average episode rewards is 49.120 	Total timesteps: 761600 	 Percentage complete 19.040
Average episode rewards is 49.582 	Total timesteps: 769600 	 Percentage complete 19.240
Average episode rewards is 76.503 	Total timesteps: 777600 	 Percentage complete 19.440
Average episode rewards is 37.592 	Total timesteps: 785600 	 Percentage complete 19.640
Average episode rewards is 39.161 	Total timesteps: 793600 	 Percentage complete 19.840
Average episode rewards is 45.864 	Total timesteps: 801600 	 Percentage complete 20.040
Average episode rewards is 124.457 	Total timesteps: 809600 	 Percentage complete 20.240
Average episode rewards is 69.524 	Total timesteps: 817600 	 Percentage complete 20.440
Average episode rewards is 24.294 	Total timesteps: 825600 	 Percentage complete 20.640
Average episode rewards is 64.098 	Total timesteps: 833600 	 Percentage complete 20.840
Average episode rewards is 8.268 	Total timesteps: 841600 	 Percentage complete 21.040
Average episode rewards is 55.301 	Total timesteps: 849600 	 Percentage complete 21.240
Average episode rewards is 30.921 	Total timesteps: 857600 	 Percentage complete 21.440
Average episode rewards is 76.189 	Total timesteps: 865600 	 Percentage complete 21.640
Average episode rewards is 48.802 	Total timesteps: 873600 	 Percentage complete 21.840
Average episode rewards is 76.789 	Total timesteps: 881600 	 Percentage complete 22.040
Average episode rewards is 57.903 	Total timesteps: 889600 	 Percentage complete 22.240
Average episode rewards is 54.512 	Total timesteps: 897600 	 Percentage complete 22.440
Average episode rewards is 36.541 	Total timesteps: 905600 	 Percentage complete 22.640
Average episode rewards is 76.442 	Total timesteps: 913600 	 Percentage complete 22.840
Average episode rewards is 114.906 	Total timesteps: 921600 	 Percentage complete 23.040
Average episode rewards is 93.140 	Total timesteps: 929600 	 Percentage complete 23.240
Average episode rewards is 126.045 	Total timesteps: 937600 	 Percentage complete 23.440
Average episode rewards is 90.144 	Total timesteps: 945600 	 Percentage complete 23.640
Average episode rewards is 63.460 	Total timesteps: 953600 	 Percentage complete 23.840
Average episode rewards is 73.733 	Total timesteps: 961600 	 Percentage complete 24.040
Average episode rewards is 98.286 	Total timesteps: 969600 	 Percentage complete 24.240
Average episode rewards is 57.056 	Total timesteps: 977600 	 Percentage complete 24.440
Average episode rewards is 33.673 	Total timesteps: 985600 	 Percentage complete 24.640
Average episode rewards is 51.276 	Total timesteps: 993600 	 Percentage complete 24.840
Average episode rewards is 38.636 	Total timesteps: 1001600 	 Percentage complete 25.040
Average episode rewards is 57.099 	Total timesteps: 1009600 	 Percentage complete 25.240
Average episode rewards is 60.008 	Total timesteps: 1017600 	 Percentage complete 25.440
Average episode rewards is 93.368 	Total timesteps: 1025600 	 Percentage complete 25.640
Average episode rewards is 71.576 	Total timesteps: 1033600 	 Percentage complete 25.840
Average episode rewards is 107.332 	Total timesteps: 1041600 	 Percentage complete 26.040
Average episode rewards is 83.169 	Total timesteps: 1049600 	 Percentage complete 26.240
Average episode rewards is 70.051 	Total timesteps: 1057600 	 Percentage complete 26.440
Average episode rewards is 115.729 	Total timesteps: 1065600 	 Percentage complete 26.640
Average episode rewards is 109.548 	Total timesteps: 1073600 	 Percentage complete 26.840
Average episode rewards is 109.087 	Total timesteps: 1081600 	 Percentage complete 27.040
Average episode rewards is 127.343 	Total timesteps: 1089600 	 Percentage complete 27.240
Average episode rewards is 170.085 	Total timesteps: 1097600 	 Percentage complete 27.440
Average episode rewards is 132.895 	Total timesteps: 1105600 	 Percentage complete 27.640
Average episode rewards is 134.351 	Total timesteps: 1113600 	 Percentage complete 27.840
Average episode rewards is 139.105 	Total timesteps: 1121600 	 Percentage complete 28.040
Average episode rewards is 164.066 	Total timesteps: 1129600 	 Percentage complete 28.240
Average episode rewards is 66.505 	Total timesteps: 1137600 	 Percentage complete 28.440
Average episode rewards is 127.749 	Total timesteps: 1145600 	 Percentage complete 28.640
Average episode rewards is 152.978 	Total timesteps: 1153600 	 Percentage complete 28.840
Average episode rewards is 154.563 	Total timesteps: 1161600 	 Percentage complete 29.040
Average episode rewards is 131.036 	Total timesteps: 1169600 	 Percentage complete 29.240
Average episode rewards is 140.805 	Total timesteps: 1177600 	 Percentage complete 29.440
Average episode rewards is 113.914 	Total timesteps: 1185600 	 Percentage complete 29.640
Average episode rewards is 104.270 	Total timesteps: 1193600 	 Percentage complete 29.840
Average episode rewards is 109.243 	Total timesteps: 1201600 	 Percentage complete 30.040
Average episode rewards is 120.622 	Total timesteps: 1209600 	 Percentage complete 30.240
Average episode rewards is 144.549 	Total timesteps: 1217600 	 Percentage complete 30.440
Average episode rewards is 164.478 	Total timesteps: 1225600 	 Percentage complete 30.640
Average episode rewards is 186.428 	Total timesteps: 1233600 	 Percentage complete 30.840
Average episode rewards is 144.525 	Total timesteps: 1241600 	 Percentage complete 31.040
Average episode rewards is 142.657 	Total timesteps: 1249600 	 Percentage complete 31.240
Average episode rewards is 135.462 	Total timesteps: 1257600 	 Percentage complete 31.440
Average episode rewards is 75.984 	Total timesteps: 1265600 	 Percentage complete 31.640
Average episode rewards is 137.454 	Total timesteps: 1273600 	 Percentage complete 31.840
Average episode rewards is 85.997 	Total timesteps: 1281600 	 Percentage complete 32.040
Average episode rewards is 110.264 	Total timesteps: 1289600 	 Percentage complete 32.240
Average episode rewards is 109.929 	Total timesteps: 1297600 	 Percentage complete 32.440
Average episode rewards is 66.483 	Total timesteps: 1305600 	 Percentage complete 32.640
Average episode rewards is 106.898 	Total timesteps: 1313600 	 Percentage complete 32.840
Average episode rewards is 79.794 	Total timesteps: 1321600 	 Percentage complete 33.040
Average episode rewards is 131.505 	Total timesteps: 1329600 	 Percentage complete 33.240
Average episode rewards is 70.040 	Total timesteps: 1337600 	 Percentage complete 33.440
Average episode rewards is 34.996 	Total timesteps: 1345600 	 Percentage complete 33.640
Average episode rewards is 133.738 	Total timesteps: 1353600 	 Percentage complete 33.840
Average episode rewards is 101.330 	Total timesteps: 1361600 	 Percentage complete 34.040
Average episode rewards is 115.456 	Total timesteps: 1369600 	 Percentage complete 34.240
Average episode rewards is 107.393 	Total timesteps: 1377600 	 Percentage complete 34.440
Average episode rewards is 87.126 	Total timesteps: 1385600 	 Percentage complete 34.640
Average episode rewards is 86.014 	Total timesteps: 1393600 	 Percentage complete 34.840
Average episode rewards is 100.378 	Total timesteps: 1401600 	 Percentage complete 35.040
Average episode rewards is 63.230 	Total timesteps: 1409600 	 Percentage complete 35.240
Average episode rewards is 41.653 	Total timesteps: 1417600 	 Percentage complete 35.440
Average episode rewards is 57.200 	Total timesteps: 1425600 	 Percentage complete 35.640
Average episode rewards is 73.677 	Total timesteps: 1433600 	 Percentage complete 35.840
Average episode rewards is 116.548 	Total timesteps: 1441600 	 Percentage complete 36.040
Average episode rewards is 134.239 	Total timesteps: 1449600 	 Percentage complete 36.240
Average episode rewards is 25.989 	Total timesteps: 1457600 	 Percentage complete 36.440
Average episode rewards is 92.241 	Total timesteps: 1465600 	 Percentage complete 36.640
Average episode rewards is 44.529 	Total timesteps: 1473600 	 Percentage complete 36.840
Average episode rewards is 96.285 	Total timesteps: 1481600 	 Percentage complete 37.040
Average episode rewards is 81.276 	Total timesteps: 1489600 	 Percentage complete 37.240
Average episode rewards is 67.375 	Total timesteps: 1497600 	 Percentage complete 37.440
Average episode rewards is 96.936 	Total timesteps: 1505600 	 Percentage complete 37.640
Average episode rewards is 123.869 	Total timesteps: 1513600 	 Percentage complete 37.840
Average episode rewards is 52.590 	Total timesteps: 1521600 	 Percentage complete 38.040
Average episode rewards is 76.582 	Total timesteps: 1529600 	 Percentage complete 38.240
Average episode rewards is 77.682 	Total timesteps: 1537600 	 Percentage complete 38.440
Average episode rewards is 25.786 	Total timesteps: 1545600 	 Percentage complete 38.640
Average episode rewards is 73.920 	Total timesteps: 1553600 	 Percentage complete 38.840
Average episode rewards is 1.055 	Total timesteps: 1561600 	 Percentage complete 39.040
Average episode rewards is 32.270 	Total timesteps: 1569600 	 Percentage complete 39.240
Average episode rewards is 75.651 	Total timesteps: 1577600 	 Percentage complete 39.440
Average episode rewards is 46.800 	Total timesteps: 1585600 	 Percentage complete 39.640
Average episode rewards is 19.679 	Total timesteps: 1593600 	 Percentage complete 39.840
Average episode rewards is 20.503 	Total timesteps: 1601600 	 Percentage complete 40.040
Average episode rewards is 64.211 	Total timesteps: 1609600 	 Percentage complete 40.240
Average episode rewards is 66.328 	Total timesteps: 1617600 	 Percentage complete 40.440
Average episode rewards is 62.775 	Total timesteps: 1625600 	 Percentage complete 40.640
Average episode rewards is 36.524 	Total timesteps: 1633600 	 Percentage complete 40.840
Average episode rewards is 24.112 	Total timesteps: 1641600 	 Percentage complete 41.040
Average episode rewards is 38.782 	Total timesteps: 1649600 	 Percentage complete 41.240
Average episode rewards is 46.079 	Total timesteps: 1657600 	 Percentage complete 41.440
Average episode rewards is 57.688 	Total timesteps: 1665600 	 Percentage complete 41.640
Average episode rewards is 11.172 	Total timesteps: 1673600 	 Percentage complete 41.840
Average episode rewards is 15.895 	Total timesteps: 1681600 	 Percentage complete 42.040
Average episode rewards is 11.961 	Total timesteps: 1689600 	 Percentage complete 42.240
Average episode rewards is 30.033 	Total timesteps: 1697600 	 Percentage complete 42.440
Average episode rewards is 42.039 	Total timesteps: 1705600 	 Percentage complete 42.640
Average episode rewards is 0.155 	Total timesteps: 1713600 	 Percentage complete 42.840
Average episode rewards is 13.743 	Total timesteps: 1721600 	 Percentage complete 43.040
Average episode rewards is 47.867 	Total timesteps: 1729600 	 Percentage complete 43.240
Average episode rewards is 25.435 	Total timesteps: 1737600 	 Percentage complete 43.440
Average episode rewards is 12.095 	Total timesteps: 1745600 	 Percentage complete 43.640
Average episode rewards is 32.895 	Total timesteps: 1753600 	 Percentage complete 43.840
Average episode rewards is 75.538 	Total timesteps: 1761600 	 Percentage complete 44.040
Average episode rewards is 61.830 	Total timesteps: 1769600 	 Percentage complete 44.240
Average episode rewards is 39.765 	Total timesteps: 1777600 	 Percentage complete 44.440
Average episode rewards is -2.258 	Total timesteps: 1785600 	 Percentage complete 44.640
Average episode rewards is 74.966 	Total timesteps: 1793600 	 Percentage complete 44.840
Average episode rewards is 60.820 	Total timesteps: 1801600 	 Percentage complete 45.040
Average episode rewards is 106.563 	Total timesteps: 1809600 	 Percentage complete 45.240
Average episode rewards is 25.888 	Total timesteps: 1817600 	 Percentage complete 45.440
Average episode rewards is 25.805 	Total timesteps: 1825600 	 Percentage complete 45.640
Average episode rewards is 98.908 	Total timesteps: 1833600 	 Percentage complete 45.840
Average episode rewards is 17.109 	Total timesteps: 1841600 	 Percentage complete 46.040
Average episode rewards is 29.551 	Total timesteps: 1849600 	 Percentage complete 46.240
Average episode rewards is -7.523 	Total timesteps: 1857600 	 Percentage complete 46.440
Average episode rewards is 11.161 	Total timesteps: 1865600 	 Percentage complete 46.640
Average episode rewards is -14.502 	Total timesteps: 1873600 	 Percentage complete 46.840
Average episode rewards is -2.270 	Total timesteps: 1881600 	 Percentage complete 47.040
Average episode rewards is 25.133 	Total timesteps: 1889600 	 Percentage complete 47.240
Average episode rewards is 8.013 	Total timesteps: 1897600 	 Percentage complete 47.440
Average episode rewards is -27.239 	Total timesteps: 1905600 	 Percentage complete 47.640
Average episode rewards is 14.420 	Total timesteps: 1913600 	 Percentage complete 47.840
Average episode rewards is 33.736 	Total timesteps: 1921600 	 Percentage complete 48.040
Average episode rewards is 62.283 	Total timesteps: 1929600 	 Percentage complete 48.240
Average episode rewards is 42.243 	Total timesteps: 1937600 	 Percentage complete 48.440
Average episode rewards is 49.707 	Total timesteps: 1945600 	 Percentage complete 48.640
Average episode rewards is 60.139 	Total timesteps: 1953600 	 Percentage complete 48.840
Average episode rewards is 16.018 	Total timesteps: 1961600 	 Percentage complete 49.040
Average episode rewards is 62.578 	Total timesteps: 1969600 	 Percentage complete 49.240
Average episode rewards is 51.873 	Total timesteps: 1977600 	 Percentage complete 49.440
Average episode rewards is 28.007 	Total timesteps: 1985600 	 Percentage complete 49.640
Average episode rewards is 31.716 	Total timesteps: 1993600 	 Percentage complete 49.840
Average episode rewards is 112.303 	Total timesteps: 2001600 	 Percentage complete 50.040
Average episode rewards is 25.090 	Total timesteps: 2009600 	 Percentage complete 50.240
Average episode rewards is -3.584 	Total timesteps: 2017600 	 Percentage complete 50.440
Average episode rewards is 109.436 	Total timesteps: 2025600 	 Percentage complete 50.640
Average episode rewards is 9.075 	Total timesteps: 2033600 	 Percentage complete 50.840
Average episode rewards is 28.368 	Total timesteps: 2041600 	 Percentage complete 51.040
Average episode rewards is 14.698 	Total timesteps: 2049600 	 Percentage complete 51.240
Average episode rewards is 112.977 	Total timesteps: 2057600 	 Percentage complete 51.440
Average episode rewards is 68.861 	Total timesteps: 2065600 	 Percentage complete 51.640
Average episode rewards is 96.102 	Total timesteps: 2073600 	 Percentage complete 51.840
Average episode rewards is 53.838 	Total timesteps: 2081600 	 Percentage complete 52.040
Average episode rewards is 76.319 	Total timesteps: 2089600 	 Percentage complete 52.240
Average episode rewards is 130.778 	Total timesteps: 2097600 	 Percentage complete 52.440
Average episode rewards is 145.667 	Total timesteps: 2105600 	 Percentage complete 52.640
Average episode rewards is 131.389 	Total timesteps: 2113600 	 Percentage complete 52.840
Average episode rewards is 89.222 	Total timesteps: 2121600 	 Percentage complete 53.040
Average episode rewards is 161.382 	Total timesteps: 2129600 	 Percentage complete 53.240
Average episode rewards is 140.553 	Total timesteps: 2137600 	 Percentage complete 53.440
Average episode rewards is 147.301 	Total timesteps: 2145600 	 Percentage complete 53.640
Average episode rewards is 189.337 	Total timesteps: 2153600 	 Percentage complete 53.840
Average episode rewards is 133.512 	Total timesteps: 2161600 	 Percentage complete 54.040
Average episode rewards is 192.945 	Total timesteps: 2169600 	 Percentage complete 54.240
Average episode rewards is 193.032 	Total timesteps: 2177600 	 Percentage complete 54.440
Average episode rewards is 195.412 	Total timesteps: 2185600 	 Percentage complete 54.640
Average episode rewards is 256.958 	Total timesteps: 2193600 	 Percentage complete 54.840
Average episode rewards is 167.673 	Total timesteps: 2201600 	 Percentage complete 55.040
Average episode rewards is 205.768 	Total timesteps: 2209600 	 Percentage complete 55.240
Average episode rewards is 269.747 	Total timesteps: 2217600 	 Percentage complete 55.440
Average episode rewards is 280.091 	Total timesteps: 2225600 	 Percentage complete 55.640
Average episode rewards is 311.018 	Total timesteps: 2233600 	 Percentage complete 55.840
Average episode rewards is 280.467 	Total timesteps: 2241600 	 Percentage complete 56.040
Average episode rewards is 290.403 	Total timesteps: 2249600 	 Percentage complete 56.240
Average episode rewards is 228.623 	Total timesteps: 2257600 	 Percentage complete 56.440
Average episode rewards is 282.587 	Total timesteps: 2265600 	 Percentage complete 56.640
Average episode rewards is 287.323 	Total timesteps: 2273600 	 Percentage complete 56.840
Average episode rewards is 266.125 	Total timesteps: 2281600 	 Percentage complete 57.040
Average episode rewards is 325.466 	Total timesteps: 2289600 	 Percentage complete 57.240
Average episode rewards is 278.343 	Total timesteps: 2297600 	 Percentage complete 57.440
Average episode rewards is 348.890 	Total timesteps: 2305600 	 Percentage complete 57.640
Average episode rewards is 292.282 	Total timesteps: 2313600 	 Percentage complete 57.840
Average episode rewards is 328.873 	Total timesteps: 2321600 	 Percentage complete 58.040
Average episode rewards is 314.918 	Total timesteps: 2329600 	 Percentage complete 58.240
Average episode rewards is 321.509 	Total timesteps: 2337600 	 Percentage complete 58.440
Average episode rewards is 281.639 	Total timesteps: 2345600 	 Percentage complete 58.640
Average episode rewards is 295.597 	Total timesteps: 2353600 	 Percentage complete 58.840
Average episode rewards is 299.946 	Total timesteps: 2361600 	 Percentage complete 59.040
Average episode rewards is 319.687 	Total timesteps: 2369600 	 Percentage complete 59.240
Average episode rewards is 287.561 	Total timesteps: 2377600 	 Percentage complete 59.440
Average episode rewards is 309.429 	Total timesteps: 2385600 	 Percentage complete 59.640
Average episode rewards is 342.174 	Total timesteps: 2393600 	 Percentage complete 59.840
Average episode rewards is 267.403 	Total timesteps: 2401600 	 Percentage complete 60.040
Average episode rewards is 278.500 	Total timesteps: 2409600 	 Percentage complete 60.240
Average episode rewards is 317.307 	Total timesteps: 2417600 	 Percentage complete 60.440
Average episode rewards is 309.588 	Total timesteps: 2425600 	 Percentage complete 60.640
Average episode rewards is 321.084 	Total timesteps: 2433600 	 Percentage complete 60.840
Average episode rewards is 324.184 	Total timesteps: 2441600 	 Percentage complete 61.040
Average episode rewards is 328.424 	Total timesteps: 2449600 	 Percentage complete 61.240
Average episode rewards is 367.230 	Total timesteps: 2457600 	 Percentage complete 61.440
Average episode rewards is 352.470 	Total timesteps: 2465600 	 Percentage complete 61.640
Average episode rewards is 350.941 	Total timesteps: 2473600 	 Percentage complete 61.840
Average episode rewards is 338.216 	Total timesteps: 2481600 	 Percentage complete 62.040
Average episode rewards is 337.900 	Total timesteps: 2489600 	 Percentage complete 62.240
Average episode rewards is 359.271 	Total timesteps: 2497600 	 Percentage complete 62.440
Average episode rewards is 328.041 	Total timesteps: 2505600 	 Percentage complete 62.640
Average episode rewards is 351.759 	Total timesteps: 2513600 	 Percentage complete 62.840
Average episode rewards is 339.660 	Total timesteps: 2521600 	 Percentage complete 63.040
Average episode rewards is 361.908 	Total timesteps: 2529600 	 Percentage complete 63.240
Average episode rewards is 336.302 	Total timesteps: 2537600 	 Percentage complete 63.440
Average episode rewards is 319.087 	Total timesteps: 2545600 	 Percentage complete 63.640
Average episode rewards is 273.777 	Total timesteps: 2553600 	 Percentage complete 63.840
Average episode rewards is 349.363 	Total timesteps: 2561600 	 Percentage complete 64.040
Average episode rewards is 302.142 	Total timesteps: 2569600 	 Percentage complete 64.240
Average episode rewards is 257.924 	Total timesteps: 2577600 	 Percentage complete 64.440
Average episode rewards is 266.109 	Total timesteps: 2585600 	 Percentage complete 64.640
Average episode rewards is 296.271 	Total timesteps: 2593600 	 Percentage complete 64.840
Average episode rewards is 227.909 	Total timesteps: 2601600 	 Percentage complete 65.040
Average episode rewards is 304.884 	Total timesteps: 2609600 	 Percentage complete 65.240
Average episode rewards is 309.414 	Total timesteps: 2617600 	 Percentage complete 65.440
Average episode rewards is 294.654 	Total timesteps: 2625600 	 Percentage complete 65.640
Average episode rewards is 284.905 	Total timesteps: 2633600 	 Percentage complete 65.840
Average episode rewards is 260.432 	Total timesteps: 2641600 	 Percentage complete 66.040
Average episode rewards is 245.255 	Total timesteps: 2649600 	 Percentage complete 66.240
Average episode rewards is 268.115 	Total timesteps: 2657600 	 Percentage complete 66.440
Average episode rewards is 183.217 	Total timesteps: 2665600 	 Percentage complete 66.640
Average episode rewards is 184.374 	Total timesteps: 2673600 	 Percentage complete 66.840
Average episode rewards is 213.891 	Total timesteps: 2681600 	 Percentage complete 67.040
Average episode rewards is 177.927 	Total timesteps: 2689600 	 Percentage complete 67.240
Average episode rewards is 166.180 	Total timesteps: 2697600 	 Percentage complete 67.440
Average episode rewards is 189.157 	Total timesteps: 2705600 	 Percentage complete 67.640
Average episode rewards is 137.662 	Total timesteps: 2713600 	 Percentage complete 67.840
Average episode rewards is 194.422 	Total timesteps: 2721600 	 Percentage complete 68.040
Average episode rewards is 206.614 	Total timesteps: 2729600 	 Percentage complete 68.240
Average episode rewards is 152.557 	Total timesteps: 2737600 	 Percentage complete 68.440
Average episode rewards is 134.908 	Total timesteps: 2745600 	 Percentage complete 68.640
Average episode rewards is 205.931 	Total timesteps: 2753600 	 Percentage complete 68.840
Average episode rewards is 120.072 	Total timesteps: 2761600 	 Percentage complete 69.040
Average episode rewards is 108.445 	Total timesteps: 2769600 	 Percentage complete 69.240
Average episode rewards is 116.825 	Total timesteps: 2777600 	 Percentage complete 69.440
Average episode rewards is 122.372 	Total timesteps: 2785600 	 Percentage complete 69.640
Average episode rewards is 165.639 	Total timesteps: 2793600 	 Percentage complete 69.840
Average episode rewards is 137.750 	Total timesteps: 2801600 	 Percentage complete 70.040
Average episode rewards is 80.538 	Total timesteps: 2809600 	 Percentage complete 70.240
Average episode rewards is 67.615 	Total timesteps: 2817600 	 Percentage complete 70.440
Average episode rewards is 92.427 	Total timesteps: 2825600 	 Percentage complete 70.640
Average episode rewards is 107.660 	Total timesteps: 2833600 	 Percentage complete 70.840
Average episode rewards is 76.579 	Total timesteps: 2841600 	 Percentage complete 71.040
Average episode rewards is 106.910 	Total timesteps: 2849600 	 Percentage complete 71.240
Average episode rewards is 37.223 	Total timesteps: 2857600 	 Percentage complete 71.440
Average episode rewards is 76.679 	Total timesteps: 2865600 	 Percentage complete 71.640
Average episode rewards is 98.656 	Total timesteps: 2873600 	 Percentage complete 71.840
Average episode rewards is 37.754 	Total timesteps: 2881600 	 Percentage complete 72.040
Average episode rewards is 90.778 	Total timesteps: 2889600 	 Percentage complete 72.240
Average episode rewards is 125.344 	Total timesteps: 2897600 	 Percentage complete 72.440
Average episode rewards is 180.117 	Total timesteps: 2905600 	 Percentage complete 72.640
Average episode rewards is 177.520 	Total timesteps: 2913600 	 Percentage complete 72.840
Average episode rewards is 132.514 	Total timesteps: 2921600 	 Percentage complete 73.040
Average episode rewards is 206.656 	Total timesteps: 2929600 	 Percentage complete 73.240
Average episode rewards is 145.551 	Total timesteps: 2937600 	 Percentage complete 73.440
Average episode rewards is 126.572 	Total timesteps: 2945600 	 Percentage complete 73.640
Average episode rewards is 163.254 	Total timesteps: 2953600 	 Percentage complete 73.840
Average episode rewards is 101.693 	Total timesteps: 2961600 	 Percentage complete 74.040
Average episode rewards is 118.849 	Total timesteps: 2969600 	 Percentage complete 74.240
Average episode rewards is 149.140 	Total timesteps: 2977600 	 Percentage complete 74.440
Average episode rewards is 128.305 	Total timesteps: 2985600 	 Percentage complete 74.640
Average episode rewards is 114.551 	Total timesteps: 2993600 	 Percentage complete 74.840
Average episode rewards is 96.235 	Total timesteps: 3001600 	 Percentage complete 75.040
Average episode rewards is 83.050 	Total timesteps: 3009600 	 Percentage complete 75.240
Average episode rewards is 154.752 	Total timesteps: 3017600 	 Percentage complete 75.440
Average episode rewards is 139.693 	Total timesteps: 3025600 	 Percentage complete 75.640
Average episode rewards is 127.495 	Total timesteps: 3033600 	 Percentage complete 75.840
Average episode rewards is 157.679 	Total timesteps: 3041600 	 Percentage complete 76.040
Average episode rewards is 210.478 	Total timesteps: 3049600 	 Percentage complete 76.240
Average episode rewards is 199.247 	Total timesteps: 3057600 	 Percentage complete 76.440
Average episode rewards is 210.812 	Total timesteps: 3065600 	 Percentage complete 76.640
Average episode rewards is 163.507 	Total timesteps: 3073600 	 Percentage complete 76.840
Average episode rewards is 236.082 	Total timesteps: 3081600 	 Percentage complete 77.040
Average episode rewards is 219.313 	Total timesteps: 3089600 	 Percentage complete 77.240
Average episode rewards is 238.536 	Total timesteps: 3097600 	 Percentage complete 77.440
Average episode rewards is 261.505 	Total timesteps: 3105600 	 Percentage complete 77.640
Average episode rewards is 236.078 	Total timesteps: 3113600 	 Percentage complete 77.840
Average episode rewards is 307.623 	Total timesteps: 3121600 	 Percentage complete 78.040
Average episode rewards is 285.548 	Total timesteps: 3129600 	 Percentage complete 78.240
Average episode rewards is 350.006 	Total timesteps: 3137600 	 Percentage complete 78.440
Average episode rewards is 341.975 	Total timesteps: 3145600 	 Percentage complete 78.640
Average episode rewards is 362.395 	Total timesteps: 3153600 	 Percentage complete 78.840
Average episode rewards is 356.738 	Total timesteps: 3161600 	 Percentage complete 79.040
Average episode rewards is 323.295 	Total timesteps: 3169600 	 Percentage complete 79.240
Average episode rewards is 334.109 	Total timesteps: 3177600 	 Percentage complete 79.440
Average episode rewards is 364.994 	Total timesteps: 3185600 	 Percentage complete 79.640
Average episode rewards is 369.297 	Total timesteps: 3193600 	 Percentage complete 79.840
Average episode rewards is 369.806 	Total timesteps: 3201600 	 Percentage complete 80.040
Average episode rewards is 390.135 	Total timesteps: 3209600 	 Percentage complete 80.240
Average episode rewards is 422.297 	Total timesteps: 3217600 	 Percentage complete 80.440
Average episode rewards is 414.068 	Total timesteps: 3225600 	 Percentage complete 80.640
Average episode rewards is 405.540 	Total timesteps: 3233600 	 Percentage complete 80.840
Average episode rewards is 403.500 	Total timesteps: 3241600 	 Percentage complete 81.040
Average episode rewards is 434.665 	Total timesteps: 3249600 	 Percentage complete 81.240
Average episode rewards is 412.107 	Total timesteps: 3257600 	 Percentage complete 81.440
Average episode rewards is 406.311 	Total timesteps: 3265600 	 Percentage complete 81.640
Average episode rewards is 429.169 	Total timesteps: 3273600 	 Percentage complete 81.840
Average episode rewards is 421.590 	Total timesteps: 3281600 	 Percentage complete 82.040
Average episode rewards is 406.421 	Total timesteps: 3289600 	 Percentage complete 82.240
Average episode rewards is 425.484 	Total timesteps: 3297600 	 Percentage complete 82.440
Average episode rewards is 428.818 	Total timesteps: 3305600 	 Percentage complete 82.640
Average episode rewards is 415.019 	Total timesteps: 3313600 	 Percentage complete 82.840
Average episode rewards is 417.394 	Total timesteps: 3321600 	 Percentage complete 83.040
Average episode rewards is 421.197 	Total timesteps: 3329600 	 Percentage complete 83.240
Average episode rewards is 431.189 	Total timesteps: 3337600 	 Percentage complete 83.440
Average episode rewards is 440.419 	Total timesteps: 3345600 	 Percentage complete 83.640
Average episode rewards is 430.558 	Total timesteps: 3353600 	 Percentage complete 83.840
Average episode rewards is 455.294 	Total timesteps: 3361600 	 Percentage complete 84.040
Average episode rewards is 442.234 	Total timesteps: 3369600 	 Percentage complete 84.240
Average episode rewards is 438.336 	Total timesteps: 3377600 	 Percentage complete 84.440
Average episode rewards is 445.810 	Total timesteps: 3385600 	 Percentage complete 84.640
Average episode rewards is 454.608 	Total timesteps: 3393600 	 Percentage complete 84.840
Average episode rewards is 416.739 	Total timesteps: 3401600 	 Percentage complete 85.040
Average episode rewards is 439.538 	Total timesteps: 3409600 	 Percentage complete 85.240
Average episode rewards is 424.780 	Total timesteps: 3417600 	 Percentage complete 85.440
Average episode rewards is 429.598 	Total timesteps: 3425600 	 Percentage complete 85.640
Average episode rewards is 398.132 	Total timesteps: 3433600 	 Percentage complete 85.840
Average episode rewards is 407.868 	Total timesteps: 3441600 	 Percentage complete 86.040
Average episode rewards is 428.173 	Total timesteps: 3449600 	 Percentage complete 86.240
Average episode rewards is 433.248 	Total timesteps: 3457600 	 Percentage complete 86.440
Average episode rewards is 435.760 	Total timesteps: 3465600 	 Percentage complete 86.640
Average episode rewards is 415.491 	Total timesteps: 3473600 	 Percentage complete 86.840
Average episode rewards is 418.299 	Total timesteps: 3481600 	 Percentage complete 87.040
Average episode rewards is 409.313 	Total timesteps: 3489600 	 Percentage complete 87.240
Average episode rewards is 417.910 	Total timesteps: 3497600 	 Percentage complete 87.440
Average episode rewards is 431.794 	Total timesteps: 3505600 	 Percentage complete 87.640
Average episode rewards is 433.492 	Total timesteps: 3513600 	 Percentage complete 87.840
Average episode rewards is 422.916 	Total timesteps: 3521600 	 Percentage complete 88.040
Average episode rewards is 417.101 	Total timesteps: 3529600 	 Percentage complete 88.240
Average episode rewards is 439.847 	Total timesteps: 3537600 	 Percentage complete 88.440
Average episode rewards is 425.142 	Total timesteps: 3545600 	 Percentage complete 88.640
Average episode rewards is 436.563 	Total timesteps: 3553600 	 Percentage complete 88.840
Average episode rewards is 429.509 	Total timesteps: 3561600 	 Percentage complete 89.040
Average episode rewards is 429.052 	Total timesteps: 3569600 	 Percentage complete 89.240
Average episode rewards is 449.208 	Total timesteps: 3577600 	 Percentage complete 89.440
Average episode rewards is 441.599 	Total timesteps: 3585600 	 Percentage complete 89.640
Average episode rewards is 444.893 	Total timesteps: 3593600 	 Percentage complete 89.840
Average episode rewards is 423.731 	Total timesteps: 3601600 	 Percentage complete 90.040
Average episode rewards is 459.047 	Total timesteps: 3609600 	 Percentage complete 90.240
Average episode rewards is 451.771 	Total timesteps: 3617600 	 Percentage complete 90.440
Average episode rewards is 453.357 	Total timesteps: 3625600 	 Percentage complete 90.640
Average episode rewards is 447.326 	Total timesteps: 3633600 	 Percentage complete 90.840
Average episode rewards is 424.929 	Total timesteps: 3641600 	 Percentage complete 91.040
Average episode rewards is 442.913 	Total timesteps: 3649600 	 Percentage complete 91.240
Average episode rewards is 461.684 	Total timesteps: 3657600 	 Percentage complete 91.440
Average episode rewards is 458.286 	Total timesteps: 3665600 	 Percentage complete 91.640
Average episode rewards is 430.574 	Total timesteps: 3673600 	 Percentage complete 91.840
Average episode rewards is 451.096 	Total timesteps: 3681600 	 Percentage complete 92.040
Average episode rewards is 458.462 	Total timesteps: 3689600 	 Percentage complete 92.240
Average episode rewards is 447.620 	Total timesteps: 3697600 	 Percentage complete 92.440
Average episode rewards is 428.542 	Total timesteps: 3705600 	 Percentage complete 92.640
Average episode rewards is 441.181 	Total timesteps: 3713600 	 Percentage complete 92.840
Average episode rewards is 432.425 	Total timesteps: 3721600 	 Percentage complete 93.040
Average episode rewards is 442.632 	Total timesteps: 3729600 	 Percentage complete 93.240
Average episode rewards is 447.462 	Total timesteps: 3737600 	 Percentage complete 93.440
Average episode rewards is 402.832 	Total timesteps: 3745600 	 Percentage complete 93.640
Average episode rewards is 425.175 	Total timesteps: 3753600 	 Percentage complete 93.840
Average episode rewards is 409.412 	Total timesteps: 3761600 	 Percentage complete 94.040
Average episode rewards is 449.685 	Total timesteps: 3769600 	 Percentage complete 94.240
Average episode rewards is 430.254 	Total timesteps: 3777600 	 Percentage complete 94.440
Average episode rewards is 448.325 	Total timesteps: 3785600 	 Percentage complete 94.640
Average episode rewards is 422.813 	Total timesteps: 3793600 	 Percentage complete 94.840
Average episode rewards is 416.576 	Total timesteps: 3801600 	 Percentage complete 95.040
Average episode rewards is 431.450 	Total timesteps: 3809600 	 Percentage complete 95.240
Average episode rewards is 428.531 	Total timesteps: 3817600 	 Percentage complete 95.440
Average episode rewards is 425.655 	Total timesteps: 3825600 	 Percentage complete 95.640
Average episode rewards is 431.693 	Total timesteps: 3833600 	 Percentage complete 95.840
Average episode rewards is 406.042 	Total timesteps: 3841600 	 Percentage complete 96.040
Average episode rewards is 398.017 	Total timesteps: 3849600 	 Percentage complete 96.240
Average episode rewards is 402.258 	Total timesteps: 3857600 	 Percentage complete 96.440
Average episode rewards is 414.342 	Total timesteps: 3865600 	 Percentage complete 96.640
Average episode rewards is 423.581 	Total timesteps: 3873600 	 Percentage complete 96.840
Average episode rewards is 390.287 	Total timesteps: 3881600 	 Percentage complete 97.040
Average episode rewards is 314.549 	Total timesteps: 3889600 	 Percentage complete 97.240
Average episode rewards is 276.123 	Total timesteps: 3897600 	 Percentage complete 97.440
Average episode rewards is 368.373 	Total timesteps: 3905600 	 Percentage complete 97.640
Average episode rewards is 282.936 	Total timesteps: 3913600 	 Percentage complete 97.840
Average episode rewards is 349.866 	Total timesteps: 3921600 	 Percentage complete 98.040
Average episode rewards is 374.609 	Total timesteps: 3929600 	 Percentage complete 98.240
Average episode rewards is 358.624 	Total timesteps: 3937600 	 Percentage complete 98.440
Average episode rewards is 343.850 	Total timesteps: 3945600 	 Percentage complete 98.640
Average episode rewards is 365.265 	Total timesteps: 3953600 	 Percentage complete 98.840
Average episode rewards is 351.433 	Total timesteps: 3961600 	 Percentage complete 99.040
Average episode rewards is 351.491 	Total timesteps: 3969600 	 Percentage complete 99.240
Average episode rewards is 317.390 	Total timesteps: 3977600 	 Percentage complete 99.440
Average episode rewards is 350.785 	Total timesteps: 3985600 	 Percentage complete 99.640
Average episode rewards is 312.573 	Total timesteps: 3993600 	 Percentage complete 99.840
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: \ 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: | 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: / 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: - 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: \ 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                actor_grad_norm â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–†â–…â–ƒâ–‡â–ƒâ–…â–ƒâ–ˆâ–†â–…â–„â–‡â–„â–…â–ƒâ–…â–„â–ƒâ–†â–†â–†â–„â–ƒâ–„â–„â–‚â–ƒâ–ƒâ–ƒ
wandb:            agent0/dist_to_goal â–ˆâ–ƒâ–‚â–„â–„â–„â–„â–ƒâ–„â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–„â–„â–ƒâ–ƒâ–‚â–â–â–â–â–â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:      agent0/individual_rewards â–â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–„â–„â–…â–„â–„â–„â–„â–ƒâ–„â–ƒâ–„â–„â–…â–‡â–‡â–‡â–†â–†â–„â–„â–„â–†â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–‡
wandb:        agent0/min_time_to_goal â–…â–…â–…â–„â–„â–ƒâ–â–ƒâ–„â–…â–†â–…â–„â–…â–„â–‡â–…â–„â–…â–„â–…â–„â–ƒâ–†â–†â–†â–…â–ƒâ–†â–ƒâ–…â–ƒâ–„â–‡â–†â–ƒâ–…â–ƒâ–…â–ˆ
wandb:    agent0/num_agent_collisions â–„â–‚â–ƒâ–„â–ƒâ–â–„â–†â–…â–…â–…â–†â–‚â–„â–„â–„â–‡â–„â–‚â–ƒâ–…â–…â–‚â–„â–ƒâ–„â–‚â–ƒâ–…â–ˆâ–ˆâ–â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–„
wandb: agent0/num_obstacle_collisions â–…â–„â–ˆâ–â–…â–‚â–ƒâ–„â–†â–…â–„â–†â–‡â–ƒâ–…â–‡â–…â–ƒâ–†â–…â–‡â–‚â–‚â–„â–„â–ƒâ–‚â–„â–ƒâ–„â–…â–‚â–ƒâ–‚â–„â–ƒâ–ƒâ–‡â–„â–ƒ
wandb:            agent0/time_to_goal â–ˆâ–†â–…â–†â–†â–†â–†â–…â–†â–„â–…â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–„â–„â–„â–ƒâ–â–â–‚â–‚â–â–â–â–‚â–‚
wandb:            agent1/dist_to_goal â–ˆâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–†â–„â–ƒâ–ƒâ–‚â–â–â–â–â–â–‚â–ƒâ–ƒâ–‚â–â–â–â–â–â–â–â–â–
wandb:      agent1/individual_rewards â–â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–„â–„â–ƒâ–„â–„â–…â–…â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–†â–†â–‡â–†â–†â–„â–„â–„â–…â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡
wandb:        agent1/min_time_to_goal â–‡â–‡â–‚â–ƒâ–ƒâ–…â–„â–‡â–ƒâ–‚â–…â–ƒâ–â–„â–…â–‚â–â–„â–ƒâ–ˆâ–ƒâ–†â–†â–†â–‚â–„â–„â–‚â–…â–ƒâ–„â–ƒâ–â–…â–ƒâ–…â–„â–†â–ƒâ–‚
wandb:    agent1/num_agent_collisions â–„â–„â–„â–â–‚â–„â–ƒâ–‡â–†â–„â–ƒâ–†â–…â–„â–ƒâ–ƒâ–†â–ˆâ–„â–â–ˆâ–…â–‚â–…â–†â–‚â–†â–ƒâ–…â–‚â–…â–„â–ƒâ–„â–…â–…â–ƒâ–„â–ƒâ–‚
wandb: agent1/num_obstacle_collisions â–„â–„â–„â–‚â–„â–‡â–…â–‚â–ˆâ–„â–…â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–…â–ƒâ–ƒâ–…â–â–â–„â–ƒâ–‚â–„â–…â–ƒâ–â–‚â–‚â–…â–‚â–â–‚â–‚
wandb:            agent1/time_to_goal â–ˆâ–†â–…â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–„â–…â–„â–„â–…â–…â–…â–…â–„â–‚â–‚â–â–‚â–‚â–„â–…â–„â–ƒâ–â–â–â–â–â–â–â–â–
wandb:            agent2/dist_to_goal â–ˆâ–ƒâ–‚â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–„â–ƒâ–„â–„â–…â–„â–ƒâ–ƒâ–‚â–â–â–â–â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:      agent2/individual_rewards â–â–ƒâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–„â–…â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–†â–†â–‡â–†â–†â–„â–„â–„â–…â–†â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡
wandb:        agent2/min_time_to_goal â–„â–â–„â–…â–ˆâ–ƒâ–ƒâ–…â–†â–…â–‚â–„â–‡â–‡â–â–ˆâ–‡â–‚â–â–ƒâ–„â–ƒâ–„â–ƒâ–ˆâ–‡â–…â–„â–‡â–…â–„â–…â–†â–‡â–†â–ƒâ–„â–…â–…â–†
wandb:    agent2/num_agent_collisions â–ˆâ–„â–†â–‚â–„â–„â–â–â–ƒâ–…â–‡â–…â–„â–…â–ƒâ–‚â–„â–„â–„â–‚â–…â–„â–â–â–…â–†â–…â–…â–…â–„â–…â–â–…â–‚â–ƒâ–…â–ƒâ–‚â–‚â–„
wandb: agent2/num_obstacle_collisions â–ˆâ–…â–â–…â–…â–…â–†â–‡â–‡â–„â–„â–…â–†â–„â–…â–ƒâ–„â–„â–ƒâ–„â–…â–‚â–…â–„â–‚â–â–ƒâ–ˆâ–ƒâ–„â–‚â–‚â–â–ƒâ–‚â–„â–â–…â–‚â–†
wandb:            agent2/time_to_goal â–ˆâ–†â–…â–†â–†â–†â–†â–…â–…â–„â–…â–„â–„â–…â–„â–…â–„â–…â–…â–…â–…â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–„â–…â–„â–‚â–‚â–â–â–â–â–â–â–â–‚
wandb:            agent3/dist_to_goal â–ˆâ–„â–ƒâ–…â–„â–…â–…â–„â–„â–‚â–‚â–‚â–ƒâ–…â–„â–…â–„â–†â–†â–ƒâ–ƒâ–‚â–â–â–â–â–‚â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:      agent3/individual_rewards â–â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–…â–„â–…â–…â–ƒâ–„â–„â–„â–ƒâ–„â–ƒâ–„â–…â–†â–†â–‡â–†â–†â–„â–„â–…â–…â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡
wandb:        agent3/min_time_to_goal â–…â–‚â–„â–â–…â–„â–‡â–†â–…â–„â–ƒâ–ƒâ–…â–†â–‚â–„â–†â–„â–„â–ƒâ–„â–…â–‚â–ƒâ–ƒâ–„â–ˆâ–ƒâ–ƒâ–„â–„â–„â–„â–†â–„â–„â–ƒâ–„â–„â–†
wandb:    agent3/num_agent_collisions â–…â–„â–…â–†â–ƒâ–†â–„â–„â–„â–ƒâ–‡â–‡â–†â–†â–ƒâ–ƒâ–…â–†â–‚â–†â–ˆâ–„â–ƒâ–‚â–…â–„â–…â–ˆâ–ƒâ–†â–†â–„â–‡â–ƒâ–‚â–â–ƒâ–…â–…â–†
wandb: agent3/num_obstacle_collisions â–†â–ƒâ–‚â–ƒâ–‚â–ƒâ–„â–‚â–…â–„â–ˆâ–…â–„â–„â–„â–…â–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–„â–‚â–ƒâ–â–ƒâ–…â–…â–ƒâ–„â–ƒâ–ƒâ–‚â–„â–ƒâ–ƒâ–â–ƒâ–ƒ
wandb:            agent3/time_to_goal â–ˆâ–†â–…â–†â–†â–…â–†â–…â–†â–„â–…â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–„â–â–‚â–‚â–‚â–ƒâ–„â–„â–„â–ƒâ–‚â–â–â–â–â–â–â–â–‚
wandb:            agent4/dist_to_goal â–ˆâ–ƒâ–ƒâ–…â–„â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–„â–„â–„â–„â–…â–…â–ƒâ–ƒâ–‚â–â–â–â–â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:      agent4/individual_rewards â–â–‚â–„â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–„â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–‡â–†â–‡â–†â–…â–…â–„â–„â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡
wandb:        agent4/min_time_to_goal â–‚â–„â–†â–…â–‡â–ƒâ–…â–ƒâ–…â–†â–‚â–ƒâ–„â–„â–ƒâ–„â–…â–ƒâ–„â–†â–ƒâ–„â–†â–„â–…â–„â–…â–„â–„â–…â–â–ˆâ–…â–‚â–ƒâ–†â–…â–ƒâ–„â–†
wandb:    agent4/num_agent_collisions â–„â–…â–„â–‚â–†â–ƒâ–ƒâ–…â–„â–ˆâ–†â–„â–ƒâ–‚â–„â–‚â–ƒâ–†â–ƒâ–ƒâ–„â–ˆâ–‚â–ƒâ–ƒâ–†â–†â–„â–†â–„â–†â–ƒâ–â–„â–â–ƒâ–„â–‚â–ƒâ–…
wandb: agent4/num_obstacle_collisions â–„â–†â–†â–‚â–ƒâ–†â–†â–„â–‡â–…â–„â–â–ƒâ–…â–…â–„â–‚â–†â–†â–ˆâ–†â–„â–â–ƒâ–‚â–…â–‚â–†â–„â–ˆâ–â–†â–‚â–„â–‚â–„â–‚â–ƒâ–„â–‚
wandb:            agent4/time_to_goal â–ˆâ–†â–…â–†â–†â–†â–†â–…â–…â–„â–„â–ƒâ–„â–…â–…â–…â–…â–…â–…â–…â–…â–„â–‚â–‚â–‚â–‚â–ƒâ–„â–…â–„â–‚â–‚â–‚â–â–â–â–â–â–â–‚
wandb:            agent5/dist_to_goal â–ˆâ–ƒâ–‚â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–„â–„â–„â–„â–…â–„â–ƒâ–ƒâ–‚â–â–â–â–â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:      agent5/individual_rewards â–â–‚â–„â–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–…â–„â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–„â–„â–…â–‡â–†â–‡â–†â–…â–…â–„â–„â–…â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–‡
wandb:        agent5/min_time_to_goal â–†â–†â–…â–‡â–ƒâ–†â–ƒâ–…â–„â–…â–…â–†â–ƒâ–†â–ˆâ–‚â–„â–‡â–ƒâ–…â–‡â–ˆâ–†â–†â–„â–‡â–„â–ˆâ–„â–ˆâ–‚â–â–‡â–„â–ƒâ–…â–ˆâ–„â–ƒâ–†
wandb:    agent5/num_agent_collisions â–†â–†â–„â–…â–ƒâ–„â–…â–…â–ˆâ–„â–„â–†â–ƒâ–„â–…â–ƒâ–„â–‡â–„â–ƒâ–…â–„â–ƒâ–‚â–â–…â–…â–…â–‡â–†â–†â–ƒâ–„â–ƒâ–ƒâ–„â–†â–„â–„â–…
wandb: agent5/num_obstacle_collisions â–ƒâ–‡â–†â–…â–‚â–†â–‡â–…â–ƒâ–ƒâ–‡â–ƒâ–†â–…â–†â–…â–ƒâ–ƒâ–„â–ˆâ–‡â–ƒâ–‡â–ƒâ–„â–…â–ƒâ–ˆâ–„â–…â–„â–…â–…â–â–ƒâ–â–â–‚â–ƒâ–„
wandb:            agent5/time_to_goal â–ˆâ–†â–…â–†â–‡â–†â–†â–…â–…â–„â–„â–„â–ƒâ–„â–…â–…â–„â–…â–…â–…â–†â–„â–‚â–‚â–‚â–‚â–‚â–„â–„â–„â–ƒâ–â–‚â–â–â–â–‚â–â–â–‚
wandb:            agent6/dist_to_goal â–ˆâ–ƒâ–ƒâ–„â–„â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–„â–ƒâ–…â–„â–…â–„â–ƒâ–ƒâ–‚â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:      agent6/individual_rewards â–â–ƒâ–„â–„â–ƒâ–ƒâ–„â–„â–„â–„â–ƒâ–„â–„â–„â–„â–ƒâ–„â–ƒâ–„â–„â–„â–„â–†â–†â–‡â–…â–†â–…â–„â–„â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡
wandb:        agent6/min_time_to_goal â–†â–…â–…â–…â–ƒâ–…â–ˆâ–…â–‚â–…â–ƒâ–ˆâ–ƒâ–‚â–‡â–‡â–…â–†â–…â–â–†â–†â–„â–„â–ƒâ–ˆâ–„â–†â–‚â–…â–„â–…â–†â–…â–…â–„â–ˆâ–…â–‚â–ˆ
wandb:    agent6/num_agent_collisions â–†â–„â–„â–â–â–‡â–„â–†â–‡â–…â–„â–‡â–„â–„â–…â–ƒâ–†â–ˆâ–…â–ƒâ–ˆâ–…â–„â–…â–…â–„â–„â–†â–…â–ƒâ–„â–…â–…â–„â–…â–ƒâ–…â–…â–ƒâ–„
wandb: agent6/num_obstacle_collisions â–‡â–…â–„â–‡â–…â–†â–…â–…â–„â–â–‡â–†â–ƒâ–‚â–‚â–ƒâ–‡â–…â–†â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–…â–ƒâ–ˆâ–‡â–†â–ƒâ–„â–„â–ƒâ–‚â–ƒâ–ƒâ–„â–‚â–ƒ
wandb:            agent6/time_to_goal â–ˆâ–†â–…â–…â–†â–†â–†â–…â–…â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–„â–‚â–‚â–â–‚â–ƒâ–„â–„â–„â–ƒâ–‚â–â–â–â–â–â–â–â–‚
wandb:        average_episode_rewards â–â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–„â–…â–„â–„â–„â–„â–ƒâ–ƒâ–„â–ƒâ–…â–†â–†â–‡â–†â–†â–„â–„â–„â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡
wandb:               critic_grad_norm â–ˆâ–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–
wandb:                   dist_entropy â–ˆâ–…â–„â–…â–…â–…â–…â–„â–„â–ƒâ–„â–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–„â–…â–„â–‚â–‚â–‚â–‚â–ƒâ–„â–„â–„â–ƒâ–‚â–‚â–â–â–â–â–â–â–
wandb:                    policy_loss â–…â–â–â–‚â–„â–„â–…â–ƒâ–…â–„â–…â–…â–…â–…â–‡â–…â–…â–…â–‡â–†â–†â–…â–ˆâ–…â–…â–„â–„â–„â–„â–†â–†â–…â–…â–„â–…â–…â–…â–…â–…â–…
wandb:                          ratio â–„â–ƒâ–…â–„â–„â–…â–‡â–„â–†â–ƒâ–„â–…â–…â–ˆâ–ˆâ–†â–†â–‡â–‡â–…â–‡â–†â–†â–‚â–…â–‡â–…â–…â–†â–†â–‡â–â–ƒâ–‚â–„â–‚â–‚â–â–ƒâ–…
wandb:                     value_loss â–†â–†â–…â–„â–‚â–ƒâ–ƒâ–„â–…â–„â–ƒâ–„â–…â–„â–„â–„â–…â–„â–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–ƒâ–‚â–†â–ƒâ–ƒâ–ƒâ–‚â–ˆâ–‚â–‚â–â–â–â–â–â–‚
wandb: 
wandb: Run summary:
wandb:                actor_grad_norm 1.03779
wandb:            agent0/dist_to_goal 0.06123
wandb:      agent0/individual_rewards 3.1603
wandb:        agent0/min_time_to_goal 0.46247
wandb:    agent0/num_agent_collisions 0.84375
wandb: agent0/num_obstacle_collisions 0.42188
wandb:            agent0/time_to_goal 1.125
wandb:            agent1/dist_to_goal 0.05191
wandb:      agent1/individual_rewards 3.24679
wandb:        agent1/min_time_to_goal 0.50769
wandb:    agent1/num_agent_collisions 0.95312
wandb: agent1/num_obstacle_collisions 0.54688
wandb:            agent1/time_to_goal 1.125
wandb:            agent2/dist_to_goal 0.05999
wandb:      agent2/individual_rewards 3.0046
wandb:        agent2/min_time_to_goal 0.48906
wandb:    agent2/num_agent_collisions 0.78125
wandb: agent2/num_obstacle_collisions 0.32812
wandb:            agent2/time_to_goal 1.16875
wandb:            agent3/dist_to_goal 0.06625
wandb:      agent3/individual_rewards 2.68554
wandb:        agent3/min_time_to_goal 0.50361
wandb:    agent3/num_agent_collisions 0.92188
wandb: agent3/num_obstacle_collisions 0.29688
wandb:            agent3/time_to_goal 1.19687
wandb:            agent4/dist_to_goal 0.05783
wandb:      agent4/individual_rewards 3.08435
wandb:        agent4/min_time_to_goal 0.46044
wandb:    agent4/num_agent_collisions 1.07812
wandb: agent4/num_obstacle_collisions 0.21875
wandb:            agent4/time_to_goal 1.09688
wandb:            agent5/dist_to_goal 0.0577
wandb:      agent5/individual_rewards 3.08407
wandb:        agent5/min_time_to_goal 0.43059
wandb:    agent5/num_agent_collisions 0.96875
wandb: agent5/num_obstacle_collisions 0.375
wandb:            agent5/time_to_goal 1.02812
wandb:            agent6/dist_to_goal 0.05113
wandb:      agent6/individual_rewards 3.16913
wandb:        agent6/min_time_to_goal 0.46749
wandb:    agent6/num_agent_collisions 1.10938
wandb: agent6/num_obstacle_collisions 0.48438
wandb:            agent6/time_to_goal 1.14531
wandb:        average_episode_rewards 312.57334
wandb:               critic_grad_norm 0.452
wandb:                   dist_entropy 0.47306
wandb:                    policy_loss -0.00317
wandb:                          ratio 1.00224
wandb:                     value_loss 0.07739
wandb: 
wandb: ðŸš€ View run rmappo_informarl_seed100 at: https://wandb.ai/golde/enemy/runs/50rjjkxp
wandb: ï¸âš¡ View job at https://wandb.ai/golde/enemy/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjM2MjUwMjIwOQ==/version_details/v2
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 3 other file(s)
wandb: Find logs at: ./onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240804_151934-50rjjkxp/logs
Initializing navigation with fixed parameters
__________________________________________________
Choose to use gpu...
__________________________________________________
__________________________________________________
||                  Arguments                   ||
__________________________________________________
|| algorithm_name: rmappo                       ||
|| project_name: informarl                      ||
|| experiment_name: informarl                   ||
|| seed: 0                                      ||
|| cuda: True                                   ||
|| cuda_deterministic: True                     ||
|| n_training_threads: 1                        ||
|| n_rollout_threads: 64                        ||
|| n_eval_rollout_threads: 1                    ||
|| n_render_rollout_threads: 1                  ||
|| num_env_steps: 4000000                       ||
|| user_name: marl                              ||
|| use_wandb: True                              ||
|| env_name: GraphMPE                           ||
|| use_obs_instead_of_state: False              ||
|| world_size: 2                                ||
|| num_scripted_agents: 0                       ||
|| obs_type: global                             ||
|| max_edge_dist: 1                             ||
|| num_nbd_entities: 3                          ||
|| use_comm: False                              ||
|| episode_length: 25                           ||
|| share_policy: True                           ||
|| use_centralized_V: True                      ||
|| stacked_frames: 1                            ||
|| use_stacked_frames: False                    ||
|| hidden_size: 64                              ||
|| layer_N: 1                                   ||
|| use_ReLU: False                              ||
|| use_popart: True                             ||
|| use_valuenorm: False                         ||
|| use_feature_normalization: True              ||
|| use_orthogonal: True                         ||
|| gain: 0.01                                   ||
|| split_batch: False                           ||
|| max_batch_size: 32                           ||
|| use_naive_recurrent_policy: False            ||
|| use_recurrent_policy: True                   ||
|| recurrent_N: 1                               ||
|| data_chunk_length: 10                        ||
|| lr: 0.0007                                   ||
|| critic_lr: 0.0007                            ||
|| opti_eps: 1e-05                              ||
|| weight_decay: 0                              ||
|| ppo_epoch: 10                                ||
|| use_clipped_value_loss: True                 ||
|| clip_param: 0.2                              ||
|| num_mini_batch: 1                            ||
|| entropy_coef: 0.01                           ||
|| value_loss_coef: 1                           ||
|| use_max_grad_norm: True                      ||
|| max_grad_norm: 10.0                          ||
|| use_gae: True                                ||
|| gamma: 0.99                                  ||
|| gae_lambda: 0.95                             ||
|| use_proper_time_limits: False                ||
|| use_huber_loss: True                         ||
|| use_value_active_masks: True                 ||
|| use_policy_active_masks: True                ||
|| huber_delta: 10.0                            ||
|| use_linear_lr_decay: False                   ||
|| save_interval: 1                             ||
|| log_interval: 5                              ||
|| use_eval: False                              ||
|| eval_interval: 25                            ||
|| eval_episodes: 32                            ||
|| save_gifs: False                             ||
|| use_render: False                            ||
|| render_episodes: 5                           ||
|| ifi: 0.1                                     ||
|| render_eval: False                           ||
|| model_dir: None                              ||
|| verbose: True                                ||
|| scenario_name: navigation_graph              ||
|| num_landmarks: 3                             ||
|| num_agents: 7                                ||
|| num_obstacles: 3                             ||
|| reward_sparsity: 1                           ||
|| collaborative: True                          ||
|| max_speed: 2                                 ||
|| collision_rew: 5.0                           ||
|| goal_rew: 5                                  ||
|| min_dist_thresh: 0.05                        ||
|| use_dones: False                             ||
|| num_embeddings: 3                            ||
|| embedding_size: 2                            ||
|| embed_hidden_size: 16                        ||
|| embed_layer_N: 1                             ||
|| embed_use_ReLU: True                         ||
|| embed_add_self_loop: False                   ||
|| gnn_hidden_size: 16                          ||
|| gnn_num_heads: 3                             ||
|| gnn_concat_heads: False                      ||
|| gnn_layer_N: 2                               ||
|| gnn_use_ReLU: True                           ||
|| graph_feat_type: relative                    ||
|| actor_graph_aggr: node                       ||
|| critic_graph_aggr: global                    ||
|| global_aggr_type: mean                       ||
|| use_cent_obs: False                          ||
|| auto_mini_batch_size: False                  ||
|| target_mini_batch_size: 32                   ||
__________________________________________________
__________________________________________________
Creating wandboard...
__________________________________________________
wandb: Currently logged in as: sharlinu (golde). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.11
wandb: Run data is saved locally in /home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240808_143105-ngtkh65y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rmappo_informarl_seed0
wandb: â­ï¸ View project at https://wandb.ai/golde/enemy
wandb: ðŸš€ View run at https://wandb.ai/golde/enemy/runs/ngtkh65y
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenariocreated scenario
Initialising graph navigation

Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenariocreated scenario
Initialising graph navigation

Initialising graph navigation
created scenario
Initialising graph navigation
Overriding Observation dimension
Overriding Observation dimension
________________________________________________________________________________
Actor Network
________________________________________________________________________________
________________________________________________________________________________
GR_Actor(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0): TransformerConv(16, 16, heads=3)
        (1): TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((22,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=22, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (act): ACTLayer(
    (action_out): Categorical(
      (linear): Linear(in_features=64, out_features=5, bias=True)
    )
  )
)
________________________________________________________________________________
________________________________________________________________________________
Critic Network
________________________________________________________________________________
________________________________________________________________________________
GR_Critic(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0): TransformerConv(16, 16, heads=3)
        (1): TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (v_out): PopArt()
)
________________________________________________________________________________
Average episode rewards is -207.625 	Total timesteps: 1600 	 Percentage complete 0.040
Average episode rewards is -203.342 	Total timesteps: 9600 	 Percentage complete 0.240
Average episode rewards is -183.686 	Total timesteps: 17600 	 Percentage complete 0.440
Average episode rewards is -166.977 	Total timesteps: 25600 	 Percentage complete 0.640
Average episode rewards is -159.801 	Total timesteps: 33600 	 Percentage complete 0.840
Average episode rewards is -157.281 	Total timesteps: 41600 	 Percentage complete 1.040
Average episode rewards is -143.159 	Total timesteps: 49600 	 Percentage complete 1.240
Average episode rewards is -121.507 	Total timesteps: 57600 	 Percentage complete 1.440
Average episode rewards is -105.356 	Total timesteps: 65600 	 Percentage complete 1.640
Average episode rewards is -94.216 	Total timesteps: 73600 	 Percentage complete 1.840
Average episode rewards is -86.999 	Total timesteps: 81600 	 Percentage complete 2.040
Average episode rewards is -47.890 	Total timesteps: 89600 	 Percentage complete 2.240
Average episode rewards is -36.516 	Total timesteps: 97600 	 Percentage complete 2.440
Average episode rewards is -13.732 	Total timesteps: 105600 	 Percentage complete 2.640
Average episode rewards is 7.702 	Total timesteps: 113600 	 Percentage complete 2.840
Average episode rewards is 26.243 	Total timesteps: 121600 	 Percentage complete 3.040
Average episode rewards is 46.254 	Total timesteps: 129600 	 Percentage complete 3.240
Average episode rewards is 48.825 	Total timesteps: 137600 	 Percentage complete 3.440
Average episode rewards is 72.005 	Total timesteps: 145600 	 Percentage complete 3.640
Average episode rewards is 79.624 	Total timesteps: 153600 	 Percentage complete 3.840
Average episode rewards is 89.193 	Total timesteps: 161600 	 Percentage complete 4.040
Average episode rewards is 107.837 	Total timesteps: 169600 	 Percentage complete 4.240
Average episode rewards is 125.460 	Total timesteps: 177600 	 Percentage complete 4.440
Average episode rewards is 136.207 	Total timesteps: 185600 	 Percentage complete 4.640
Average episode rewards is 137.632 	Total timesteps: 193600 	 Percentage complete 4.840
Average episode rewards is 150.761 	Total timesteps: 201600 	 Percentage complete 5.040
Average episode rewards is 150.276 	Total timesteps: 209600 	 Percentage complete 5.240
Average episode rewards is 166.505 	Total timesteps: 217600 	 Percentage complete 5.440
Average episode rewards is 192.478 	Total timesteps: 225600 	 Percentage complete 5.640
Average episode rewards is 187.028 	Total timesteps: 233600 	 Percentage complete 5.840
Average episode rewards is 185.055 	Total timesteps: 241600 	 Percentage complete 6.040
Average episode rewards is 209.388 	Total timesteps: 249600 	 Percentage complete 6.240
Average episode rewards is 202.929 	Total timesteps: 257600 	 Percentage complete 6.440
Average episode rewards is 239.305 	Total timesteps: 265600 	 Percentage complete 6.640
Average episode rewards is 241.599 	Total timesteps: 273600 	 Percentage complete 6.840
Average episode rewards is 227.580 	Total timesteps: 281600 	 Percentage complete 7.040
Average episode rewards is 241.829 	Total timesteps: 289600 	 Percentage complete 7.240
Average episode rewards is 255.431 	Total timesteps: 297600 	 Percentage complete 7.440
Average episode rewards is 254.186 	Total timesteps: 305600 	 Percentage complete 7.640
Average episode rewards is 256.593 	Total timesteps: 313600 	 Percentage complete 7.840
Average episode rewards is 274.754 	Total timesteps: 321600 	 Percentage complete 8.040
Average episode rewards is 269.567 	Total timesteps: 329600 	 Percentage complete 8.240
Average episode rewards is 268.178 	Total timesteps: 337600 	 Percentage complete 8.440
Average episode rewards is 278.691 	Total timesteps: 345600 	 Percentage complete 8.640
Average episode rewards is 254.547 	Total timesteps: 353600 	 Percentage complete 8.840
Average episode rewards is 251.661 	Total timesteps: 361600 	 Percentage complete 9.040
Average episode rewards is 238.789 	Total timesteps: 369600 	 Percentage complete 9.240
Average episode rewards is 241.002 	Total timesteps: 377600 	 Percentage complete 9.440
Average episode rewards is 253.021 	Total timesteps: 385600 	 Percentage complete 9.640
Average episode rewards is 239.181 	Total timesteps: 393600 	 Percentage complete 9.840
Average episode rewards is 242.579 	Total timesteps: 401600 	 Percentage complete 10.040
Average episode rewards is 245.842 	Total timesteps: 409600 	 Percentage complete 10.240
Average episode rewards is 258.149 	Total timesteps: 417600 	 Percentage complete 10.440
Average episode rewards is 255.588 	Total timesteps: 425600 	 Percentage complete 10.640
Average episode rewards is 276.952 	Total timesteps: 433600 	 Percentage complete 10.840
Average episode rewards is 254.690 	Total timesteps: 441600 	 Percentage complete 11.040
Average episode rewards is 259.003 	Total timesteps: 449600 	 Percentage complete 11.240
Average episode rewards is 282.490 	Total timesteps: 457600 	 Percentage complete 11.440
Average episode rewards is 280.871 	Total timesteps: 465600 	 Percentage complete 11.640
Average episode rewards is 279.360 	Total timesteps: 473600 	 Percentage complete 11.840
Average episode rewards is 285.719 	Total timesteps: 481600 	 Percentage complete 12.040
Average episode rewards is 278.155 	Total timesteps: 489600 	 Percentage complete 12.240
Average episode rewards is 298.449 	Total timesteps: 497600 	 Percentage complete 12.440
Average episode rewards is 300.326 	Total timesteps: 505600 	 Percentage complete 12.640
Average episode rewards is 298.667 	Total timesteps: 513600 	 Percentage complete 12.840
Average episode rewards is 289.794 	Total timesteps: 521600 	 Percentage complete 13.040
Average episode rewards is 266.100 	Total timesteps: 529600 	 Percentage complete 13.240
Average episode rewards is 299.268 	Total timesteps: 537600 	 Percentage complete 13.440
Average episode rewards is 304.835 	Total timesteps: 545600 	 Percentage complete 13.640
Average episode rewards is 310.746 	Total timesteps: 553600 	 Percentage complete 13.840
Average episode rewards is 326.322 	Total timesteps: 561600 	 Percentage complete 14.040
Average episode rewards is 322.780 	Total timesteps: 569600 	 Percentage complete 14.240
Average episode rewards is 313.324 	Total timesteps: 577600 	 Percentage complete 14.440
Average episode rewards is 327.903 	Total timesteps: 585600 	 Percentage complete 14.640
Average episode rewards is 309.883 	Total timesteps: 593600 	 Percentage complete 14.840
Average episode rewards is 326.415 	Total timesteps: 601600 	 Percentage complete 15.040
Average episode rewards is 297.338 	Total timesteps: 609600 	 Percentage complete 15.240
Average episode rewards is 292.645 	Total timesteps: 617600 	 Percentage complete 15.440
Average episode rewards is 309.406 	Total timesteps: 625600 	 Percentage complete 15.640
Average episode rewards is 319.775 	Total timesteps: 633600 	 Percentage complete 15.840
Average episode rewards is 344.883 	Total timesteps: 641600 	 Percentage complete 16.040
Average episode rewards is 338.304 	Total timesteps: 649600 	 Percentage complete 16.240
Average episode rewards is 323.431 	Total timesteps: 657600 	 Percentage complete 16.440
Average episode rewards is 326.515 	Total timesteps: 665600 	 Percentage complete 16.640
Average episode rewards is 329.268 	Total timesteps: 673600 	 Percentage complete 16.840
Average episode rewards is 324.928 	Total timesteps: 681600 	 Percentage complete 17.040
Average episode rewards is 312.220 	Total timesteps: 689600 	 Percentage complete 17.240
Average episode rewards is 335.875 	Total timesteps: 697600 	 Percentage complete 17.440
Average episode rewards is 350.411 	Total timesteps: 705600 	 Percentage complete 17.640
Average episode rewards is 332.029 	Total timesteps: 713600 	 Percentage complete 17.840
Average episode rewards is 289.782 	Total timesteps: 721600 	 Percentage complete 18.040
Average episode rewards is 266.323 	Total timesteps: 729600 	 Percentage complete 18.240
Average episode rewards is 311.521 	Total timesteps: 737600 	 Percentage complete 18.440
Average episode rewards is 316.450 	Total timesteps: 745600 	 Percentage complete 18.640
Average episode rewards is 264.418 	Total timesteps: 753600 	 Percentage complete 18.840
Average episode rewards is 288.670 	Total timesteps: 761600 	 Percentage complete 19.040
Average episode rewards is 302.347 	Total timesteps: 769600 	 Percentage complete 19.240
Average episode rewards is 303.960 	Total timesteps: 777600 	 Percentage complete 19.440
Average episode rewards is 288.483 	Total timesteps: 785600 	 Percentage complete 19.640
Average episode rewards is 307.343 	Total timesteps: 793600 	 Percentage complete 19.840
Average episode rewards is 296.730 	Total timesteps: 801600 	 Percentage complete 20.040
Average episode rewards is 288.723 	Total timesteps: 809600 	 Percentage complete 20.240
Average episode rewards is 279.556 	Total timesteps: 817600 	 Percentage complete 20.440
Average episode rewards is 294.481 	Total timesteps: 825600 	 Percentage complete 20.640
Average episode rewards is 340.537 	Total timesteps: 833600 	 Percentage complete 20.840
Average episode rewards is 299.178 	Total timesteps: 841600 	 Percentage complete 21.040
Average episode rewards is 296.265 	Total timesteps: 849600 	 Percentage complete 21.240
Average episode rewards is 298.750 	Total timesteps: 857600 	 Percentage complete 21.440
Average episode rewards is 285.758 	Total timesteps: 865600 	 Percentage complete 21.640
Average episode rewards is 281.412 	Total timesteps: 873600 	 Percentage complete 21.840
Average episode rewards is 256.455 	Total timesteps: 881600 	 Percentage complete 22.040
Average episode rewards is 284.566 	Total timesteps: 889600 	 Percentage complete 22.240
Average episode rewards is 310.887 	Total timesteps: 897600 	 Percentage complete 22.440
Average episode rewards is 276.451 	Total timesteps: 905600 	 Percentage complete 22.640
Average episode rewards is 265.157 	Total timesteps: 913600 	 Percentage complete 22.840
Average episode rewards is 246.294 	Total timesteps: 921600 	 Percentage complete 23.040
Average episode rewards is 247.420 	Total timesteps: 929600 	 Percentage complete 23.240
Average episode rewards is 279.411 	Total timesteps: 937600 	 Percentage complete 23.440
Average episode rewards is 302.775 	Total timesteps: 945600 	 Percentage complete 23.640
Average episode rewards is 287.540 	Total timesteps: 953600 	 Percentage complete 23.840
Average episode rewards is 311.167 	Total timesteps: 961600 	 Percentage complete 24.040
Average episode rewards is 300.728 	Total timesteps: 969600 	 Percentage complete 24.240
Average episode rewards is 294.845 	Total timesteps: 977600 	 Percentage complete 24.440
Average episode rewards is 236.508 	Total timesteps: 985600 	 Percentage complete 24.640
Average episode rewards is 271.779 	Total timesteps: 993600 	 Percentage complete 24.840
Average episode rewards is 303.881 	Total timesteps: 1001600 	 Percentage complete 25.040
Average episode rewards is 295.491 	Total timesteps: 1009600 	 Percentage complete 25.240
Average episode rewards is 308.710 	Total timesteps: 1017600 	 Percentage complete 25.440
Average episode rewards is 285.301 	Total timesteps: 1025600 	 Percentage complete 25.640
Average episode rewards is 298.571 	Total timesteps: 1033600 	 Percentage complete 25.840
Average episode rewards is 288.920 	Total timesteps: 1041600 	 Percentage complete 26.040
Average episode rewards is 288.840 	Total timesteps: 1049600 	 Percentage complete 26.240
Average episode rewards is 272.302 	Total timesteps: 1057600 	 Percentage complete 26.440
Average episode rewards is 287.684 	Total timesteps: 1065600 	 Percentage complete 26.640
Average episode rewards is 303.007 	Total timesteps: 1073600 	 Percentage complete 26.840
Average episode rewards is 260.311 	Total timesteps: 1081600 	 Percentage complete 27.040
Average episode rewards is 261.976 	Total timesteps: 1089600 	 Percentage complete 27.240
Average episode rewards is 264.899 	Total timesteps: 1097600 	 Percentage complete 27.440
Average episode rewards is 286.153 	Total timesteps: 1105600 	 Percentage complete 27.640
Average episode rewards is 293.917 	Total timesteps: 1113600 	 Percentage complete 27.840
Average episode rewards is 290.298 	Total timesteps: 1121600 	 Percentage complete 28.040
Average episode rewards is 295.249 	Total timesteps: 1129600 	 Percentage complete 28.240
Average episode rewards is 291.918 	Total timesteps: 1137600 	 Percentage complete 28.440
Average episode rewards is 288.467 	Total timesteps: 1145600 	 Percentage complete 28.640
Average episode rewards is 284.624 	Total timesteps: 1153600 	 Percentage complete 28.840
Average episode rewards is 290.255 	Total timesteps: 1161600 	 Percentage complete 29.040
Average episode rewards is 296.208 	Total timesteps: 1169600 	 Percentage complete 29.240
Average episode rewards is 283.996 	Total timesteps: 1177600 	 Percentage complete 29.440
Average episode rewards is 286.617 	Total timesteps: 1185600 	 Percentage complete 29.640
Average episode rewards is 293.577 	Total timesteps: 1193600 	 Percentage complete 29.840
Average episode rewards is 319.291 	Total timesteps: 1201600 	 Percentage complete 30.040
Average episode rewards is 341.817 	Total timesteps: 1209600 	 Percentage complete 30.240
Average episode rewards is 297.090 	Total timesteps: 1217600 	 Percentage complete 30.440
Average episode rewards is 306.779 	Total timesteps: 1225600 	 Percentage complete 30.640
Average episode rewards is 298.984 	Total timesteps: 1233600 	 Percentage complete 30.840
Average episode rewards is 301.394 	Total timesteps: 1241600 	 Percentage complete 31.040
Average episode rewards is 288.368 	Total timesteps: 1249600 	 Percentage complete 31.240
Average episode rewards is 284.217 	Total timesteps: 1257600 	 Percentage complete 31.440
Average episode rewards is 273.414 	Total timesteps: 1265600 	 Percentage complete 31.640
Average episode rewards is 280.468 	Total timesteps: 1273600 	 Percentage complete 31.840
Average episode rewards is 292.742 	Total timesteps: 1281600 	 Percentage complete 32.040
Average episode rewards is 293.088 	Total timesteps: 1289600 	 Percentage complete 32.240
Average episode rewards is 295.626 	Total timesteps: 1297600 	 Percentage complete 32.440
Average episode rewards is 277.721 	Total timesteps: 1305600 	 Percentage complete 32.640
Average episode rewards is 317.037 	Total timesteps: 1313600 	 Percentage complete 32.840
Average episode rewards is 288.090 	Total timesteps: 1321600 	 Percentage complete 33.040
Average episode rewards is 255.885 	Total timesteps: 1329600 	 Percentage complete 33.240
Average episode rewards is 255.592 	Total timesteps: 1337600 	 Percentage complete 33.440
Average episode rewards is 290.793 	Total timesteps: 1345600 	 Percentage complete 33.640
Average episode rewards is 314.508 	Total timesteps: 1353600 	 Percentage complete 33.840
Average episode rewards is 301.373 	Total timesteps: 1361600 	 Percentage complete 34.040
Average episode rewards is 320.082 	Total timesteps: 1369600 	 Percentage complete 34.240
Average episode rewards is 293.566 	Total timesteps: 1377600 	 Percentage complete 34.440
Average episode rewards is 318.894 	Total timesteps: 1385600 	 Percentage complete 34.640
Average episode rewards is 327.539 	Total timesteps: 1393600 	 Percentage complete 34.840
Average episode rewards is 315.726 	Total timesteps: 1401600 	 Percentage complete 35.040
Average episode rewards is 273.806 	Total timesteps: 1409600 	 Percentage complete 35.240
Average episode rewards is 322.561 	Total timesteps: 1417600 	 Percentage complete 35.440
Average episode rewards is 308.457 	Total timesteps: 1425600 	 Percentage complete 35.640
Average episode rewards is 320.993 	Total timesteps: 1433600 	 Percentage complete 35.840
Average episode rewards is 300.471 	Total timesteps: 1441600 	 Percentage complete 36.040
Average episode rewards is 323.818 	Total timesteps: 1449600 	 Percentage complete 36.240
Average episode rewards is 318.575 	Total timesteps: 1457600 	 Percentage complete 36.440
Average episode rewards is 275.288 	Total timesteps: 1465600 	 Percentage complete 36.640
Average episode rewards is 261.937 	Total timesteps: 1473600 	 Percentage complete 36.840
Average episode rewards is 241.648 	Total timesteps: 1481600 	 Percentage complete 37.040
Average episode rewards is 235.803 	Total timesteps: 1489600 	 Percentage complete 37.240
Average episode rewards is 277.401 	Total timesteps: 1497600 	 Percentage complete 37.440
Average episode rewards is 262.720 	Total timesteps: 1505600 	 Percentage complete 37.640
Average episode rewards is 268.794 	Total timesteps: 1513600 	 Percentage complete 37.840
Average episode rewards is 250.301 	Total timesteps: 1521600 	 Percentage complete 38.040
Average episode rewards is 256.921 	Total timesteps: 1529600 	 Percentage complete 38.240
Average episode rewards is 265.484 	Total timesteps: 1537600 	 Percentage complete 38.440
Average episode rewards is 290.137 	Total timesteps: 1545600 	 Percentage complete 38.640
Average episode rewards is 261.285 	Total timesteps: 1553600 	 Percentage complete 38.840
Average episode rewards is 273.485 	Total timesteps: 1561600 	 Percentage complete 39.040
Average episode rewards is 284.630 	Total timesteps: 1569600 	 Percentage complete 39.240
Average episode rewards is 283.246 	Total timesteps: 1577600 	 Percentage complete 39.440
Average episode rewards is 305.302 	Total timesteps: 1585600 	 Percentage complete 39.640
Average episode rewards is 275.128 	Total timesteps: 1593600 	 Percentage complete 39.840
Average episode rewards is 288.721 	Total timesteps: 1601600 	 Percentage complete 40.040
Average episode rewards is 286.738 	Total timesteps: 1609600 	 Percentage complete 40.240
Average episode rewards is 268.903 	Total timesteps: 1617600 	 Percentage complete 40.440
Average episode rewards is 288.449 	Total timesteps: 1625600 	 Percentage complete 40.640
Average episode rewards is 299.021 	Total timesteps: 1633600 	 Percentage complete 40.840
Average episode rewards is 303.786 	Total timesteps: 1641600 	 Percentage complete 41.040
Average episode rewards is 311.219 	Total timesteps: 1649600 	 Percentage complete 41.240
Average episode rewards is 322.415 	Total timesteps: 1657600 	 Percentage complete 41.440
Average episode rewards is 275.379 	Total timesteps: 1665600 	 Percentage complete 41.640
Average episode rewards is 284.695 	Total timesteps: 1673600 	 Percentage complete 41.840
Average episode rewards is 302.990 	Total timesteps: 1681600 	 Percentage complete 42.040
Average episode rewards is 317.306 	Total timesteps: 1689600 	 Percentage complete 42.240
Average episode rewards is 301.647 	Total timesteps: 1697600 	 Percentage complete 42.440
Average episode rewards is 299.823 	Total timesteps: 1705600 	 Percentage complete 42.640
Average episode rewards is 305.041 	Total timesteps: 1713600 	 Percentage complete 42.840
Average episode rewards is 317.260 	Total timesteps: 1721600 	 Percentage complete 43.040
Average episode rewards is 291.076 	Total timesteps: 1729600 	 Percentage complete 43.240
Average episode rewards is 280.494 	Total timesteps: 1737600 	 Percentage complete 43.440
Average episode rewards is 295.470 	Total timesteps: 1745600 	 Percentage complete 43.640
Average episode rewards is 307.458 	Total timesteps: 1753600 	 Percentage complete 43.840
Average episode rewards is 318.644 	Total timesteps: 1761600 	 Percentage complete 44.040
Average episode rewards is 308.723 	Total timesteps: 1769600 	 Percentage complete 44.240
Average episode rewards is 301.513 	Total timesteps: 1777600 	 Percentage complete 44.440
Average episode rewards is 275.467 	Total timesteps: 1785600 	 Percentage complete 44.640
Average episode rewards is 307.392 	Total timesteps: 1793600 	 Percentage complete 44.840
Average episode rewards is 320.623 	Total timesteps: 1801600 	 Percentage complete 45.040
Average episode rewards is 298.104 	Total timesteps: 1809600 	 Percentage complete 45.240
Average episode rewards is 333.688 	Total timesteps: 1817600 	 Percentage complete 45.440
Average episode rewards is 321.465 	Total timesteps: 1825600 	 Percentage complete 45.640
Average episode rewards is 302.895 	Total timesteps: 1833600 	 Percentage complete 45.840
Average episode rewards is 309.172 	Total timesteps: 1841600 	 Percentage complete 46.040
Average episode rewards is 315.038 	Total timesteps: 1849600 	 Percentage complete 46.240
Average episode rewards is 311.161 	Total timesteps: 1857600 	 Percentage complete 46.440
Average episode rewards is 316.458 	Total timesteps: 1865600 	 Percentage complete 46.640
Average episode rewards is 310.272 	Total timesteps: 1873600 	 Percentage complete 46.840
Average episode rewards is 319.869 	Total timesteps: 1881600 	 Percentage complete 47.040
Average episode rewards is 325.741 	Total timesteps: 1889600 	 Percentage complete 47.240
Average episode rewards is 310.684 	Total timesteps: 1897600 	 Percentage complete 47.440
Average episode rewards is 288.602 	Total timesteps: 1905600 	 Percentage complete 47.640
Average episode rewards is 333.763 	Total timesteps: 1913600 	 Percentage complete 47.840
Average episode rewards is 331.544 	Total timesteps: 1921600 	 Percentage complete 48.040
Average episode rewards is 325.648 	Total timesteps: 1929600 	 Percentage complete 48.240
Average episode rewards is 309.815 	Total timesteps: 1937600 	 Percentage complete 48.440
Average episode rewards is 301.892 	Total timesteps: 1945600 	 Percentage complete 48.640
Average episode rewards is 305.572 	Total timesteps: 1953600 	 Percentage complete 48.840
Average episode rewards is 296.003 	Total timesteps: 1961600 	 Percentage complete 49.040
Average episode rewards is 316.581 	Total timesteps: 1969600 	 Percentage complete 49.240
Average episode rewards is 327.515 	Total timesteps: 1977600 	 Percentage complete 49.440
Average episode rewards is 316.837 	Total timesteps: 1985600 	 Percentage complete 49.640
Average episode rewards is 322.230 	Total timesteps: 1993600 	 Percentage complete 49.840
Average episode rewards is 342.654 	Total timesteps: 2001600 	 Percentage complete 50.040
Average episode rewards is 343.504 	Total timesteps: 2009600 	 Percentage complete 50.240
Average episode rewards is 315.263 	Total timesteps: 2017600 	 Percentage complete 50.440
Average episode rewards is 314.624 	Total timesteps: 2025600 	 Percentage complete 50.640
Average episode rewards is 322.728 	Total timesteps: 2033600 	 Percentage complete 50.840
Average episode rewards is 321.398 	Total timesteps: 2041600 	 Percentage complete 51.040
Average episode rewards is 331.352 	Total timesteps: 2049600 	 Percentage complete 51.240
Average episode rewards is 331.690 	Total timesteps: 2057600 	 Percentage complete 51.440
Average episode rewards is 324.934 	Total timesteps: 2065600 	 Percentage complete 51.640
Average episode rewards is 317.685 	Total timesteps: 2073600 	 Percentage complete 51.840
Average episode rewards is 312.225 	Total timesteps: 2081600 	 Percentage complete 52.040
Average episode rewards is 330.520 	Total timesteps: 2089600 	 Percentage complete 52.240
Average episode rewards is 328.448 	Total timesteps: 2097600 	 Percentage complete 52.440
Average episode rewards is 289.336 	Total timesteps: 2105600 	 Percentage complete 52.640
Average episode rewards is 290.118 	Total timesteps: 2113600 	 Percentage complete 52.840
Average episode rewards is 291.127 	Total timesteps: 2121600 	 Percentage complete 53.040
Average episode rewards is 342.334 	Total timesteps: 2129600 	 Percentage complete 53.240
Average episode rewards is 331.695 	Total timesteps: 2137600 	 Percentage complete 53.440
Average episode rewards is 325.808 	Total timesteps: 2145600 	 Percentage complete 53.640
Average episode rewards is 339.743 	Total timesteps: 2153600 	 Percentage complete 53.840
Average episode rewards is 351.882 	Total timesteps: 2161600 	 Percentage complete 54.040
Average episode rewards is 373.150 	Total timesteps: 2169600 	 Percentage complete 54.240
Average episode rewards is 332.867 	Total timesteps: 2177600 	 Percentage complete 54.440
Average episode rewards is 322.854 	Total timesteps: 2185600 	 Percentage complete 54.640
Average episode rewards is 342.343 	Total timesteps: 2193600 	 Percentage complete 54.840
Average episode rewards is 318.961 	Total timesteps: 2201600 	 Percentage complete 55.040
Average episode rewards is 300.768 	Total timesteps: 2209600 	 Percentage complete 55.240
Average episode rewards is 342.287 	Total timesteps: 2217600 	 Percentage complete 55.440
Average episode rewards is 341.235 	Total timesteps: 2225600 	 Percentage complete 55.640
Average episode rewards is 324.442 	Total timesteps: 2233600 	 Percentage complete 55.840
Average episode rewards is 324.449 	Total timesteps: 2241600 	 Percentage complete 56.040
Average episode rewards is 365.097 	Total timesteps: 2249600 	 Percentage complete 56.240
Average episode rewards is 362.605 	Total timesteps: 2257600 	 Percentage complete 56.440
Average episode rewards is 357.062 	Total timesteps: 2265600 	 Percentage complete 56.640
Average episode rewards is 365.386 	Total timesteps: 2273600 	 Percentage complete 56.840
Average episode rewards is 377.583 	Total timesteps: 2281600 	 Percentage complete 57.040
Average episode rewards is 373.564 	Total timesteps: 2289600 	 Percentage complete 57.240
Average episode rewards is 375.769 	Total timesteps: 2297600 	 Percentage complete 57.440
Average episode rewards is 366.240 	Total timesteps: 2305600 	 Percentage complete 57.640
Average episode rewards is 350.077 	Total timesteps: 2313600 	 Percentage complete 57.840
Average episode rewards is 338.539 	Total timesteps: 2321600 	 Percentage complete 58.040
Average episode rewards is 336.013 	Total timesteps: 2329600 	 Percentage complete 58.240
Average episode rewards is 374.067 	Total timesteps: 2337600 	 Percentage complete 58.440
Average episode rewards is 362.346 	Total timesteps: 2345600 	 Percentage complete 58.640
Average episode rewards is 354.412 	Total timesteps: 2353600 	 Percentage complete 58.840
Average episode rewards is 352.721 	Total timesteps: 2361600 	 Percentage complete 59.040
Average episode rewards is 358.735 	Total timesteps: 2369600 	 Percentage complete 59.240
Average episode rewards is 352.618 	Total timesteps: 2377600 	 Percentage complete 59.440
Average episode rewards is 344.300 	Total timesteps: 2385600 	 Percentage complete 59.640
Average episode rewards is 353.437 	Total timesteps: 2393600 	 Percentage complete 59.840
Average episode rewards is 368.099 	Total timesteps: 2401600 	 Percentage complete 60.040
Average episode rewards is 369.499 	Total timesteps: 2409600 	 Percentage complete 60.240
Average episode rewards is 347.003 	Total timesteps: 2417600 	 Percentage complete 60.440
Average episode rewards is 363.174 	Total timesteps: 2425600 	 Percentage complete 60.640
Average episode rewards is 335.133 	Total timesteps: 2433600 	 Percentage complete 60.840
Average episode rewards is 316.058 	Total timesteps: 2441600 	 Percentage complete 61.040
Average episode rewards is 335.914 	Total timesteps: 2449600 	 Percentage complete 61.240
Average episode rewards is 349.903 	Total timesteps: 2457600 	 Percentage complete 61.440
Average episode rewards is 360.584 	Total timesteps: 2465600 	 Percentage complete 61.640
Average episode rewards is 342.244 	Total timesteps: 2473600 	 Percentage complete 61.840
Average episode rewards is 355.566 	Total timesteps: 2481600 	 Percentage complete 62.040
Average episode rewards is 343.166 	Total timesteps: 2489600 	 Percentage complete 62.240
Average episode rewards is 348.110 	Total timesteps: 2497600 	 Percentage complete 62.440
Average episode rewards is 353.770 	Total timesteps: 2505600 	 Percentage complete 62.640
Average episode rewards is 357.584 	Total timesteps: 2513600 	 Percentage complete 62.840
Average episode rewards is 354.421 	Total timesteps: 2521600 	 Percentage complete 63.040
Average episode rewards is 345.124 	Total timesteps: 2529600 	 Percentage complete 63.240
Average episode rewards is 321.591 	Total timesteps: 2537600 	 Percentage complete 63.440
Average episode rewards is 298.955 	Total timesteps: 2545600 	 Percentage complete 63.640
Average episode rewards is 328.045 	Total timesteps: 2553600 	 Percentage complete 63.840
Average episode rewards is 354.532 	Total timesteps: 2561600 	 Percentage complete 64.040
Average episode rewards is 355.280 	Total timesteps: 2569600 	 Percentage complete 64.240
Average episode rewards is 346.893 	Total timesteps: 2577600 	 Percentage complete 64.440
Average episode rewards is 345.699 	Total timesteps: 2585600 	 Percentage complete 64.640
Average episode rewards is 354.822 	Total timesteps: 2593600 	 Percentage complete 64.840
Average episode rewards is 342.972 	Total timesteps: 2601600 	 Percentage complete 65.040
Average episode rewards is 347.687 	Total timesteps: 2609600 	 Percentage complete 65.240
Average episode rewards is 335.934 	Total timesteps: 2617600 	 Percentage complete 65.440
Average episode rewards is 351.895 	Total timesteps: 2625600 	 Percentage complete 65.640
Average episode rewards is 349.778 	Total timesteps: 2633600 	 Percentage complete 65.840
Average episode rewards is 335.891 	Total timesteps: 2641600 	 Percentage complete 66.040
Average episode rewards is 355.216 	Total timesteps: 2649600 	 Percentage complete 66.240
Average episode rewards is 335.559 	Total timesteps: 2657600 	 Percentage complete 66.440
Average episode rewards is 342.937 	Total timesteps: 2665600 	 Percentage complete 66.640
Average episode rewards is 345.595 	Total timesteps: 2673600 	 Percentage complete 66.840
Average episode rewards is 321.357 	Total timesteps: 2681600 	 Percentage complete 67.040
Average episode rewards is 349.460 	Total timesteps: 2689600 	 Percentage complete 67.240
Average episode rewards is 350.322 	Total timesteps: 2697600 	 Percentage complete 67.440
Average episode rewards is 347.311 	Total timesteps: 2705600 	 Percentage complete 67.640
Average episode rewards is 330.059 	Total timesteps: 2713600 	 Percentage complete 67.840
Average episode rewards is 332.044 	Total timesteps: 2721600 	 Percentage complete 68.040
Average episode rewards is 349.289 	Total timesteps: 2729600 	 Percentage complete 68.240
Average episode rewards is 346.476 	Total timesteps: 2737600 	 Percentage complete 68.440
Average episode rewards is 334.474 	Total timesteps: 2745600 	 Percentage complete 68.640
Average episode rewards is 344.125 	Total timesteps: 2753600 	 Percentage complete 68.840
Average episode rewards is 329.926 	Total timesteps: 2761600 	 Percentage complete 69.040
Average episode rewards is 301.594 	Total timesteps: 2769600 	 Percentage complete 69.240
Average episode rewards is 334.829 	Total timesteps: 2777600 	 Percentage complete 69.440
Average episode rewards is 353.703 	Total timesteps: 2785600 	 Percentage complete 69.640
Average episode rewards is 377.830 	Total timesteps: 2793600 	 Percentage complete 69.840
Average episode rewards is 364.983 	Total timesteps: 2801600 	 Percentage complete 70.040
Average episode rewards is 360.618 	Total timesteps: 2809600 	 Percentage complete 70.240
Average episode rewards is 331.012 	Total timesteps: 2817600 	 Percentage complete 70.440
Average episode rewards is 364.892 	Total timesteps: 2825600 	 Percentage complete 70.640
Average episode rewards is 346.858 	Total timesteps: 2833600 	 Percentage complete 70.840
Average episode rewards is 339.488 	Total timesteps: 2841600 	 Percentage complete 71.040
Average episode rewards is 363.400 	Total timesteps: 2849600 	 Percentage complete 71.240
Average episode rewards is 359.423 	Total timesteps: 2857600 	 Percentage complete 71.440
Average episode rewards is 357.498 	Total timesteps: 2865600 	 Percentage complete 71.640
Average episode rewards is 359.230 	Total timesteps: 2873600 	 Percentage complete 71.840
Average episode rewards is 346.120 	Total timesteps: 2881600 	 Percentage complete 72.040
Average episode rewards is 348.469 	Total timesteps: 2889600 	 Percentage complete 72.240
Average episode rewards is 365.504 	Total timesteps: 2897600 	 Percentage complete 72.440
Average episode rewards is 352.842 	Total timesteps: 2905600 	 Percentage complete 72.640
Average episode rewards is 359.848 	Total timesteps: 2913600 	 Percentage complete 72.840
Average episode rewards is 339.158 	Total timesteps: 2921600 	 Percentage complete 73.040
Average episode rewards is 348.581 	Total timesteps: 2929600 	 Percentage complete 73.240
Average episode rewards is 355.953 	Total timesteps: 2937600 	 Percentage complete 73.440
Average episode rewards is 353.003 	Total timesteps: 2945600 	 Percentage complete 73.640
Average episode rewards is 365.926 	Total timesteps: 2953600 	 Percentage complete 73.840
Average episode rewards is 339.912 	Total timesteps: 2961600 	 Percentage complete 74.040
Average episode rewards is 356.493 	Total timesteps: 2969600 	 Percentage complete 74.240
Average episode rewards is 355.372 	Total timesteps: 2977600 	 Percentage complete 74.440
Average episode rewards is 349.465 	Total timesteps: 2985600 	 Percentage complete 74.640
Average episode rewards is 348.509 	Total timesteps: 2993600 	 Percentage complete 74.840
Average episode rewards is 354.248 	Total timesteps: 3001600 	 Percentage complete 75.040
Average episode rewards is 368.509 	Total timesteps: 3009600 	 Percentage complete 75.240
Average episode rewards is 351.681 	Total timesteps: 3017600 	 Percentage complete 75.440
Average episode rewards is 353.476 	Total timesteps: 3025600 	 Percentage complete 75.640
Average episode rewards is 330.045 	Total timesteps: 3033600 	 Percentage complete 75.840
Average episode rewards is 330.460 	Total timesteps: 3041600 	 Percentage complete 76.040
Average episode rewards is 358.645 	Total timesteps: 3049600 	 Percentage complete 76.240
Average episode rewards is 338.883 	Total timesteps: 3057600 	 Percentage complete 76.440
Average episode rewards is 341.800 	Total timesteps: 3065600 	 Percentage complete 76.640
Average episode rewards is 362.790 	Total timesteps: 3073600 	 Percentage complete 76.840
Average episode rewards is 375.149 	Total timesteps: 3081600 	 Percentage complete 77.040
Average episode rewards is 360.212 	Total timesteps: 3089600 	 Percentage complete 77.240
Average episode rewards is 373.669 	Total timesteps: 3097600 	 Percentage complete 77.440
Average episode rewards is 387.160 	Total timesteps: 3105600 	 Percentage complete 77.640
Average episode rewards is 369.770 	Total timesteps: 3113600 	 Percentage complete 77.840
Average episode rewards is 388.234 	Total timesteps: 3121600 	 Percentage complete 78.040
Average episode rewards is 375.555 	Total timesteps: 3129600 	 Percentage complete 78.240
Average episode rewards is 372.648 	Total timesteps: 3137600 	 Percentage complete 78.440
Average episode rewards is 352.923 	Total timesteps: 3145600 	 Percentage complete 78.640
Average episode rewards is 374.543 	Total timesteps: 3153600 	 Percentage complete 78.840
Average episode rewards is 338.667 	Total timesteps: 3161600 	 Percentage complete 79.040
Average episode rewards is 335.788 	Total timesteps: 3169600 	 Percentage complete 79.240
Average episode rewards is 332.895 	Total timesteps: 3177600 	 Percentage complete 79.440
Average episode rewards is 348.626 	Total timesteps: 3185600 	 Percentage complete 79.640
Average episode rewards is 339.421 	Total timesteps: 3193600 	 Percentage complete 79.840
Average episode rewards is 378.279 	Total timesteps: 3201600 	 Percentage complete 80.040
Average episode rewards is 360.804 	Total timesteps: 3209600 	 Percentage complete 80.240
Average episode rewards is 373.216 	Total timesteps: 3217600 	 Percentage complete 80.440
Average episode rewards is 341.889 	Total timesteps: 3225600 	 Percentage complete 80.640
Average episode rewards is 378.743 	Total timesteps: 3233600 	 Percentage complete 80.840
Average episode rewards is 383.214 	Total timesteps: 3241600 	 Percentage complete 81.040
Average episode rewards is 389.605 	Total timesteps: 3249600 	 Percentage complete 81.240
Average episode rewards is 379.807 	Total timesteps: 3257600 	 Percentage complete 81.440
Average episode rewards is 399.552 	Total timesteps: 3265600 	 Percentage complete 81.640
Average episode rewards is 365.157 	Total timesteps: 3273600 	 Percentage complete 81.840
Average episode rewards is 369.377 	Total timesteps: 3281600 	 Percentage complete 82.040
Average episode rewards is 387.294 	Total timesteps: 3289600 	 Percentage complete 82.240
Average episode rewards is 400.802 	Total timesteps: 3297600 	 Percentage complete 82.440
Average episode rewards is 360.706 	Total timesteps: 3305600 	 Percentage complete 82.640
Average episode rewards is 364.353 	Total timesteps: 3313600 	 Percentage complete 82.840
Average episode rewards is 368.432 	Total timesteps: 3321600 	 Percentage complete 83.040
Average episode rewards is 371.460 	Total timesteps: 3329600 	 Percentage complete 83.240
Average episode rewards is 390.444 	Total timesteps: 3337600 	 Percentage complete 83.440
Average episode rewards is 393.257 	Total timesteps: 3345600 	 Percentage complete 83.640
Average episode rewards is 371.076 	Total timesteps: 3353600 	 Percentage complete 83.840
Average episode rewards is 376.909 	Total timesteps: 3361600 	 Percentage complete 84.040
Average episode rewards is 362.637 	Total timesteps: 3369600 	 Percentage complete 84.240
Average episode rewards is 379.369 	Total timesteps: 3377600 	 Percentage complete 84.440
Average episode rewards is 366.787 	Total timesteps: 3385600 	 Percentage complete 84.640
Average episode rewards is 375.643 	Total timesteps: 3393600 	 Percentage complete 84.840
Average episode rewards is 363.086 	Total timesteps: 3401600 	 Percentage complete 85.040
Average episode rewards is 370.768 	Total timesteps: 3409600 	 Percentage complete 85.240
Average episode rewards is 358.094 	Total timesteps: 3417600 	 Percentage complete 85.440
Average episode rewards is 376.202 	Total timesteps: 3425600 	 Percentage complete 85.640
Average episode rewards is 401.187 	Total timesteps: 3433600 	 Percentage complete 85.840
Average episode rewards is 387.892 	Total timesteps: 3441600 	 Percentage complete 86.040
Average episode rewards is 376.074 	Total timesteps: 3449600 	 Percentage complete 86.240
Average episode rewards is 351.972 	Total timesteps: 3457600 	 Percentage complete 86.440
Average episode rewards is 358.208 	Total timesteps: 3465600 	 Percentage complete 86.640
Average episode rewards is 382.153 	Total timesteps: 3473600 	 Percentage complete 86.840
Average episode rewards is 377.967 	Total timesteps: 3481600 	 Percentage complete 87.040
Average episode rewards is 392.976 	Total timesteps: 3489600 	 Percentage complete 87.240
Average episode rewards is 389.194 	Total timesteps: 3497600 	 Percentage complete 87.440
Average episode rewards is 365.796 	Total timesteps: 3505600 	 Percentage complete 87.640
Average episode rewards is 366.052 	Total timesteps: 3513600 	 Percentage complete 87.840
Average episode rewards is 349.761 	Total timesteps: 3521600 	 Percentage complete 88.040
Average episode rewards is 366.681 	Total timesteps: 3529600 	 Percentage complete 88.240
Average episode rewards is 374.071 	Total timesteps: 3537600 	 Percentage complete 88.440
Average episode rewards is 357.211 	Total timesteps: 3545600 	 Percentage complete 88.640
Average episode rewards is 358.705 	Total timesteps: 3553600 	 Percentage complete 88.840
Average episode rewards is 371.036 	Total timesteps: 3561600 	 Percentage complete 89.040
Average episode rewards is 375.291 	Total timesteps: 3569600 	 Percentage complete 89.240
Average episode rewards is 351.099 	Total timesteps: 3577600 	 Percentage complete 89.440
Average episode rewards is 362.675 	Total timesteps: 3585600 	 Percentage complete 89.640
Average episode rewards is 359.838 	Total timesteps: 3593600 	 Percentage complete 89.840
Average episode rewards is 336.837 	Total timesteps: 3601600 	 Percentage complete 90.040
Average episode rewards is 338.539 	Total timesteps: 3609600 	 Percentage complete 90.240
Average episode rewards is 357.287 	Total timesteps: 3617600 	 Percentage complete 90.440
Average episode rewards is 355.584 	Total timesteps: 3625600 	 Percentage complete 90.640
Average episode rewards is 350.288 	Total timesteps: 3633600 	 Percentage complete 90.840
Average episode rewards is 379.694 	Total timesteps: 3641600 	 Percentage complete 91.040
Average episode rewards is 377.543 	Total timesteps: 3649600 	 Percentage complete 91.240
Average episode rewards is 357.586 	Total timesteps: 3657600 	 Percentage complete 91.440
Average episode rewards is 361.695 	Total timesteps: 3665600 	 Percentage complete 91.640
Average episode rewards is 382.850 	Total timesteps: 3673600 	 Percentage complete 91.840
Average episode rewards is 392.869 	Total timesteps: 3681600 	 Percentage complete 92.040
Average episode rewards is 350.695 	Total timesteps: 3689600 	 Percentage complete 92.240
Average episode rewards is 308.899 	Total timesteps: 3697600 	 Percentage complete 92.440
Average episode rewards is 335.264 	Total timesteps: 3705600 	 Percentage complete 92.640
Average episode rewards is 331.688 	Total timesteps: 3713600 	 Percentage complete 92.840
Average episode rewards is 369.610 	Total timesteps: 3721600 	 Percentage complete 93.040
Average episode rewards is 369.199 	Total timesteps: 3729600 	 Percentage complete 93.240
Average episode rewards is 347.097 	Total timesteps: 3737600 	 Percentage complete 93.440
Average episode rewards is 357.034 	Total timesteps: 3745600 	 Percentage complete 93.640
Average episode rewards is 370.891 	Total timesteps: 3753600 	 Percentage complete 93.840
Average episode rewards is 344.746 	Total timesteps: 3761600 	 Percentage complete 94.040
Average episode rewards is 384.599 	Total timesteps: 3769600 	 Percentage complete 94.240
Average episode rewards is 343.034 	Total timesteps: 3777600 	 Percentage complete 94.440
Average episode rewards is 379.878 	Total timesteps: 3785600 	 Percentage complete 94.640
Average episode rewards is 374.141 	Total timesteps: 3793600 	 Percentage complete 94.840
Average episode rewards is 382.706 	Total timesteps: 3801600 	 Percentage complete 95.040
Average episode rewards is 348.686 	Total timesteps: 3809600 	 Percentage complete 95.240
Average episode rewards is 362.352 	Total timesteps: 3817600 	 Percentage complete 95.440
Average episode rewards is 396.990 	Total timesteps: 3825600 	 Percentage complete 95.640
Average episode rewards is 387.307 	Total timesteps: 3833600 	 Percentage complete 95.840
Average episode rewards is 372.512 	Total timesteps: 3841600 	 Percentage complete 96.040
Average episode rewards is 369.199 	Total timesteps: 3849600 	 Percentage complete 96.240
Average episode rewards is 378.120 	Total timesteps: 3857600 	 Percentage complete 96.440
Average episode rewards is 411.965 	Total timesteps: 3865600 	 Percentage complete 96.640
Average episode rewards is 408.833 	Total timesteps: 3873600 	 Percentage complete 96.840
Average episode rewards is 391.870 	Total timesteps: 3881600 	 Percentage complete 97.040
Average episode rewards is 403.831 	Total timesteps: 3889600 	 Percentage complete 97.240
Average episode rewards is 405.382 	Total timesteps: 3897600 	 Percentage complete 97.440
Average episode rewards is 389.753 	Total timesteps: 3905600 	 Percentage complete 97.640
Average episode rewards is 412.424 	Total timesteps: 3913600 	 Percentage complete 97.840
Average episode rewards is 387.866 	Total timesteps: 3921600 	 Percentage complete 98.040
Average episode rewards is 407.329 	Total timesteps: 3929600 	 Percentage complete 98.240
Average episode rewards is 408.585 	Total timesteps: 3937600 	 Percentage complete 98.440
Average episode rewards is 413.793 	Total timesteps: 3945600 	 Percentage complete 98.640
Average episode rewards is 407.658 	Total timesteps: 3953600 	 Percentage complete 98.840
Average episode rewards is 392.338 	Total timesteps: 3961600 	 Percentage complete 99.040
Average episode rewards is 373.551 	Total timesteps: 3969600 	 Percentage complete 99.240
Average episode rewards is 409.525 	Total timesteps: 3977600 	 Percentage complete 99.440
Average episode rewards is 426.749 	Total timesteps: 3985600 	 Percentage complete 99.640
Average episode rewards is 434.951 	Total timesteps: 3993600 	 Percentage complete 99.840
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                actor_grad_norm â–â–…â–…â–ƒâ–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–„â–…â–…â–†â–‡â–ˆâ–…â–…â–…â–„â–…â–…â–†â–…â–‡â–ƒ
wandb:            agent0/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–
wandb:      agent0/individual_rewards â–â–„â–…â–ˆâ–‡â–†â–†â–…â–†â–…â–†â–…â–…â–„â–…â–ƒâ–…â–„â–ƒâ–…â–‡â–†â–…â–…â–‡â–…â–…â–„â–†â–…â–…â–…â–ˆâ–†â–‡â–†â–†â–‡â–‡â–‡
wandb:        agent0/min_time_to_goal â–…â–ƒâ–‡â–„â–†â–†â–ƒâ–…â–ƒâ–ˆâ–…â–„â–ƒâ–‡â–ƒâ–†â–ˆâ–ƒâ–â–ˆâ–†â–ƒâ–‡â–„â–‡â–…â–ƒâ–‚â–†â–„â–†â–…â–‡â–„â–†â–ƒâ–‡â–„â–‚â–‚
wandb:    agent0/num_agent_collisions â–…â–…â–†â–„â–ƒâ–‡â–ƒâ–†â–ƒâ–„â–„â–„â–„â–ƒâ–„â–‚â–…â–ƒâ–„â–†â–ˆâ–„â–„â–†â–†â–ƒâ–â–ˆâ–ˆâ–ƒâ–‡â–ƒâ–„â–ƒâ–†â–„â–ƒâ–â–„â–ƒ
wandb: agent0/num_obstacle_collisions â–‡â–†â–„â–â–ƒâ–…â–ˆâ–ƒâ–†â–‚â–‚â–ƒâ–‚â–„â–‚â–‚â–„â–„â–ƒâ–†â–…â–…â–ƒâ–„â–„â–ƒâ–ƒâ–…â–‚â–ƒâ–ƒâ–…â–…â–…â–ƒâ–‚â–ƒâ–…â–…â–…
wandb:            agent0/time_to_goal â–ˆâ–…â–„â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–â–‚â–â–â–‚â–â–â–‚â–‚â–â–‚â–â–‚â–â–â–â–‚â–â–â–â–‚â–â–â–â–‚â–â–â–
wandb:            agent1/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–
wandb:      agent1/individual_rewards â–â–„â–†â–‡â–†â–‡â–†â–…â–…â–…â–†â–…â–…â–ƒâ–…â–„â–ƒâ–…â–ƒâ–…â–…â–…â–„â–†â–†â–†â–…â–„â–†â–‡â–…â–…â–‡â–†â–†â–…â–†â–†â–‡â–ˆ
wandb:        agent1/min_time_to_goal â–‚â–„â–â–„â–†â–ƒâ–†â–†â–…â–†â–ƒâ–„â–„â–ˆâ–ƒâ–ƒâ–„â–†â–„â–„â–†â–†â–ƒâ–†â–ƒâ–†â–â–‡â–‚â–†â–†â–†â–†â–„â–†â–…â–…â–‚â–†â–†
wandb:    agent1/num_agent_collisions â–‡â–‡â–†â–‡â–†â–‚â–â–„â–†â–…â–„â–†â–‡â–„â–‡â–†â–ˆâ–…â–…â–…â–„â–†â–…â–…â–‡â–…â–„â–†â–†â–†â–†â–„â–ƒâ–†â–†â–ƒâ–â–…â–„â–ƒ
wandb: agent1/num_obstacle_collisions â–„â–…â–ˆâ–‚â–…â–„â–„â–â–ˆâ–„â–…â–ƒâ–†â–†â–„â–„â–„â–„â–†â–†â–ƒâ–‡â–ƒâ–…â–â–ƒâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–‚â–…â–ƒâ–…â–…â–…â–„
wandb:            agent1/time_to_goal â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–‚â–‚â–â–‚â–â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚
wandb:            agent2/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–
wandb:      agent2/individual_rewards â–â–„â–†â–ˆâ–…â–†â–…â–„â–…â–„â–…â–…â–…â–„â–„â–„â–„â–„â–„â–„â–…â–…â–†â–†â–†â–†â–„â–…â–†â–†â–…â–…â–†â–†â–†â–†â–…â–†â–‡â–ˆ
wandb:        agent2/min_time_to_goal â–†â–ƒâ–ƒâ–ˆâ–„â–†â–‚â–„â–…â–ƒâ–„â–„â–…â–†â–ƒâ–†â–ƒâ–…â–â–†â–‚â–ƒâ–„â–†â–…â–†â–ƒâ–†â–„â–†â–‚â–‡â–‚â–ƒâ–„â–„â–‚â–…â–„â–ƒ
wandb:    agent2/num_agent_collisions â–‚â–…â–„â–†â–„â–ƒâ–…â–…â–†â–‡â–…â–†â–„â–„â–ƒâ–„â–‡â–…â–‡â–…â–…â–ƒâ–„â–ƒâ–†â–†â–ˆâ–ƒâ–ƒâ–„â–‡â–„â–…â–†â–„â–…â–â–…â–â–…
wandb: agent2/num_obstacle_collisions â–†â–„â–ƒâ–„â–‚â–‚â–ƒâ–„â–‚â–„â–„â–ƒâ–„â–„â–ƒâ–„â–ƒâ–…â–â–ˆâ–…â–ƒâ–‚â–ƒâ–„â–…â–ƒâ–ƒâ–„â–‚â–â–„â–†â–‚â–ƒâ–„â–‚â–„â–â–ƒ
wandb:            agent2/time_to_goal â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–â–â–‚â–â–‚â–â–â–â–‚â–â–‚â–â–â–â–‚â–â–‚â–â–â–â–â–â–‚â–â–
wandb:            agent3/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–‚â–â–â–
wandb:      agent3/individual_rewards â–â–…â–‡â–‡â–†â–ˆâ–‡â–…â–‡â–„â–…â–…â–„â–„â–…â–ƒâ–…â–…â–„â–„â–…â–†â–…â–‡â–†â–†â–…â–†â–‡â–‡â–†â–‡â–‡â–ˆâ–‡â–…â–†â–†â–ˆâ–ˆ
wandb:        agent3/min_time_to_goal â–†â–ƒâ–â–„â–†â–†â–ƒâ–„â–…â–„â–†â–…â–ƒâ–‡â–â–…â–ƒâ–ƒâ–…â–…â–†â–ƒâ–ƒâ–„â–…â–†â–…â–…â–†â–ƒâ–ƒâ–„â–ˆâ–ƒâ–ƒâ–…â–…â–‚â–ƒâ–„
wandb:    agent3/num_agent_collisions â–…â–†â–ƒâ–…â–‡â–ƒâ–ƒâ–‡â–ˆâ–…â–†â–ˆâ–‡â–†â–ƒâ–…â–„â–„â–â–‡â–…â–…â–‡â–„â–‡â–…â–„â–ƒâ–…â–ˆâ–†â–„â–…â–…â–„â–†â–ƒâ–…â–‚â–ƒ
wandb: agent3/num_obstacle_collisions â–„â–„â–„â–ƒâ–ˆâ–‡â–‡â–„â–†â–…â–…â–„â–…â–„â–„â–„â–â–‚â–ƒâ–ƒâ–‚â–ƒâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–„â–†â–‡â–ƒâ–‚â–ƒâ–…â–„â–ƒâ–ƒâ–…â–‚
wandb:            agent3/time_to_goal â–ˆâ–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–
wandb:            agent4/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–
wandb:      agent4/individual_rewards â–â–…â–†â–ˆâ–‡â–ˆâ–‡â–…â–‡â–…â–†â–„â–„â–„â–…â–ƒâ–„â–…â–…â–„â–„â–…â–…â–…â–‡â–†â–…â–…â–‡â–†â–†â–†â–‡â–‡â–†â–†â–†â–†â–ˆâ–ˆ
wandb:        agent4/min_time_to_goal â–†â–‡â–‡â–…â–„â–„â–…â–†â–„â–ˆâ–„â–â–„â–†â–…â–â–ˆâ–…â–„â–‚â–†â–†â–„â–„â–…â–‚â–†â–‡â–…â–†â–…â–‡â–†â–ƒâ–…â–…â–…â–†â–…â–ˆ
wandb:    agent4/num_agent_collisions â–ƒâ–‡â–ˆâ–…â–…â–†â–„â–‡â–„â–ˆâ–†â–‡â–…â–†â–ƒâ–ˆâ–ˆâ–†â–†â–ˆâ–†â–†â–†â–ˆâ–„â–†â–†â–…â–‡â–…â–â–ƒâ–…â–…â–„â–‡â–â–‡â–ƒâ–„
wandb: agent4/num_obstacle_collisions â–†â–…â–‡â–‚â–„â–ƒâ–„â–‡â–„â–†â–‚â–ˆâ–â–ƒâ–„â–…â–„â–„â–„â–‡â–ƒâ–…â–â–ƒâ–…â–‡â–‚â–„â–ƒâ–„â–„â–…â–ƒâ–†â–‚â–ƒâ–†â–„â–‡â–…
wandb:            agent4/time_to_goal â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–‚â–â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚
wandb:            agent5/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–
wandb:      agent5/individual_rewards â–â–„â–†â–ˆâ–„â–†â–†â–…â–†â–„â–„â–ƒâ–…â–„â–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–†â–…â–„â–†â–‡â–„â–„â–„â–…â–…â–†â–„â–†â–‡â–‡â–†â–†â–‡â–‡â–‡
wandb:        agent5/min_time_to_goal â–…â–†â–…â–†â–â–…â–…â–ƒâ–†â–…â–„â–…â–„â–…â–…â–ƒâ–‚â–…â–‚â–‚â–†â–…â–„â–„â–„â–‡â–…â–†â–ˆâ–ƒâ–†â–…â–‚â–†â–…â–‡â–…â–ƒâ–„â–…
wandb:    agent5/num_agent_collisions â–…â–ˆâ–†â–ƒâ–„â–ƒâ–ƒâ–†â–‡â–ƒâ–„â–†â–„â–ˆâ–ƒâ–…â–…â–…â–ƒâ–‡â–†â–ƒâ–‡â–‡â–‡â–†â–ƒâ–…â–†â–„â–ƒâ–„â–„â–„â–„â–‡â–â–ƒâ–‚â–ƒ
wandb: agent5/num_obstacle_collisions â–†â–ƒâ–‚â–‚â–ˆâ–†â–ƒâ–…â–…â–„â–â–ƒâ–…â–‚â–…â–…â–†â–†â–ƒâ–â–‚â–…â–ƒâ–†â–ƒâ–…â–‚â–ƒâ–ƒâ–…â–„â–…â–…â–ˆâ–ƒâ–‡â–†â–ƒâ–ƒâ–„
wandb:            agent5/time_to_goal â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–â–‚
wandb:            agent6/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–
wandb:      agent6/individual_rewards â–â–ƒâ–†â–ˆâ–†â–‡â–‡â–†â–‡â–…â–…â–…â–„â–…â–…â–„â–„â–…â–„â–…â–…â–…â–…â–ˆâ–†â–…â–…â–…â–†â–‡â–†â–†â–‡â–ˆâ–‡â–†â–‡â–‡â–ˆâ–‡
wandb:        agent6/min_time_to_goal â–…â–‚â–â–ˆâ–„â–ƒâ–†â–†â–…â–â–‚â–ˆâ–‡â–†â–ƒâ–„â–„â–‚â–â–…â–‡â–„â–‚â–„â–ƒâ–„â–ƒâ–†â–…â–‡â–…â–„â–„â–‡â–‚â–„â–†â–‚â–„â–†
wandb:    agent6/num_agent_collisions â–„â–„â–„â–…â–…â–„â–â–ƒâ–ƒâ–†â–ƒâ–…â–…â–ƒâ–ˆâ–„â–ƒâ–ƒâ–…â–†â–…â–ƒâ–‚â–„â–‡â–ƒâ–‚â–…â–…â–…â–„â–…â–‚â–†â–‚â–„â–ƒâ–„â–‚â–
wandb: agent6/num_obstacle_collisions â–‚â–„â–…â–ƒâ–‚â–…â–„â–…â–ˆâ–â–ƒâ–„â–…â–‡â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–…â–„â–â–ƒâ–„â–‚â–†â–…â–…â–‚â–ƒâ–„â–…â–ƒâ–„â–‚â–ƒâ–…â–ƒâ–‚
wandb:            agent6/time_to_goal â–ˆâ–…â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–‚â–â–‚â–â–â–â–‚â–â–â–‚â–â–â–‚
wandb:        average_episode_rewards â–â–ƒâ–…â–†â–†â–‡â–‡â–†â–‡â–†â–‡â–‡â–‡â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:               critic_grad_norm â–ˆâ–â–â–‚â–„â–â–ƒâ–ƒâ–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–‚â–
wandb:                   dist_entropy â–ˆâ–„â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    policy_loss â–…â–‚â–â–ƒâ–‚â–„â–…â–…â–…â–…â–…â–…â–…â–„â–„â–…â–„â–„â–„â–„â–…â–…â–…â–…â–…â–„â–„â–†â–†â–ˆâ–†â–†â–‡â–†â–…â–†â–†â–†â–†â–‡
wandb:                          ratio â–ƒâ–„â–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–„â–‚â–…â–ƒâ–†â–„â–„â–ƒâ–†â–‚â–ƒâ–ˆâ–‚â–‚â–ƒâ–â–‚â–‚â–â–‚â–ƒâ–â–‚â–ƒâ–‚â–„â–ƒâ–„â–‚
wandb:                     value_loss â–‚â–‡â–ˆâ–†â–„â–…â–†â–‚â–ƒâ–ƒâ–„â–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–â–â–„â–
wandb: 
wandb: Run summary:
wandb:                actor_grad_norm 0.81599
wandb:            agent0/dist_to_goal 0.03805
wandb:      agent0/individual_rewards 3.8117
wandb:        agent0/min_time_to_goal 0.44527
wandb:    agent0/num_agent_collisions 0.57812
wandb: agent0/num_obstacle_collisions 0.1875
wandb:            agent0/time_to_goal 0.88438
wandb:            agent1/dist_to_goal 0.03917
wandb:      agent1/individual_rewards 3.57556
wandb:        agent1/min_time_to_goal 0.48415
wandb:    agent1/num_agent_collisions 1.01562
wandb: agent1/num_obstacle_collisions 0.25
wandb:            agent1/time_to_goal 0.99687
wandb:            agent2/dist_to_goal 0.04409
wandb:      agent2/individual_rewards 3.57101
wandb:        agent2/min_time_to_goal 0.44513
wandb:    agent2/num_agent_collisions 0.70312
wandb: agent2/num_obstacle_collisions 0.375
wandb:            agent2/time_to_goal 0.94844
wandb:            agent3/dist_to_goal 0.03642
wandb:      agent3/individual_rewards 3.65504
wandb:        agent3/min_time_to_goal 0.44405
wandb:    agent3/num_agent_collisions 0.59375
wandb: agent3/num_obstacle_collisions 0.39062
wandb:            agent3/time_to_goal 0.88906
wandb:            agent4/dist_to_goal 0.03351
wandb:      agent4/individual_rewards 3.89158
wandb:        agent4/min_time_to_goal 0.41929
wandb:    agent4/num_agent_collisions 0.625
wandb: agent4/num_obstacle_collisions 0.39062
wandb:            agent4/time_to_goal 0.86562
wandb:            agent5/dist_to_goal 0.03131
wandb:      agent5/individual_rewards 4.28699
wandb:        agent5/min_time_to_goal 0.46337
wandb:    agent5/num_agent_collisions 1.0
wandb: agent5/num_obstacle_collisions 0.20312
wandb:            agent5/time_to_goal 0.97969
wandb:            agent6/dist_to_goal 0.03249
wandb:      agent6/individual_rewards 4.20899
wandb:        agent6/min_time_to_goal 0.4561
wandb:    agent6/num_agent_collisions 1.17188
wandb: agent6/num_obstacle_collisions 0.17188
wandb:            agent6/time_to_goal 0.93594
wandb:        average_episode_rewards 434.9514
wandb:               critic_grad_norm 0.05307
wandb:                   dist_entropy 0.29412
wandb:                    policy_loss -0.00452
wandb:                          ratio 0.99599
wandb:                     value_loss 0.05791
wandb: 
wandb: ðŸš€ View run rmappo_informarl_seed0 at: https://wandb.ai/golde/enemy/runs/ngtkh65y
wandb: ï¸âš¡ View job at https://wandb.ai/golde/enemy/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjM2MjUwMjIwOQ==/version_details/v2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)
wandb: Find logs at: ./onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240808_143105-ngtkh65y/logs
Initializing navigation with fixed parameters
__________________________________________________
Choose to use gpu...
__________________________________________________
__________________________________________________
||                  Arguments                   ||
__________________________________________________
|| algorithm_name: rmappo                       ||
|| project_name: informarl                      ||
|| experiment_name: informarl                   ||
|| seed: 0                                      ||
|| cuda: True                                   ||
|| cuda_deterministic: True                     ||
|| n_training_threads: 1                        ||
|| n_rollout_threads: 64                        ||
|| n_eval_rollout_threads: 1                    ||
|| n_render_rollout_threads: 1                  ||
|| num_env_steps: 4000000                       ||
|| user_name: marl                              ||
|| use_wandb: True                              ||
|| env_name: GraphMPE                           ||
|| use_obs_instead_of_state: False              ||
|| world_size: 2                                ||
|| num_scripted_agents: 0                       ||
|| obs_type: global                             ||
|| max_edge_dist: 1                             ||
|| num_nbd_entities: 3                          ||
|| use_comm: False                              ||
|| episode_length: 25                           ||
|| share_policy: True                           ||
|| use_centralized_V: True                      ||
|| stacked_frames: 1                            ||
|| use_stacked_frames: False                    ||
|| hidden_size: 64                              ||
|| layer_N: 1                                   ||
|| use_ReLU: False                              ||
|| use_popart: True                             ||
|| use_valuenorm: False                         ||
|| use_feature_normalization: True              ||
|| use_orthogonal: True                         ||
|| gain: 0.01                                   ||
|| split_batch: False                           ||
|| max_batch_size: 32                           ||
|| use_naive_recurrent_policy: False            ||
|| use_recurrent_policy: True                   ||
|| recurrent_N: 1                               ||
|| data_chunk_length: 10                        ||
|| lr: 0.0007                                   ||
|| critic_lr: 0.0007                            ||
|| opti_eps: 1e-05                              ||
|| weight_decay: 0                              ||
|| ppo_epoch: 10                                ||
|| use_clipped_value_loss: True                 ||
|| clip_param: 0.2                              ||
|| num_mini_batch: 1                            ||
|| entropy_coef: 0.01                           ||
|| value_loss_coef: 1                           ||
|| use_max_grad_norm: True                      ||
|| max_grad_norm: 10.0                          ||
|| use_gae: True                                ||
|| gamma: 0.99                                  ||
|| gae_lambda: 0.95                             ||
|| use_proper_time_limits: False                ||
|| use_huber_loss: True                         ||
|| use_value_active_masks: True                 ||
|| use_policy_active_masks: True                ||
|| huber_delta: 10.0                            ||
|| use_linear_lr_decay: False                   ||
|| save_interval: 1                             ||
|| log_interval: 5                              ||
|| use_eval: False                              ||
|| eval_interval: 25                            ||
|| eval_episodes: 32                            ||
|| save_gifs: False                             ||
|| use_render: False                            ||
|| render_episodes: 5                           ||
|| ifi: 0.1                                     ||
|| render_eval: False                           ||
|| model_dir: None                              ||
|| verbose: True                                ||
|| scenario_name: navigation_graph              ||
|| num_landmarks: 3                             ||
|| num_agents: 7                                ||
|| num_obstacles: 3                             ||
|| reward_sparsity: 1                           ||
|| collaborative: True                          ||
|| max_speed: 2                                 ||
|| collision_rew: 5.0                           ||
|| goal_rew: 5                                  ||
|| min_dist_thresh: 0.05                        ||
|| use_dones: False                             ||
|| num_embeddings: 3                            ||
|| embedding_size: 2                            ||
|| embed_hidden_size: 16                        ||
|| embed_layer_N: 1                             ||
|| embed_use_ReLU: True                         ||
|| embed_add_self_loop: False                   ||
|| gnn_hidden_size: 16                          ||
|| gnn_num_heads: 3                             ||
|| gnn_concat_heads: False                      ||
|| gnn_layer_N: 2                               ||
|| gnn_use_ReLU: True                           ||
|| graph_feat_type: relative                    ||
|| actor_graph_aggr: node                       ||
|| critic_graph_aggr: global                    ||
|| global_aggr_type: mean                       ||
|| use_cent_obs: False                          ||
|| auto_mini_batch_size: False                  ||
|| target_mini_batch_size: 32                   ||
__________________________________________________
__________________________________________________
Creating wandboard...
__________________________________________________
wandb: Currently logged in as: sharlinu (golde). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.11
wandb: Run data is saved locally in /home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240812_083715-h7zjr8wn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rmappo_informarl_seed0
wandb: â­ï¸ View project at https://wandb.ai/golde/enemy
wandb: ðŸš€ View run at https://wandb.ai/golde/enemy/runs/h7zjr8wn
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenariocreated scenario
Initialising graph navigation

Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
Overriding Observation dimension
Overriding Observation dimension
________________________________________________________________________________
Actor Network
________________________________________________________________________________
________________________________________________________________________________
GR_Actor(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0): TransformerConv(16, 16, heads=3)
        (1): TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((22,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=22, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (act): ACTLayer(
    (action_out): Categorical(
      (linear): Linear(in_features=64, out_features=5, bias=True)
    )
  )
)
________________________________________________________________________________
________________________________________________________________________________
Critic Network
________________________________________________________________________________
________________________________________________________________________________
GR_Critic(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0): TransformerConv(16, 16, heads=3)
        (1): TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (v_out): PopArt()
)
________________________________________________________________________________
Initializing navigation with fixed parameters
__________________________________________________
Choose to use gpu...
__________________________________________________
__________________________________________________
||                  Arguments                   ||
__________________________________________________
|| algorithm_name: rmappo                       ||
|| project_name: informarl                      ||
|| experiment_name: informarl                   ||
|| seed: 200                                    ||
|| cuda: True                                   ||
|| cuda_deterministic: True                     ||
|| n_training_threads: 1                        ||
|| n_rollout_threads: 64                        ||
|| n_eval_rollout_threads: 1                    ||
|| n_render_rollout_threads: 1                  ||
|| num_env_steps: 4000000                       ||
|| user_name: marl                              ||
|| use_wandb: True                              ||
|| env_name: GraphMPE                           ||
|| use_obs_instead_of_state: False              ||
|| world_size: 2                                ||
|| num_scripted_agents: 0                       ||
|| obs_type: global                             ||
|| max_edge_dist: 1                             ||
|| num_nbd_entities: 3                          ||
|| use_comm: False                              ||
|| episode_length: 25                           ||
|| share_policy: True                           ||
|| use_centralized_V: True                      ||
|| stacked_frames: 1                            ||
|| use_stacked_frames: False                    ||
|| hidden_size: 64                              ||
|| layer_N: 1                                   ||
|| use_ReLU: False                              ||
|| use_popart: True                             ||
|| use_valuenorm: False                         ||
|| use_feature_normalization: True              ||
|| use_orthogonal: True                         ||
|| gain: 0.01                                   ||
|| split_batch: False                           ||
|| max_batch_size: 32                           ||
|| use_naive_recurrent_policy: False            ||
|| use_recurrent_policy: True                   ||
|| recurrent_N: 1                               ||
|| data_chunk_length: 10                        ||
|| lr: 0.0007                                   ||
|| critic_lr: 0.0007                            ||
|| opti_eps: 1e-05                              ||
|| weight_decay: 0                              ||
|| ppo_epoch: 10                                ||
|| use_clipped_value_loss: True                 ||
|| clip_param: 0.2                              ||
|| num_mini_batch: 1                            ||
|| entropy_coef: 0.01                           ||
|| value_loss_coef: 1                           ||
|| use_max_grad_norm: True                      ||
|| max_grad_norm: 10.0                          ||
|| use_gae: True                                ||
|| gamma: 0.99                                  ||
|| gae_lambda: 0.95                             ||
|| use_proper_time_limits: False                ||
|| use_huber_loss: True                         ||
|| use_value_active_masks: True                 ||
|| use_policy_active_masks: True                ||
|| huber_delta: 10.0                            ||
|| use_linear_lr_decay: False                   ||
|| save_interval: 1                             ||
|| log_interval: 5                              ||
|| use_eval: False                              ||
|| eval_interval: 25                            ||
|| eval_episodes: 32                            ||
|| save_gifs: False                             ||
|| use_render: False                            ||
|| render_episodes: 5                           ||
|| ifi: 0.1                                     ||
|| render_eval: False                           ||
|| model_dir: None                              ||
|| verbose: True                                ||
|| scenario_name: navigation_graph              ||
|| num_landmarks: 3                             ||
|| num_agents: 7                                ||
|| num_obstacles: 3                             ||
|| reward_sparsity: 1                           ||
|| collaborative: True                          ||
|| max_speed: 2                                 ||
|| collision_rew: 5.0                           ||
|| goal_rew: 5                                  ||
|| min_dist_thresh: 0.05                        ||
|| use_dones: False                             ||
|| num_embeddings: 3                            ||
|| embedding_size: 2                            ||
|| embed_hidden_size: 16                        ||
|| embed_layer_N: 1                             ||
|| embed_use_ReLU: True                         ||
|| embed_add_self_loop: False                   ||
|| gnn_hidden_size: 16                          ||
|| gnn_num_heads: 3                             ||
|| gnn_concat_heads: False                      ||
|| gnn_layer_N: 2                               ||
|| gnn_use_ReLU: True                           ||
|| graph_feat_type: relative                    ||
|| actor_graph_aggr: node                       ||
|| critic_graph_aggr: global                    ||
|| global_aggr_type: mean                       ||
|| use_cent_obs: False                          ||
|| auto_mini_batch_size: False                  ||
|| target_mini_batch_size: 32                   ||
__________________________________________________
__________________________________________________
Creating wandboard...
__________________________________________________
wandb: Currently logged in as: sharlinu (golde). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.11
wandb: Run data is saved locally in /home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240812_083746-9egtea5h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rmappo_informarl_seed200
wandb: â­ï¸ View project at https://wandb.ai/golde/enemy
wandb: ðŸš€ View run at https://wandb.ai/golde/enemy/runs/9egtea5h
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigationcreated scenario

Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenariocreated scenario

Initialising graph navigationInitialising graph navigation

created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
Overriding Observation dimension
Overriding Observation dimension
________________________________________________________________________________
Actor Network
________________________________________________________________________________
________________________________________________________________________________
GR_Actor(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0): TransformerConv(16, 16, heads=3)
        (1): TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((22,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=22, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (act): ACTLayer(
    (action_out): Categorical(
      (linear): Linear(in_features=64, out_features=5, bias=True)
    )
  )
)
________________________________________________________________________________
________________________________________________________________________________
Critic Network
________________________________________________________________________________
________________________________________________________________________________
GR_Critic(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0): TransformerConv(16, 16, heads=3)
        (1): TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (v_out): PopArt()
)
________________________________________________________________________________
Average episode rewards is -210.385 	Total timesteps: 1600 	 Percentage complete 0.040
Initializing navigation with fixed parameters
__________________________________________________
Choose to use gpu...
__________________________________________________
__________________________________________________
||                  Arguments                   ||
__________________________________________________
|| algorithm_name: rmappo                       ||
|| project_name: informarl                      ||
|| experiment_name: informarl                   ||
|| seed: 400                                    ||
|| cuda: True                                   ||
|| cuda_deterministic: True                     ||
|| n_training_threads: 1                        ||
|| n_rollout_threads: 64                        ||
|| n_eval_rollout_threads: 1                    ||
|| n_render_rollout_threads: 1                  ||
|| num_env_steps: 4000000                       ||
|| user_name: marl                              ||
|| use_wandb: True                              ||
|| env_name: GraphMPE                           ||
|| use_obs_instead_of_state: False              ||
|| world_size: 2                                ||
|| num_scripted_agents: 0                       ||
|| obs_type: global                             ||
|| max_edge_dist: 1                             ||
|| num_nbd_entities: 3                          ||
|| use_comm: False                              ||
|| episode_length: 25                           ||
|| share_policy: True                           ||
|| use_centralized_V: True                      ||
|| stacked_frames: 1                            ||
|| use_stacked_frames: False                    ||
|| hidden_size: 64                              ||
|| layer_N: 1                                   ||
|| use_ReLU: False                              ||
|| use_popart: True                             ||
|| use_valuenorm: False                         ||
|| use_feature_normalization: True              ||
|| use_orthogonal: True                         ||
|| gain: 0.01                                   ||
|| split_batch: False                           ||
|| max_batch_size: 32                           ||
|| use_naive_recurrent_policy: False            ||
|| use_recurrent_policy: True                   ||
|| recurrent_N: 1                               ||
|| data_chunk_length: 10                        ||
|| lr: 0.0007                                   ||
|| critic_lr: 0.0007                            ||
|| opti_eps: 1e-05                              ||
|| weight_decay: 0                              ||
|| ppo_epoch: 10                                ||
|| use_clipped_value_loss: True                 ||
|| clip_param: 0.2                              ||
|| num_mini_batch: 1                            ||
|| entropy_coef: 0.01                           ||
|| value_loss_coef: 1                           ||
|| use_max_grad_norm: True                      ||
|| max_grad_norm: 10.0                          ||
|| use_gae: True                                ||
|| gamma: 0.99                                  ||
|| gae_lambda: 0.95                             ||
|| use_proper_time_limits: False                ||
|| use_huber_loss: True                         ||
|| use_value_active_masks: True                 ||
|| use_policy_active_masks: True                ||
|| huber_delta: 10.0                            ||
|| use_linear_lr_decay: False                   ||
|| save_interval: 1                             ||
|| log_interval: 5                              ||
|| use_eval: False                              ||
|| eval_interval: 25                            ||
|| eval_episodes: 32                            ||
|| save_gifs: False                             ||
|| use_render: False                            ||
|| render_episodes: 5                           ||
|| ifi: 0.1                                     ||
|| render_eval: False                           ||
|| model_dir: None                              ||
|| verbose: True                                ||
|| scenario_name: navigation_graph              ||
|| num_landmarks: 3                             ||
|| num_agents: 7                                ||
|| num_obstacles: 3                             ||
|| reward_sparsity: 1                           ||
|| collaborative: True                          ||
|| max_speed: 2                                 ||
|| collision_rew: 5.0                           ||
|| goal_rew: 5                                  ||
|| min_dist_thresh: 0.05                        ||
|| use_dones: False                             ||
|| num_embeddings: 3                            ||
|| embedding_size: 2                            ||
|| embed_hidden_size: 16                        ||
|| embed_layer_N: 1                             ||
|| embed_use_ReLU: True                         ||
|| embed_add_self_loop: False                   ||
|| gnn_hidden_size: 16                          ||
|| gnn_num_heads: 3                             ||
|| gnn_concat_heads: False                      ||
|| gnn_layer_N: 2                               ||
|| gnn_use_ReLU: True                           ||
|| graph_feat_type: relative                    ||
|| actor_graph_aggr: node                       ||
|| critic_graph_aggr: global                    ||
|| global_aggr_type: mean                       ||
|| use_cent_obs: False                          ||
|| auto_mini_batch_size: False                  ||
|| target_mini_batch_size: 32                   ||
__________________________________________________
__________________________________________________
Creating wandboard...
__________________________________________________
wandb: Currently logged in as: sharlinu (golde). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.11
wandb: Run data is saved locally in /home/utke_s@WMGDS.WMG.WARWICK.AC.UK/github/InforMARL/onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240812_083855-fama9fwe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rmappo_informarl_seed400
wandb: â­ï¸ View project at https://wandb.ai/golde/enemy
wandb: ðŸš€ View run at https://wandb.ai/golde/enemy/runs/fama9fwe
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
created scenario
Initialising graph navigation
Overriding Observation dimension
created scenario
Initialising graph navigation
Overriding Observation dimension
________________________________________________________________________________
Actor Network
________________________________________________________________________________
________________________________________________________________________________
GR_Actor(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0): TransformerConv(16, 16, heads=3)
        (1): TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((22,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=22, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (act): ACTLayer(
    (action_out): Categorical(
      (linear): Linear(in_features=64, out_features=5, bias=True)
    )
  )
)
________________________________________________________________________________
________________________________________________________________________________
Critic Network
________________________________________________________________________________
________________________________________________________________________________
GR_Critic(
  (gnn_base): GNNBase(
    (gnn): TransformerConvNet(
      (active_func): ReLU()
      (embed_layer): EmbedConv()
      (gnn1): TransformerConv(16, 16, heads=3)
      (gnn2): ModuleList(
        (0): TransformerConv(16, 16, heads=3)
        (1): TransformerConv(16, 16, heads=3)
      )
    )
  )
  (base): MLPBase(
    (feature_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (mlp): MLPLayer(
      (fc1): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc_h): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): Tanh()
        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (fc2): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): Tanh()
          (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (rnn): RNNLayer(
    (rnn): GRU(64, 64)
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (v_out): PopArt()
)
________________________________________________________________________________
Average episode rewards is -211.140 	Total timesteps: 1600 	 Percentage complete 0.040
Average episode rewards is -209.034 	Total timesteps: 9600 	 Percentage complete 0.240
Average episode rewards is -191.306 	Total timesteps: 9600 	 Percentage complete 0.240
Average episode rewards is -190.704 	Total timesteps: 17600 	 Percentage complete 0.440
Average episode rewards is -185.880 	Total timesteps: 17600 	 Percentage complete 0.440
Average episode rewards is -183.987 	Total timesteps: 25600 	 Percentage complete 0.640
Average episode rewards is -172.977 	Total timesteps: 25600 	 Percentage complete 0.640
Average episode rewards is -180.525 	Total timesteps: 33600 	 Percentage complete 0.840
Average episode rewards is -167.779 	Total timesteps: 33600 	 Percentage complete 0.840
Average episode rewards is -164.401 	Total timesteps: 41600 	 Percentage complete 1.040
Average episode rewards is -150.390 	Total timesteps: 41600 	 Percentage complete 1.040
Average episode rewards is -148.037 	Total timesteps: 49600 	 Percentage complete 1.240
Average episode rewards is -137.824 	Total timesteps: 49600 	 Percentage complete 1.240
Average episode rewards is -148.991 	Total timesteps: 57600 	 Percentage complete 1.440
Average episode rewards is -119.646 	Total timesteps: 57600 	 Percentage complete 1.440
Average episode rewards is -134.677 	Total timesteps: 65600 	 Percentage complete 1.640
Average episode rewards is -105.309 	Total timesteps: 65600 	 Percentage complete 1.640
Average episode rewards is -117.143 	Total timesteps: 73600 	 Percentage complete 1.840
Average episode rewards is -88.328 	Total timesteps: 73600 	 Percentage complete 1.840
Average episode rewards is -114.996 	Total timesteps: 81600 	 Percentage complete 2.040
Average episode rewards is -62.964 	Total timesteps: 81600 	 Percentage complete 2.040
Average episode rewards is -101.569 	Total timesteps: 89600 	 Percentage complete 2.240
Average episode rewards is -11.925 	Total timesteps: 89600 	 Percentage complete 2.240
Average episode rewards is -96.647 	Total timesteps: 97600 	 Percentage complete 2.440
Average episode rewards is -4.729 	Total timesteps: 97600 	 Percentage complete 2.440
Average episode rewards is -81.335 	Total timesteps: 105600 	 Percentage complete 2.640
Average episode rewards is 35.326 	Total timesteps: 105600 	 Percentage complete 2.640
Average episode rewards is -63.074 	Total timesteps: 113600 	 Percentage complete 2.840
Average episode rewards is 68.561 	Total timesteps: 113600 	 Percentage complete 2.840
Average episode rewards is -32.036 	Total timesteps: 121600 	 Percentage complete 3.040
Average episode rewards is 104.739 	Total timesteps: 121600 	 Percentage complete 3.040
Average episode rewards is -11.320 	Total timesteps: 129600 	 Percentage complete 3.240
Average episode rewards is 124.387 	Total timesteps: 129600 	 Percentage complete 3.240
Average episode rewards is 31.669 	Total timesteps: 137600 	 Percentage complete 3.440
Average episode rewards is 128.823 	Total timesteps: 137600 	 Percentage complete 3.440
Average episode rewards is 21.898 	Total timesteps: 145600 	 Percentage complete 3.640
Average episode rewards is 164.064 	Total timesteps: 145600 	 Percentage complete 3.640
Average episode rewards is 85.887 	Total timesteps: 153600 	 Percentage complete 3.840
Average episode rewards is 181.969 	Total timesteps: 153600 	 Percentage complete 3.840
Average episode rewards is 103.975 	Total timesteps: 161600 	 Percentage complete 4.040
Average episode rewards is 172.519 	Total timesteps: 161600 	 Percentage complete 4.040
Average episode rewards is 128.058 	Total timesteps: 169600 	 Percentage complete 4.240
Average episode rewards is 192.492 	Total timesteps: 169600 	 Percentage complete 4.240
Average episode rewards is 135.577 	Total timesteps: 177600 	 Percentage complete 4.440
Average episode rewards is 222.137 	Total timesteps: 177600 	 Percentage complete 4.440
Average episode rewards is 151.303 	Total timesteps: 185600 	 Percentage complete 4.640
Average episode rewards is 250.248 	Total timesteps: 185600 	 Percentage complete 4.640
Average episode rewards is 166.903 	Total timesteps: 193600 	 Percentage complete 4.840
Average episode rewards is 229.317 	Total timesteps: 193600 	 Percentage complete 4.840
Average episode rewards is 175.265 	Total timesteps: 201600 	 Percentage complete 5.040
Average episode rewards is 235.961 	Total timesteps: 201600 	 Percentage complete 5.040
Average episode rewards is 177.614 	Total timesteps: 209600 	 Percentage complete 5.240
Average episode rewards is 259.806 	Total timesteps: 209600 	 Percentage complete 5.240
Average episode rewards is 192.957 	Total timesteps: 217600 	 Percentage complete 5.440
Average episode rewards is 281.591 	Total timesteps: 217600 	 Percentage complete 5.440
Average episode rewards is 197.165 	Total timesteps: 225600 	 Percentage complete 5.640
Average episode rewards is 287.372 	Total timesteps: 225600 	 Percentage complete 5.640
Average episode rewards is 211.070 	Total timesteps: 233600 	 Percentage complete 5.840
Average episode rewards is 287.326 	Total timesteps: 233600 	 Percentage complete 5.840
Average episode rewards is 235.348 	Total timesteps: 241600 	 Percentage complete 6.040
Average episode rewards is 307.486 	Total timesteps: 241600 	 Percentage complete 6.040
Average episode rewards is 243.710 	Total timesteps: 249600 	 Percentage complete 6.240
Average episode rewards is 301.052 	Total timesteps: 249600 	 Percentage complete 6.240
Average episode rewards is 258.344 	Total timesteps: 257600 	 Percentage complete 6.440
Average episode rewards is 308.139 	Total timesteps: 257600 	 Percentage complete 6.440
Average episode rewards is 256.149 	Total timesteps: 265600 	 Percentage complete 6.640
Average episode rewards is 295.209 	Total timesteps: 265600 	 Percentage complete 6.640
Average episode rewards is 264.722 	Total timesteps: 273600 	 Percentage complete 6.840
Average episode rewards is 293.416 	Total timesteps: 273600 	 Percentage complete 6.840
Average episode rewards is 251.091 	Total timesteps: 281600 	 Percentage complete 7.040
Average episode rewards is 273.208 	Total timesteps: 281600 	 Percentage complete 7.040
Average episode rewards is 276.441 	Total timesteps: 289600 	 Percentage complete 7.240
Average episode rewards is 289.048 	Total timesteps: 289600 	 Percentage complete 7.240
Average episode rewards is 266.012 	Total timesteps: 297600 	 Percentage complete 7.440
Average episode rewards is 292.912 	Total timesteps: 297600 	 Percentage complete 7.440
Average episode rewards is 284.110 	Total timesteps: 305600 	 Percentage complete 7.640
Average episode rewards is 303.415 	Total timesteps: 305600 	 Percentage complete 7.640
Average episode rewards is 272.283 	Total timesteps: 313600 	 Percentage complete 7.840
Average episode rewards is 287.633 	Total timesteps: 313600 	 Percentage complete 7.840
Average episode rewards is 257.995 	Total timesteps: 321600 	 Percentage complete 8.040
Average episode rewards is 297.817 	Total timesteps: 321600 	 Percentage complete 8.040
Average episode rewards is 281.090 	Total timesteps: 329600 	 Percentage complete 8.240
Average episode rewards is 295.983 	Total timesteps: 329600 	 Percentage complete 8.240
Average episode rewards is 278.230 	Total timesteps: 337600 	 Percentage complete 8.440
Average episode rewards is 270.773 	Total timesteps: 337600 	 Percentage complete 8.440
Average episode rewards is 277.022 	Total timesteps: 345600 	 Percentage complete 8.640
Average episode rewards is 282.327 	Total timesteps: 345600 	 Percentage complete 8.640
wandb: Network error (ConnectTimeout), entering retry loop.
wandb: Network error (ConnectTimeout), entering retry loop.
Average episode rewards is 292.746 	Total timesteps: 353600 	 Percentage complete 8.840
Average episode rewards is 291.613 	Total timesteps: 353600 	 Percentage complete 8.840
Average episode rewards is 295.230 	Total timesteps: 361600 	 Percentage complete 9.040
Average episode rewards is 295.559 	Total timesteps: 361600 	 Percentage complete 9.040
Average episode rewards is 320.402 	Total timesteps: 369600 	 Percentage complete 9.240
Average episode rewards is 290.065 	Total timesteps: 369600 	 Percentage complete 9.240
Average episode rewards is 310.958 	Total timesteps: 377600 	 Percentage complete 9.440
Average episode rewards is 292.922 	Total timesteps: 377600 	 Percentage complete 9.440
Average episode rewards is 302.534 	Total timesteps: 385600 	 Percentage complete 9.640
Average episode rewards is 275.953 	Total timesteps: 385600 	 Percentage complete 9.640
Average episode rewards is 308.521 	Total timesteps: 393600 	 Percentage complete 9.840
Average episode rewards is 301.613 	Total timesteps: 393600 	 Percentage complete 9.840
Average episode rewards is 311.736 	Total timesteps: 401600 	 Percentage complete 10.040
Average episode rewards is 285.332 	Total timesteps: 401600 	 Percentage complete 10.040
Average episode rewards is 308.805 	Total timesteps: 409600 	 Percentage complete 10.240
Average episode rewards is 275.832 	Total timesteps: 409600 	 Percentage complete 10.240
Average episode rewards is 319.912 	Total timesteps: 417600 	 Percentage complete 10.440
Average episode rewards is 284.928 	Total timesteps: 417600 	 Percentage complete 10.440
Average episode rewards is 300.441 	Total timesteps: 425600 	 Percentage complete 10.640
Average episode rewards is 293.568 	Total timesteps: 425600 	 Percentage complete 10.640
Average episode rewards is 296.190 	Total timesteps: 433600 	 Percentage complete 10.840
Average episode rewards is 305.818 	Total timesteps: 433600 	 Percentage complete 10.840
Average episode rewards is 295.386 	Total timesteps: 441600 	 Percentage complete 11.040
Average episode rewards is 311.864 	Total timesteps: 441600 	 Percentage complete 11.040
Average episode rewards is 290.400 	Total timesteps: 449600 	 Percentage complete 11.240
Average episode rewards is 293.275 	Total timesteps: 449600 	 Percentage complete 11.240
Average episode rewards is 286.839 	Total timesteps: 457600 	 Percentage complete 11.440
Average episode rewards is 293.519 	Total timesteps: 457600 	 Percentage complete 11.440
Average episode rewards is 305.947 	Total timesteps: 465600 	 Percentage complete 11.640
Average episode rewards is 309.660 	Total timesteps: 465600 	 Percentage complete 11.640
Average episode rewards is 282.021 	Total timesteps: 473600 	 Percentage complete 11.840
Average episode rewards is 313.412 	Total timesteps: 473600 	 Percentage complete 11.840
Average episode rewards is 276.216 	Total timesteps: 481600 	 Percentage complete 12.040
Average episode rewards is 287.144 	Total timesteps: 481600 	 Percentage complete 12.040
Average episode rewards is 294.312 	Total timesteps: 489600 	 Percentage complete 12.240
Average episode rewards is 281.332 	Total timesteps: 489600 	 Percentage complete 12.240
Average episode rewards is 274.594 	Total timesteps: 497600 	 Percentage complete 12.440
Average episode rewards is 282.464 	Total timesteps: 497600 	 Percentage complete 12.440
Average episode rewards is 301.334 	Total timesteps: 505600 	 Percentage complete 12.640
Average episode rewards is 306.422 	Total timesteps: 505600 	 Percentage complete 12.640
Average episode rewards is 281.774 	Total timesteps: 513600 	 Percentage complete 12.840
Average episode rewards is 288.271 	Total timesteps: 513600 	 Percentage complete 12.840
Average episode rewards is 278.876 	Total timesteps: 521600 	 Percentage complete 13.040
Average episode rewards is 303.128 	Total timesteps: 521600 	 Percentage complete 13.040
Average episode rewards is 314.336 	Total timesteps: 529600 	 Percentage complete 13.240
Average episode rewards is 273.283 	Total timesteps: 529600 	 Percentage complete 13.240
Average episode rewards is 308.458 	Total timesteps: 537600 	 Percentage complete 13.440
Average episode rewards is 286.678 	Total timesteps: 537600 	 Percentage complete 13.440
Average episode rewards is 323.200 	Total timesteps: 545600 	 Percentage complete 13.640
Average episode rewards is 278.405 	Total timesteps: 545600 	 Percentage complete 13.640
Average episode rewards is 300.323 	Total timesteps: 553600 	 Percentage complete 13.840
Average episode rewards is 292.114 	Total timesteps: 553600 	 Percentage complete 13.840
Average episode rewards is 300.680 	Total timesteps: 561600 	 Percentage complete 14.040
Average episode rewards is 284.007 	Total timesteps: 561600 	 Percentage complete 14.040
Average episode rewards is 306.093 	Total timesteps: 569600 	 Percentage complete 14.240
Average episode rewards is 309.211 	Total timesteps: 569600 	 Percentage complete 14.240
Average episode rewards is 289.065 	Total timesteps: 577600 	 Percentage complete 14.440
Average episode rewards is 288.708 	Total timesteps: 577600 	 Percentage complete 14.440
Average episode rewards is 289.328 	Total timesteps: 585600 	 Percentage complete 14.640
Average episode rewards is 290.881 	Total timesteps: 585600 	 Percentage complete 14.640
Average episode rewards is 306.387 	Total timesteps: 593600 	 Percentage complete 14.840
Average episode rewards is 290.800 	Total timesteps: 593600 	 Percentage complete 14.840
Average episode rewards is 320.828 	Total timesteps: 601600 	 Percentage complete 15.040
Average episode rewards is 283.090 	Total timesteps: 601600 	 Percentage complete 15.040
Average episode rewards is 298.744 	Total timesteps: 609600 	 Percentage complete 15.240
Average episode rewards is 272.699 	Total timesteps: 609600 	 Percentage complete 15.240
Average episode rewards is 281.190 	Total timesteps: 617600 	 Percentage complete 15.440
Average episode rewards is 280.416 	Total timesteps: 617600 	 Percentage complete 15.440
Average episode rewards is 320.195 	Total timesteps: 625600 	 Percentage complete 15.640
Average episode rewards is 293.368 	Total timesteps: 625600 	 Percentage complete 15.640
Average episode rewards is 288.999 	Total timesteps: 633600 	 Percentage complete 15.840
Average episode rewards is 284.405 	Total timesteps: 633600 	 Percentage complete 15.840
Average episode rewards is 284.346 	Total timesteps: 641600 	 Percentage complete 16.040
Average episode rewards is 283.092 	Total timesteps: 641600 	 Percentage complete 16.040
Average episode rewards is 291.488 	Total timesteps: 649600 	 Percentage complete 16.240
Average episode rewards is 284.368 	Total timesteps: 649600 	 Percentage complete 16.240
Average episode rewards is 303.374 	Total timesteps: 657600 	 Percentage complete 16.440
Average episode rewards is 301.282 	Total timesteps: 657600 	 Percentage complete 16.440
Average episode rewards is 331.172 	Total timesteps: 665600 	 Percentage complete 16.640
Average episode rewards is 311.101 	Total timesteps: 665600 	 Percentage complete 16.640
Average episode rewards is 309.505 	Total timesteps: 673600 	 Percentage complete 16.840
Average episode rewards is 309.296 	Total timesteps: 673600 	 Percentage complete 16.840
Average episode rewards is 303.455 	Total timesteps: 681600 	 Percentage complete 17.040
Average episode rewards is 281.546 	Total timesteps: 681600 	 Percentage complete 17.040
Average episode rewards is 305.758 	Total timesteps: 689600 	 Percentage complete 17.240
Average episode rewards is 300.429 	Total timesteps: 689600 	 Percentage complete 17.240
Average episode rewards is 292.863 	Total timesteps: 697600 	 Percentage complete 17.440
Average episode rewards is 290.855 	Total timesteps: 697600 	 Percentage complete 17.440
Average episode rewards is 301.561 	Total timesteps: 705600 	 Percentage complete 17.640
Average episode rewards is 288.550 	Total timesteps: 705600 	 Percentage complete 17.640
Average episode rewards is 311.539 	Total timesteps: 713600 	 Percentage complete 17.840
Average episode rewards is 289.263 	Total timesteps: 713600 	 Percentage complete 17.840
Average episode rewards is 294.279 	Total timesteps: 721600 	 Percentage complete 18.040
Average episode rewards is 307.406 	Total timesteps: 721600 	 Percentage complete 18.040
Average episode rewards is 287.901 	Total timesteps: 729600 	 Percentage complete 18.240
Average episode rewards is 287.943 	Total timesteps: 729600 	 Percentage complete 18.240
Average episode rewards is 320.603 	Total timesteps: 737600 	 Percentage complete 18.440
Average episode rewards is 313.842 	Total timesteps: 737600 	 Percentage complete 18.440
Average episode rewards is 327.554 	Total timesteps: 745600 	 Percentage complete 18.640
Average episode rewards is 272.098 	Total timesteps: 745600 	 Percentage complete 18.640
Average episode rewards is 312.835 	Total timesteps: 753600 	 Percentage complete 18.840
Average episode rewards is 291.851 	Total timesteps: 753600 	 Percentage complete 18.840
Average episode rewards is 304.440 	Total timesteps: 761600 	 Percentage complete 19.040
Average episode rewards is 293.000 	Total timesteps: 761600 	 Percentage complete 19.040
Average episode rewards is 320.915 	Total timesteps: 769600 	 Percentage complete 19.240
Average episode rewards is 308.231 	Total timesteps: 769600 	 Percentage complete 19.240
Average episode rewards is 294.525 	Total timesteps: 777600 	 Percentage complete 19.440
Average episode rewards is 294.804 	Total timesteps: 777600 	 Percentage complete 19.440
Average episode rewards is 338.131 	Total timesteps: 785600 	 Percentage complete 19.640
Average episode rewards is 313.934 	Total timesteps: 785600 	 Percentage complete 19.640
Average episode rewards is 310.013 	Total timesteps: 793600 	 Percentage complete 19.840
Average episode rewards is 309.920 	Total timesteps: 793600 	 Percentage complete 19.840
Average episode rewards is 304.199 	Total timesteps: 801600 	 Percentage complete 20.040
Average episode rewards is 313.466 	Total timesteps: 801600 	 Percentage complete 20.040
Average episode rewards is 295.770 	Total timesteps: 809600 	 Percentage complete 20.240
Average episode rewards is 288.561 	Total timesteps: 809600 	 Percentage complete 20.240
Average episode rewards is 331.810 	Total timesteps: 817600 	 Percentage complete 20.440
Average episode rewards is 280.257 	Total timesteps: 817600 	 Percentage complete 20.440
Average episode rewards is 334.161 	Total timesteps: 825600 	 Percentage complete 20.640
Average episode rewards is 283.406 	Total timesteps: 825600 	 Percentage complete 20.640
Average episode rewards is 313.640 	Total timesteps: 833600 	 Percentage complete 20.840
Average episode rewards is 305.791 	Total timesteps: 833600 	 Percentage complete 20.840
Average episode rewards is 308.068 	Total timesteps: 841600 	 Percentage complete 21.040
Average episode rewards is 292.828 	Total timesteps: 841600 	 Percentage complete 21.040
Average episode rewards is 310.685 	Total timesteps: 849600 	 Percentage complete 21.240
Average episode rewards is 289.905 	Total timesteps: 849600 	 Percentage complete 21.240
Average episode rewards is 297.448 	Total timesteps: 857600 	 Percentage complete 21.440
Average episode rewards is 298.826 	Total timesteps: 857600 	 Percentage complete 21.440
Average episode rewards is 324.895 	Total timesteps: 865600 	 Percentage complete 21.640
Average episode rewards is 310.188 	Total timesteps: 865600 	 Percentage complete 21.640
Average episode rewards is 287.262 	Total timesteps: 873600 	 Percentage complete 21.840
Average episode rewards is 308.303 	Total timesteps: 873600 	 Percentage complete 21.840
Average episode rewards is 312.697 	Total timesteps: 881600 	 Percentage complete 22.040
Average episode rewards is 310.148 	Total timesteps: 881600 	 Percentage complete 22.040
Average episode rewards is 291.016 	Total timesteps: 889600 	 Percentage complete 22.240
Average episode rewards is 301.938 	Total timesteps: 889600 	 Percentage complete 22.240
Average episode rewards is 302.337 	Total timesteps: 897600 	 Percentage complete 22.440
Average episode rewards is 290.634 	Total timesteps: 897600 	 Percentage complete 22.440
Average episode rewards is 276.286 	Total timesteps: 905600 	 Percentage complete 22.640
Average episode rewards is 302.945 	Total timesteps: 905600 	 Percentage complete 22.640
Average episode rewards is 313.487 	Total timesteps: 913600 	 Percentage complete 22.840
Average episode rewards is 303.733 	Total timesteps: 913600 	 Percentage complete 22.840
Average episode rewards is 297.706 	Total timesteps: 921600 	 Percentage complete 23.040
Average episode rewards is 290.279 	Total timesteps: 921600 	 Percentage complete 23.040
Average episode rewards is 280.107 	Total timesteps: 929600 	 Percentage complete 23.240
Average episode rewards is 301.255 	Total timesteps: 929600 	 Percentage complete 23.240
Average episode rewards is 313.020 	Total timesteps: 937600 	 Percentage complete 23.440
Average episode rewards is 320.337 	Total timesteps: 937600 	 Percentage complete 23.440
Average episode rewards is 300.896 	Total timesteps: 945600 	 Percentage complete 23.640
Average episode rewards is 320.614 	Total timesteps: 945600 	 Percentage complete 23.640
Average episode rewards is 325.118 	Total timesteps: 953600 	 Percentage complete 23.840
Average episode rewards is 323.080 	Total timesteps: 953600 	 Percentage complete 23.840
Average episode rewards is 329.239 	Total timesteps: 961600 	 Percentage complete 24.040
Average episode rewards is 321.861 	Total timesteps: 961600 	 Percentage complete 24.040
Average episode rewards is 311.286 	Total timesteps: 969600 	 Percentage complete 24.240
Average episode rewards is 321.405 	Total timesteps: 969600 	 Percentage complete 24.240
Average episode rewards is 340.288 	Total timesteps: 977600 	 Percentage complete 24.440
Average episode rewards is 310.641 	Total timesteps: 977600 	 Percentage complete 24.440
Average episode rewards is 336.490 	Total timesteps: 985600 	 Percentage complete 24.640
Average episode rewards is 277.094 	Total timesteps: 985600 	 Percentage complete 24.640
Average episode rewards is 331.687 	Total timesteps: 993600 	 Percentage complete 24.840
Average episode rewards is 311.101 	Total timesteps: 993600 	 Percentage complete 24.840
Average episode rewards is 332.935 	Total timesteps: 1001600 	 Percentage complete 25.040
Average episode rewards is 331.906 	Total timesteps: 1001600 	 Percentage complete 25.040
Average episode rewards is 331.534 	Total timesteps: 1009600 	 Percentage complete 25.240
Average episode rewards is 325.967 	Total timesteps: 1009600 	 Percentage complete 25.240
Average episode rewards is 310.080 	Total timesteps: 1017600 	 Percentage complete 25.440
Average episode rewards is 332.028 	Total timesteps: 1017600 	 Percentage complete 25.440
Average episode rewards is 313.287 	Total timesteps: 1025600 	 Percentage complete 25.640
Average episode rewards is 336.512 	Total timesteps: 1025600 	 Percentage complete 25.640
Average episode rewards is 355.116 	Total timesteps: 1033600 	 Percentage complete 25.840
Average episode rewards is 332.527 	Total timesteps: 1033600 	 Percentage complete 25.840
Average episode rewards is 367.255 	Total timesteps: 1041600 	 Percentage complete 26.040
Average episode rewards is 306.573 	Total timesteps: 1041600 	 Percentage complete 26.040
Average episode rewards is 325.626 	Total timesteps: 1049600 	 Percentage complete 26.240
Average episode rewards is 319.122 	Total timesteps: 1049600 	 Percentage complete 26.240
Average episode rewards is 336.441 	Total timesteps: 1057600 	 Percentage complete 26.440
Average episode rewards is 346.410 	Total timesteps: 1057600 	 Percentage complete 26.440
Average episode rewards is 330.200 	Total timesteps: 1065600 	 Percentage complete 26.640
Average episode rewards is 333.247 	Total timesteps: 1065600 	 Percentage complete 26.640
Average episode rewards is 346.279 	Total timesteps: 1073600 	 Percentage complete 26.840
Average episode rewards is 326.928 	Total timesteps: 1073600 	 Percentage complete 26.840
Average episode rewards is 337.757 	Total timesteps: 1081600 	 Percentage complete 27.040
Average episode rewards is 274.604 	Total timesteps: 1081600 	 Percentage complete 27.040
Average episode rewards is 363.938 	Total timesteps: 1089600 	 Percentage complete 27.240
Average episode rewards is 302.294 	Total timesteps: 1089600 	 Percentage complete 27.240
Average episode rewards is 332.702 	Total timesteps: 1097600 	 Percentage complete 27.440
Average episode rewards is 300.915 	Total timesteps: 1097600 	 Percentage complete 27.440
Average episode rewards is 338.792 	Total timesteps: 1105600 	 Percentage complete 27.640
Average episode rewards is 303.019 	Total timesteps: 1105600 	 Percentage complete 27.640
Average episode rewards is 332.554 	Total timesteps: 1113600 	 Percentage complete 27.840
Average episode rewards is 323.799 	Total timesteps: 1113600 	 Percentage complete 27.840
Average episode rewards is 342.515 	Total timesteps: 1121600 	 Percentage complete 28.040
Average episode rewards is 334.224 	Total timesteps: 1121600 	 Percentage complete 28.040
Average episode rewards is 333.388 	Total timesteps: 1129600 	 Percentage complete 28.240
Average episode rewards is 341.739 	Total timesteps: 1129600 	 Percentage complete 28.240
Average episode rewards is 336.733 	Total timesteps: 1137600 	 Percentage complete 28.440
Average episode rewards is 289.345 	Total timesteps: 1137600 	 Percentage complete 28.440
Average episode rewards is 319.214 	Total timesteps: 1145600 	 Percentage complete 28.640
Average episode rewards is 317.427 	Total timesteps: 1145600 	 Percentage complete 28.640
Average episode rewards is 325.117 	Total timesteps: 1153600 	 Percentage complete 28.840
Average episode rewards is 340.168 	Total timesteps: 1153600 	 Percentage complete 28.840
Average episode rewards is 327.636 	Total timesteps: 1161600 	 Percentage complete 29.040
Average episode rewards is 323.746 	Total timesteps: 1161600 	 Percentage complete 29.040
Average episode rewards is 342.182 	Total timesteps: 1169600 	 Percentage complete 29.240
Average episode rewards is 322.574 	Total timesteps: 1169600 	 Percentage complete 29.240
Average episode rewards is 356.022 	Total timesteps: 1177600 	 Percentage complete 29.440
Average episode rewards is 317.194 	Total timesteps: 1177600 	 Percentage complete 29.440
Average episode rewards is 333.037 	Total timesteps: 1185600 	 Percentage complete 29.640
Average episode rewards is 338.064 	Total timesteps: 1185600 	 Percentage complete 29.640
Average episode rewards is 320.217 	Total timesteps: 1193600 	 Percentage complete 29.840
Average episode rewards is 327.105 	Total timesteps: 1193600 	 Percentage complete 29.840
Average episode rewards is 334.057 	Total timesteps: 1201600 	 Percentage complete 30.040
Average episode rewards is 296.226 	Total timesteps: 1201600 	 Percentage complete 30.040
Average episode rewards is 330.550 	Total timesteps: 1209600 	 Percentage complete 30.240
Average episode rewards is 329.203 	Total timesteps: 1209600 	 Percentage complete 30.240
Average episode rewards is 312.938 	Total timesteps: 1217600 	 Percentage complete 30.440
Average episode rewards is 327.709 	Total timesteps: 1217600 	 Percentage complete 30.440
Average episode rewards is 318.187 	Total timesteps: 1225600 	 Percentage complete 30.640
Average episode rewards is 325.374 	Total timesteps: 1225600 	 Percentage complete 30.640
Average episode rewards is 313.302 	Total timesteps: 1233600 	 Percentage complete 30.840
Average episode rewards is 316.801 	Total timesteps: 1233600 	 Percentage complete 30.840
Average episode rewards is 305.387 	Total timesteps: 1241600 	 Percentage complete 31.040
Average episode rewards is 311.086 	Total timesteps: 1241600 	 Percentage complete 31.040
Average episode rewards is 296.262 	Total timesteps: 1249600 	 Percentage complete 31.240
Average episode rewards is 324.369 	Total timesteps: 1249600 	 Percentage complete 31.240
Average episode rewards is 304.369 	Total timesteps: 1257600 	 Percentage complete 31.440
Average episode rewards is 346.883 	Total timesteps: 1257600 	 Percentage complete 31.440
Average episode rewards is 308.859 	Total timesteps: 1265600 	 Percentage complete 31.640
Average episode rewards is 331.888 	Total timesteps: 1265600 	 Percentage complete 31.640
Average episode rewards is 335.654 	Total timesteps: 1273600 	 Percentage complete 31.840
Average episode rewards is 338.837 	Total timesteps: 1273600 	 Percentage complete 31.840
Average episode rewards is 319.365 	Total timesteps: 1281600 	 Percentage complete 32.040
Average episode rewards is 313.535 	Total timesteps: 1281600 	 Percentage complete 32.040
Average episode rewards is 315.573 	Total timesteps: 1289600 	 Percentage complete 32.240
Average episode rewards is 318.294 	Total timesteps: 1289600 	 Percentage complete 32.240
Average episode rewards is 290.798 	Total timesteps: 1297600 	 Percentage complete 32.440
Average episode rewards is 306.900 	Total timesteps: 1297600 	 Percentage complete 32.440
Average episode rewards is 298.564 	Total timesteps: 1305600 	 Percentage complete 32.640
Average episode rewards is 304.205 	Total timesteps: 1305600 	 Percentage complete 32.640
Average episode rewards is 312.145 	Total timesteps: 1313600 	 Percentage complete 32.840
Average episode rewards is 319.717 	Total timesteps: 1313600 	 Percentage complete 32.840
Average episode rewards is 280.123 	Total timesteps: 1321600 	 Percentage complete 33.040
Average episode rewards is 315.438 	Total timesteps: 1321600 	 Percentage complete 33.040
Average episode rewards is 289.986 	Total timesteps: 1329600 	 Percentage complete 33.240
Average episode rewards is 299.954 	Total timesteps: 1329600 	 Percentage complete 33.240
Average episode rewards is 313.365 	Total timesteps: 1337600 	 Percentage complete 33.440
Average episode rewards is 295.551 	Total timesteps: 1337600 	 Percentage complete 33.440
Average episode rewards is 335.895 	Total timesteps: 1345600 	 Percentage complete 33.640
Average episode rewards is 312.830 	Total timesteps: 1345600 	 Percentage complete 33.640
Average episode rewards is 305.819 	Total timesteps: 1353600 	 Percentage complete 33.840
Average episode rewards is 301.256 	Total timesteps: 1353600 	 Percentage complete 33.840
Average episode rewards is 322.701 	Total timesteps: 1361600 	 Percentage complete 34.040
Average episode rewards is 301.811 	Total timesteps: 1361600 	 Percentage complete 34.040
Average episode rewards is 316.779 	Total timesteps: 1369600 	 Percentage complete 34.240
Average episode rewards is 264.505 	Total timesteps: 1369600 	 Percentage complete 34.240
Average episode rewards is 300.556 	Total timesteps: 1377600 	 Percentage complete 34.440
Average episode rewards is 268.013 	Total timesteps: 1377600 	 Percentage complete 34.440
Average episode rewards is 312.280 	Total timesteps: 1385600 	 Percentage complete 34.640
Average episode rewards is 300.608 	Total timesteps: 1385600 	 Percentage complete 34.640
Average episode rewards is 292.072 	Total timesteps: 1393600 	 Percentage complete 34.840
Average episode rewards is 260.480 	Total timesteps: 1393600 	 Percentage complete 34.840
Average episode rewards is 282.991 	Total timesteps: 1401600 	 Percentage complete 35.040
Average episode rewards is 289.044 	Total timesteps: 1401600 	 Percentage complete 35.040
Average episode rewards is 271.909 	Total timesteps: 1409600 	 Percentage complete 35.240
Average episode rewards is 275.598 	Total timesteps: 1409600 	 Percentage complete 35.240
Average episode rewards is 286.339 	Total timesteps: 1417600 	 Percentage complete 35.440
Average episode rewards is 299.625 	Total timesteps: 1417600 	 Percentage complete 35.440
Average episode rewards is 292.481 	Total timesteps: 1425600 	 Percentage complete 35.640
Average episode rewards is 262.935 	Total timesteps: 1425600 	 Percentage complete 35.640
Average episode rewards is 285.200 	Total timesteps: 1433600 	 Percentage complete 35.840
Average episode rewards is 278.428 	Total timesteps: 1433600 	 Percentage complete 35.840
Average episode rewards is 271.165 	Total timesteps: 1441600 	 Percentage complete 36.040
Average episode rewards is 306.632 	Total timesteps: 1441600 	 Percentage complete 36.040
Average episode rewards is 275.464 	Total timesteps: 1449600 	 Percentage complete 36.240
Average episode rewards is 298.603 	Total timesteps: 1449600 	 Percentage complete 36.240
Average episode rewards is 302.686 	Total timesteps: 1457600 	 Percentage complete 36.440
Average episode rewards is 323.598 	Total timesteps: 1457600 	 Percentage complete 36.440
Average episode rewards is 295.365 	Total timesteps: 1465600 	 Percentage complete 36.640
Average episode rewards is 278.586 	Total timesteps: 1465600 	 Percentage complete 36.640
Average episode rewards is 294.814 	Total timesteps: 1473600 	 Percentage complete 36.840
Average episode rewards is 313.598 	Total timesteps: 1473600 	 Percentage complete 36.840
Average episode rewards is 280.864 	Total timesteps: 1481600 	 Percentage complete 37.040
Average episode rewards is 326.076 	Total timesteps: 1481600 	 Percentage complete 37.040
Average episode rewards is 309.066 	Total timesteps: 1489600 	 Percentage complete 37.240
Average episode rewards is 287.568 	Total timesteps: 1489600 	 Percentage complete 37.240
Average episode rewards is 267.206 	Total timesteps: 1497600 	 Percentage complete 37.440
Average episode rewards is 320.357 	Total timesteps: 1497600 	 Percentage complete 37.440
Average episode rewards is 283.452 	Total timesteps: 1505600 	 Percentage complete 37.640
Average episode rewards is 310.335 	Total timesteps: 1505600 	 Percentage complete 37.640
Average episode rewards is 299.658 	Total timesteps: 1513600 	 Percentage complete 37.840
Average episode rewards is 313.179 	Total timesteps: 1513600 	 Percentage complete 37.840
Average episode rewards is 308.969 	Total timesteps: 1521600 	 Percentage complete 38.040
Average episode rewards is 324.014 	Total timesteps: 1521600 	 Percentage complete 38.040
Average episode rewards is 276.081 	Total timesteps: 1529600 	 Percentage complete 38.240
Average episode rewards is 321.260 	Total timesteps: 1529600 	 Percentage complete 38.240
Average episode rewards is 300.629 	Total timesteps: 1537600 	 Percentage complete 38.440
Average episode rewards is 325.096 	Total timesteps: 1537600 	 Percentage complete 38.440
Average episode rewards is 307.110 	Total timesteps: 1545600 	 Percentage complete 38.640
Average episode rewards is 303.774 	Total timesteps: 1545600 	 Percentage complete 38.640
Average episode rewards is 314.625 	Total timesteps: 1553600 	 Percentage complete 38.840
Average episode rewards is 309.131 	Total timesteps: 1553600 	 Percentage complete 38.840
Average episode rewards is 306.938 	Total timesteps: 1561600 	 Percentage complete 39.040
Average episode rewards is 306.886 	Total timesteps: 1561600 	 Percentage complete 39.040
Average episode rewards is 306.649 	Total timesteps: 1569600 	 Percentage complete 39.240
Average episode rewards is 308.704 	Total timesteps: 1569600 	 Percentage complete 39.240
Average episode rewards is 292.069 	Total timesteps: 1577600 	 Percentage complete 39.440
Average episode rewards is 297.281 	Total timesteps: 1577600 	 Percentage complete 39.440
Average episode rewards is 307.165 	Total timesteps: 1585600 	 Percentage complete 39.640
Average episode rewards is 327.890 	Total timesteps: 1585600 	 Percentage complete 39.640
Average episode rewards is 290.180 	Total timesteps: 1593600 	 Percentage complete 39.840
Average episode rewards is 322.965 	Total timesteps: 1593600 	 Percentage complete 39.840
Average episode rewards is 318.611 	Total timesteps: 1601600 	 Percentage complete 40.040
Average episode rewards is 305.795 	Total timesteps: 1601600 	 Percentage complete 40.040
Average episode rewards is 294.509 	Total timesteps: 1609600 	 Percentage complete 40.240
Average episode rewards is 283.347 	Total timesteps: 1609600 	 Percentage complete 40.240
Average episode rewards is 291.960 	Total timesteps: 1617600 	 Percentage complete 40.440
Average episode rewards is 307.426 	Total timesteps: 1617600 	 Percentage complete 40.440
Average episode rewards is 285.279 	Total timesteps: 1625600 	 Percentage complete 40.640
Average episode rewards is 301.450 	Total timesteps: 1625600 	 Percentage complete 40.640
Average episode rewards is 311.952 	Total timesteps: 1633600 	 Percentage complete 40.840
Average episode rewards is 333.612 	Total timesteps: 1633600 	 Percentage complete 40.840
Average episode rewards is 298.515 	Total timesteps: 1641600 	 Percentage complete 41.040
Average episode rewards is 308.309 	Total timesteps: 1641600 	 Percentage complete 41.040
Average episode rewards is 280.999 	Total timesteps: 1649600 	 Percentage complete 41.240
Average episode rewards is 331.074 	Total timesteps: 1649600 	 Percentage complete 41.240
Average episode rewards is 312.398 	Total timesteps: 1657600 	 Percentage complete 41.440
Average episode rewards is 332.829 	Total timesteps: 1657600 	 Percentage complete 41.440
Average episode rewards is 308.218 	Total timesteps: 1665600 	 Percentage complete 41.640
Average episode rewards is 334.777 	Total timesteps: 1665600 	 Percentage complete 41.640
Average episode rewards is 295.569 	Total timesteps: 1673600 	 Percentage complete 41.840
Average episode rewards is 335.998 	Total timesteps: 1673600 	 Percentage complete 41.840
Average episode rewards is 296.257 	Total timesteps: 1681600 	 Percentage complete 42.040
Average episode rewards is 322.952 	Total timesteps: 1681600 	 Percentage complete 42.040
Average episode rewards is 312.332 	Total timesteps: 1689600 	 Percentage complete 42.240
Average episode rewards is 326.536 	Total timesteps: 1689600 	 Percentage complete 42.240
Average episode rewards is 316.329 	Total timesteps: 1697600 	 Percentage complete 42.440
Average episode rewards is 331.464 	Total timesteps: 1697600 	 Percentage complete 42.440
Average episode rewards is 316.584 	Total timesteps: 1705600 	 Percentage complete 42.640
Average episode rewards is 308.343 	Total timesteps: 1705600 	 Percentage complete 42.640
Average episode rewards is 325.251 	Total timesteps: 1713600 	 Percentage complete 42.840
Average episode rewards is 301.113 	Total timesteps: 1713600 	 Percentage complete 42.840
Average episode rewards is 324.107 	Total timesteps: 1721600 	 Percentage complete 43.040
Average episode rewards is 330.262 	Total timesteps: 1721600 	 Percentage complete 43.040
Average episode rewards is 330.971 	Total timesteps: 1729600 	 Percentage complete 43.240
Average episode rewards is 344.242 	Total timesteps: 1729600 	 Percentage complete 43.240
Average episode rewards is 342.518 	Total timesteps: 1737600 	 Percentage complete 43.440
Average episode rewards is 365.298 	Total timesteps: 1737600 	 Percentage complete 43.440
Average episode rewards is 342.127 	Total timesteps: 1745600 	 Percentage complete 43.640
Average episode rewards is 341.918 	Total timesteps: 1745600 	 Percentage complete 43.640
Average episode rewards is 363.818 	Total timesteps: 1753600 	 Percentage complete 43.840
Average episode rewards is 323.899 	Total timesteps: 1753600 	 Percentage complete 43.840
Average episode rewards is 362.837 	Total timesteps: 1761600 	 Percentage complete 44.040
Average episode rewards is 342.210 	Total timesteps: 1761600 	 Percentage complete 44.040
Average episode rewards is 346.192 	Total timesteps: 1769600 	 Percentage complete 44.240
Average episode rewards is 330.540 	Total timesteps: 1769600 	 Percentage complete 44.240
Average episode rewards is 385.094 	Total timesteps: 1777600 	 Percentage complete 44.440
Average episode rewards is 305.177 	Total timesteps: 1777600 	 Percentage complete 44.440
Average episode rewards is 388.123 	Total timesteps: 1785600 	 Percentage complete 44.640
Average episode rewards is 327.889 	Total timesteps: 1785600 	 Percentage complete 44.640
Average episode rewards is 323.965 	Total timesteps: 1793600 	 Percentage complete 44.840
Average episode rewards is 314.531 	Total timesteps: 1793600 	 Percentage complete 44.840
Average episode rewards is 285.489 	Total timesteps: 1801600 	 Percentage complete 45.040
Average episode rewards is 280.441 	Total timesteps: 1801600 	 Percentage complete 45.040
Average episode rewards is 334.986 	Total timesteps: 1809600 	 Percentage complete 45.240
Average episode rewards is 287.606 	Total timesteps: 1809600 	 Percentage complete 45.240
Average episode rewards is 359.624 	Total timesteps: 1817600 	 Percentage complete 45.440
Average episode rewards is 332.051 	Total timesteps: 1817600 	 Percentage complete 45.440
Average episode rewards is 355.974 	Total timesteps: 1825600 	 Percentage complete 45.640
Average episode rewards is 342.607 	Total timesteps: 1825600 	 Percentage complete 45.640
Average episode rewards is 366.140 	Total timesteps: 1833600 	 Percentage complete 45.840
Average episode rewards is 319.513 	Total timesteps: 1833600 	 Percentage complete 45.840
Average episode rewards is 374.439 	Total timesteps: 1841600 	 Percentage complete 46.040
Average episode rewards is 351.443 	Total timesteps: 1841600 	 Percentage complete 46.040
Average episode rewards is 354.251 	Total timesteps: 1849600 	 Percentage complete 46.240
Average episode rewards is 336.243 	Total timesteps: 1849600 	 Percentage complete 46.240
Average episode rewards is 368.171 	Total timesteps: 1857600 	 Percentage complete 46.440
Average episode rewards is 327.732 	Total timesteps: 1857600 	 Percentage complete 46.440
Average episode rewards is 369.624 	Total timesteps: 1865600 	 Percentage complete 46.640
Average episode rewards is 351.337 	Total timesteps: 1865600 	 Percentage complete 46.640
Average episode rewards is 376.275 	Total timesteps: 1873600 	 Percentage complete 46.840
Average episode rewards is 319.572 	Total timesteps: 1873600 	 Percentage complete 46.840
Average episode rewards is 383.058 	Total timesteps: 1881600 	 Percentage complete 47.040
Average episode rewards is 332.542 	Total timesteps: 1881600 	 Percentage complete 47.040
Average episode rewards is 372.963 	Total timesteps: 1889600 	 Percentage complete 47.240
Average episode rewards is 347.591 	Total timesteps: 1889600 	 Percentage complete 47.240
Average episode rewards is 359.884 	Total timesteps: 1897600 	 Percentage complete 47.440
Average episode rewards is 333.790 	Total timesteps: 1897600 	 Percentage complete 47.440
Average episode rewards is 362.817 	Total timesteps: 1905600 	 Percentage complete 47.640
Average episode rewards is 332.153 	Total timesteps: 1905600 	 Percentage complete 47.640
Average episode rewards is 345.820 	Total timesteps: 1913600 	 Percentage complete 47.840
Average episode rewards is 334.329 	Total timesteps: 1913600 	 Percentage complete 47.840
Average episode rewards is 347.228 	Total timesteps: 1921600 	 Percentage complete 48.040
Average episode rewards is 323.416 	Total timesteps: 1921600 	 Percentage complete 48.040
Average episode rewards is 326.839 	Total timesteps: 1929600 	 Percentage complete 48.240
Average episode rewards is 300.691 	Total timesteps: 1929600 	 Percentage complete 48.240
Average episode rewards is 333.775 	Total timesteps: 1937600 	 Percentage complete 48.440
Average episode rewards is 319.417 	Total timesteps: 1937600 	 Percentage complete 48.440
Average episode rewards is 348.679 	Total timesteps: 1945600 	 Percentage complete 48.640
Average episode rewards is 319.692 	Total timesteps: 1945600 	 Percentage complete 48.640
Average episode rewards is 329.089 	Total timesteps: 1953600 	 Percentage complete 48.840
Average episode rewards is 350.940 	Total timesteps: 1953600 	 Percentage complete 48.840
Average episode rewards is 330.731 	Total timesteps: 1961600 	 Percentage complete 49.040
Average episode rewards is 280.915 	Total timesteps: 1961600 	 Percentage complete 49.040
Average episode rewards is 328.597 	Total timesteps: 1969600 	 Percentage complete 49.240
Average episode rewards is 319.075 	Total timesteps: 1969600 	 Percentage complete 49.240
Average episode rewards is 300.745 	Total timesteps: 1977600 	 Percentage complete 49.440
Average episode rewards is 341.124 	Total timesteps: 1977600 	 Percentage complete 49.440
Average episode rewards is 333.454 	Total timesteps: 1985600 	 Percentage complete 49.640
Average episode rewards is 356.797 	Total timesteps: 1985600 	 Percentage complete 49.640
Average episode rewards is 338.739 	Total timesteps: 1993600 	 Percentage complete 49.840
Average episode rewards is 323.555 	Total timesteps: 1993600 	 Percentage complete 49.840
Average episode rewards is 332.015 	Total timesteps: 2001600 	 Percentage complete 50.040
Average episode rewards is 335.529 	Total timesteps: 2001600 	 Percentage complete 50.040
Average episode rewards is 342.103 	Total timesteps: 2009600 	 Percentage complete 50.240
Average episode rewards is 354.128 	Total timesteps: 2009600 	 Percentage complete 50.240
Average episode rewards is 331.087 	Total timesteps: 2017600 	 Percentage complete 50.440
Average episode rewards is 339.201 	Total timesteps: 2017600 	 Percentage complete 50.440
Average episode rewards is 361.054 	Total timesteps: 2025600 	 Percentage complete 50.640
Average episode rewards is 358.628 	Total timesteps: 2025600 	 Percentage complete 50.640
Average episode rewards is 334.692 	Total timesteps: 2033600 	 Percentage complete 50.840
Average episode rewards is 346.027 	Total timesteps: 2033600 	 Percentage complete 50.840
Average episode rewards is 334.303 	Total timesteps: 2041600 	 Percentage complete 51.040
Average episode rewards is 337.516 	Total timesteps: 2041600 	 Percentage complete 51.040
Average episode rewards is 336.113 	Total timesteps: 2049600 	 Percentage complete 51.240
Average episode rewards is 350.421 	Total timesteps: 2049600 	 Percentage complete 51.240
Average episode rewards is 323.531 	Total timesteps: 2057600 	 Percentage complete 51.440
Average episode rewards is 341.541 	Total timesteps: 2057600 	 Percentage complete 51.440
Average episode rewards is 316.168 	Total timesteps: 2065600 	 Percentage complete 51.640
Average episode rewards is 337.227 	Total timesteps: 2065600 	 Percentage complete 51.640
Average episode rewards is 334.367 	Total timesteps: 2073600 	 Percentage complete 51.840
Average episode rewards is 340.966 	Total timesteps: 2073600 	 Percentage complete 51.840
Average episode rewards is 333.077 	Total timesteps: 2081600 	 Percentage complete 52.040
Average episode rewards is 305.932 	Total timesteps: 2081600 	 Percentage complete 52.040
Average episode rewards is 334.004 	Total timesteps: 2089600 	 Percentage complete 52.240
Average episode rewards is 295.675 	Total timesteps: 2089600 	 Percentage complete 52.240
Average episode rewards is 366.088 	Total timesteps: 2097600 	 Percentage complete 52.440
Average episode rewards is 299.455 	Total timesteps: 2097600 	 Percentage complete 52.440
Average episode rewards is 361.167 	Total timesteps: 2105600 	 Percentage complete 52.640
Average episode rewards is 345.785 	Total timesteps: 2105600 	 Percentage complete 52.640
Average episode rewards is 380.729 	Total timesteps: 2113600 	 Percentage complete 52.840
Average episode rewards is 308.275 	Total timesteps: 2113600 	 Percentage complete 52.840
Average episode rewards is 366.036 	Total timesteps: 2121600 	 Percentage complete 53.040
Average episode rewards is 318.466 	Total timesteps: 2121600 	 Percentage complete 53.040
Average episode rewards is 375.065 	Total timesteps: 2129600 	 Percentage complete 53.240
Average episode rewards is 319.392 	Total timesteps: 2129600 	 Percentage complete 53.240
Average episode rewards is 384.787 	Total timesteps: 2137600 	 Percentage complete 53.440
Average episode rewards is 350.989 	Total timesteps: 2137600 	 Percentage complete 53.440
Average episode rewards is 382.104 	Total timesteps: 2145600 	 Percentage complete 53.640
Average episode rewards is 350.762 	Total timesteps: 2145600 	 Percentage complete 53.640
Average episode rewards is 379.552 	Total timesteps: 2153600 	 Percentage complete 53.840
Average episode rewards is 360.699 	Total timesteps: 2153600 	 Percentage complete 53.840
Average episode rewards is 374.046 	Total timesteps: 2161600 	 Percentage complete 54.040
Average episode rewards is 328.306 	Total timesteps: 2161600 	 Percentage complete 54.040
Average episode rewards is 346.849 	Total timesteps: 2169600 	 Percentage complete 54.240
Average episode rewards is 363.149 	Total timesteps: 2169600 	 Percentage complete 54.240
Average episode rewards is 329.355 	Total timesteps: 2177600 	 Percentage complete 54.440
Average episode rewards is 347.268 	Total timesteps: 2177600 	 Percentage complete 54.440
Average episode rewards is 338.213 	Total timesteps: 2185600 	 Percentage complete 54.640
Average episode rewards is 326.883 	Total timesteps: 2185600 	 Percentage complete 54.640
Average episode rewards is 368.634 	Total timesteps: 2193600 	 Percentage complete 54.840
Average episode rewards is 345.251 	Total timesteps: 2193600 	 Percentage complete 54.840
Average episode rewards is 374.734 	Total timesteps: 2201600 	 Percentage complete 55.040
Average episode rewards is 338.681 	Total timesteps: 2201600 	 Percentage complete 55.040
Average episode rewards is 355.480 	Total timesteps: 2209600 	 Percentage complete 55.240
Average episode rewards is 333.851 	Total timesteps: 2209600 	 Percentage complete 55.240
Average episode rewards is 369.273 	Total timesteps: 2217600 	 Percentage complete 55.440
Average episode rewards is 357.254 	Total timesteps: 2217600 	 Percentage complete 55.440
Average episode rewards is 344.595 	Total timesteps: 2225600 	 Percentage complete 55.640
Average episode rewards is 358.212 	Total timesteps: 2225600 	 Percentage complete 55.640
Average episode rewards is 355.235 	Total timesteps: 2233600 	 Percentage complete 55.840
Average episode rewards is 349.118 	Total timesteps: 2233600 	 Percentage complete 55.840
Average episode rewards is 350.611 	Total timesteps: 2241600 	 Percentage complete 56.040
Average episode rewards is 336.935 	Total timesteps: 2241600 	 Percentage complete 56.040
Average episode rewards is 350.985 	Total timesteps: 2249600 	 Percentage complete 56.240
Average episode rewards is 361.633 	Total timesteps: 2249600 	 Percentage complete 56.240
Average episode rewards is 354.102 	Total timesteps: 2257600 	 Percentage complete 56.440
Average episode rewards is 347.914 	Total timesteps: 2257600 	 Percentage complete 56.440
Average episode rewards is 354.865 	Total timesteps: 2265600 	 Percentage complete 56.640
Average episode rewards is 336.205 	Total timesteps: 2265600 	 Percentage complete 56.640
Average episode rewards is 343.971 	Total timesteps: 2273600 	 Percentage complete 56.840
Average episode rewards is 320.608 	Total timesteps: 2273600 	 Percentage complete 56.840
Average episode rewards is 355.499 	Total timesteps: 2281600 	 Percentage complete 57.040
Average episode rewards is 317.234 	Total timesteps: 2281600 	 Percentage complete 57.040
Average episode rewards is 338.594 	Total timesteps: 2289600 	 Percentage complete 57.240
Average episode rewards is 333.888 	Total timesteps: 2289600 	 Percentage complete 57.240
Average episode rewards is 369.198 	Total timesteps: 2297600 	 Percentage complete 57.440
Average episode rewards is 322.613 	Total timesteps: 2297600 	 Percentage complete 57.440
Average episode rewards is 367.923 	Total timesteps: 2305600 	 Percentage complete 57.640
Average episode rewards is 343.190 	Total timesteps: 2305600 	 Percentage complete 57.640
Average episode rewards is 342.570 	Total timesteps: 2313600 	 Percentage complete 57.840
Average episode rewards is 367.153 	Total timesteps: 2313600 	 Percentage complete 57.840
Average episode rewards is 351.955 	Total timesteps: 2321600 	 Percentage complete 58.040
Average episode rewards is 358.813 	Total timesteps: 2321600 	 Percentage complete 58.040
Average episode rewards is 324.425 	Total timesteps: 2329600 	 Percentage complete 58.240
Average episode rewards is 353.083 	Total timesteps: 2329600 	 Percentage complete 58.240
Average episode rewards is 329.744 	Total timesteps: 2337600 	 Percentage complete 58.440
Average episode rewards is 377.131 	Total timesteps: 2337600 	 Percentage complete 58.440
Average episode rewards is 336.725 	Total timesteps: 2345600 	 Percentage complete 58.640
Average episode rewards is 334.092 	Total timesteps: 2345600 	 Percentage complete 58.640
Average episode rewards is 344.711 	Total timesteps: 2353600 	 Percentage complete 58.840
Average episode rewards is 352.324 	Total timesteps: 2353600 	 Percentage complete 58.840
Average episode rewards is 341.760 	Total timesteps: 2361600 	 Percentage complete 59.040
Average episode rewards is 326.335 	Total timesteps: 2361600 	 Percentage complete 59.040
Average episode rewards is 353.689 	Total timesteps: 2369600 	 Percentage complete 59.240
Average episode rewards is 324.541 	Total timesteps: 2369600 	 Percentage complete 59.240
Average episode rewards is 325.514 	Total timesteps: 2377600 	 Percentage complete 59.440
Average episode rewards is 332.084 	Total timesteps: 2377600 	 Percentage complete 59.440
Average episode rewards is 390.624 	Total timesteps: 2385600 	 Percentage complete 59.640
Average episode rewards is 351.748 	Total timesteps: 2385600 	 Percentage complete 59.640
Average episode rewards is 345.943 	Total timesteps: 2393600 	 Percentage complete 59.840
Average episode rewards is 350.561 	Total timesteps: 2393600 	 Percentage complete 59.840
Average episode rewards is 336.479 	Total timesteps: 2401600 	 Percentage complete 60.040
Average episode rewards is 343.562 	Total timesteps: 2401600 	 Percentage complete 60.040
Average episode rewards is 324.299 	Total timesteps: 2409600 	 Percentage complete 60.240
Average episode rewards is 363.753 	Total timesteps: 2409600 	 Percentage complete 60.240
Average episode rewards is 333.869 	Total timesteps: 2417600 	 Percentage complete 60.440
Average episode rewards is 352.097 	Total timesteps: 2417600 	 Percentage complete 60.440
Average episode rewards is 357.312 	Total timesteps: 2425600 	 Percentage complete 60.640
Average episode rewards is 342.230 	Total timesteps: 2425600 	 Percentage complete 60.640
Average episode rewards is 362.076 	Total timesteps: 2433600 	 Percentage complete 60.840
Average episode rewards is 304.450 	Total timesteps: 2433600 	 Percentage complete 60.840
Average episode rewards is 342.755 	Total timesteps: 2441600 	 Percentage complete 61.040
Average episode rewards is 314.461 	Total timesteps: 2441600 	 Percentage complete 61.040
Average episode rewards is 352.475 	Total timesteps: 2449600 	 Percentage complete 61.240
Average episode rewards is 355.536 	Total timesteps: 2449600 	 Percentage complete 61.240
Average episode rewards is 354.250 	Total timesteps: 2457600 	 Percentage complete 61.440
Average episode rewards is 350.322 	Total timesteps: 2457600 	 Percentage complete 61.440
Average episode rewards is 344.330 	Total timesteps: 2465600 	 Percentage complete 61.640
Average episode rewards is 371.426 	Total timesteps: 2465600 	 Percentage complete 61.640
Average episode rewards is 360.619 	Total timesteps: 2473600 	 Percentage complete 61.840
Average episode rewards is 363.895 	Total timesteps: 2473600 	 Percentage complete 61.840
Average episode rewards is 336.689 	Total timesteps: 2481600 	 Percentage complete 62.040
Average episode rewards is 340.881 	Total timesteps: 2481600 	 Percentage complete 62.040
Average episode rewards is 324.410 	Total timesteps: 2489600 	 Percentage complete 62.240
Average episode rewards is 335.186 	Total timesteps: 2489600 	 Percentage complete 62.240
Average episode rewards is 344.459 	Total timesteps: 2497600 	 Percentage complete 62.440
Average episode rewards is 355.223 	Total timesteps: 2497600 	 Percentage complete 62.440
Average episode rewards is 325.436 	Total timesteps: 2505600 	 Percentage complete 62.640
Average episode rewards is 363.366 	Total timesteps: 2505600 	 Percentage complete 62.640
Average episode rewards is 331.368 	Total timesteps: 2513600 	 Percentage complete 62.840
Average episode rewards is 357.712 	Total timesteps: 2513600 	 Percentage complete 62.840
Average episode rewards is 335.624 	Total timesteps: 2521600 	 Percentage complete 63.040
Average episode rewards is 380.410 	Total timesteps: 2521600 	 Percentage complete 63.040
Average episode rewards is 345.971 	Total timesteps: 2529600 	 Percentage complete 63.240
Average episode rewards is 365.564 	Total timesteps: 2529600 	 Percentage complete 63.240
Average episode rewards is 321.867 	Total timesteps: 2537600 	 Percentage complete 63.440
Average episode rewards is 372.879 	Total timesteps: 2537600 	 Percentage complete 63.440
Average episode rewards is 349.273 	Total timesteps: 2545600 	 Percentage complete 63.640
Average episode rewards is 381.172 	Total timesteps: 2545600 	 Percentage complete 63.640
Average episode rewards is 333.888 	Total timesteps: 2553600 	 Percentage complete 63.840
Average episode rewards is 379.255 	Total timesteps: 2553600 	 Percentage complete 63.840
Average episode rewards is 376.474 	Total timesteps: 2561600 	 Percentage complete 64.040
Average episode rewards is 365.258 	Total timesteps: 2561600 	 Percentage complete 64.040
Average episode rewards is 359.774 	Total timesteps: 2569600 	 Percentage complete 64.240
Average episode rewards is 375.328 	Total timesteps: 2569600 	 Percentage complete 64.240
Average episode rewards is 365.366 	Total timesteps: 2577600 	 Percentage complete 64.440
Average episode rewards is 375.115 	Total timesteps: 2577600 	 Percentage complete 64.440
Average episode rewards is 352.497 	Total timesteps: 2585600 	 Percentage complete 64.640
Average episode rewards is 385.261 	Total timesteps: 2585600 	 Percentage complete 64.640
Average episode rewards is 345.516 	Total timesteps: 2593600 	 Percentage complete 64.840
Average episode rewards is 386.383 	Total timesteps: 2593600 	 Percentage complete 64.840
Average episode rewards is 360.627 	Total timesteps: 2601600 	 Percentage complete 65.040
Average episode rewards is 394.734 	Total timesteps: 2601600 	 Percentage complete 65.040
Average episode rewards is 372.915 	Total timesteps: 2609600 	 Percentage complete 65.240
Average episode rewards is 377.311 	Total timesteps: 2609600 	 Percentage complete 65.240
Average episode rewards is 343.970 	Total timesteps: 2617600 	 Percentage complete 65.440
Average episode rewards is 384.571 	Total timesteps: 2617600 	 Percentage complete 65.440
Average episode rewards is 328.011 	Total timesteps: 2625600 	 Percentage complete 65.640
Average episode rewards is 355.551 	Total timesteps: 2625600 	 Percentage complete 65.640
Average episode rewards is 363.347 	Total timesteps: 2633600 	 Percentage complete 65.840
Average episode rewards is 392.986 	Total timesteps: 2633600 	 Percentage complete 65.840
Average episode rewards is 356.838 	Total timesteps: 2641600 	 Percentage complete 66.040
Average episode rewards is 379.448 	Total timesteps: 2641600 	 Percentage complete 66.040
Average episode rewards is 338.355 	Total timesteps: 2649600 	 Percentage complete 66.240
Average episode rewards is 369.891 	Total timesteps: 2649600 	 Percentage complete 66.240
Average episode rewards is 382.437 	Total timesteps: 2657600 	 Percentage complete 66.440
Average episode rewards is 378.125 	Total timesteps: 2657600 	 Percentage complete 66.440
Average episode rewards is 379.008 	Total timesteps: 2665600 	 Percentage complete 66.640
Average episode rewards is 359.492 	Total timesteps: 2665600 	 Percentage complete 66.640
Average episode rewards is 378.550 	Total timesteps: 2673600 	 Percentage complete 66.840
Average episode rewards is 374.427 	Total timesteps: 2673600 	 Percentage complete 66.840
Average episode rewards is 387.812 	Total timesteps: 2681600 	 Percentage complete 67.040
Average episode rewards is 380.040 	Total timesteps: 2681600 	 Percentage complete 67.040
Average episode rewards is 370.514 	Total timesteps: 2689600 	 Percentage complete 67.240
Average episode rewards is 371.591 	Total timesteps: 2689600 	 Percentage complete 67.240
Average episode rewards is 372.402 	Total timesteps: 2697600 	 Percentage complete 67.440
Average episode rewards is 410.943 	Total timesteps: 2697600 	 Percentage complete 67.440
Average episode rewards is 379.895 	Total timesteps: 2705600 	 Percentage complete 67.640
Average episode rewards is 410.199 	Total timesteps: 2705600 	 Percentage complete 67.640
Average episode rewards is 365.590 	Total timesteps: 2713600 	 Percentage complete 67.840
Average episode rewards is 391.615 	Total timesteps: 2713600 	 Percentage complete 67.840
Average episode rewards is 378.268 	Total timesteps: 2721600 	 Percentage complete 68.040
Average episode rewards is 384.003 	Total timesteps: 2721600 	 Percentage complete 68.040
Average episode rewards is 357.602 	Total timesteps: 2729600 	 Percentage complete 68.240
Average episode rewards is 391.105 	Total timesteps: 2729600 	 Percentage complete 68.240
Average episode rewards is 366.226 	Total timesteps: 2737600 	 Percentage complete 68.440
Average episode rewards is 354.052 	Total timesteps: 2737600 	 Percentage complete 68.440
Average episode rewards is 352.876 	Total timesteps: 2745600 	 Percentage complete 68.640
Average episode rewards is 330.055 	Total timesteps: 2745600 	 Percentage complete 68.640
Average episode rewards is 352.060 	Total timesteps: 2753600 	 Percentage complete 68.840
Average episode rewards is 357.077 	Total timesteps: 2753600 	 Percentage complete 68.840
Average episode rewards is 355.790 	Total timesteps: 2761600 	 Percentage complete 69.040
Average episode rewards is 375.301 	Total timesteps: 2761600 	 Percentage complete 69.040
Average episode rewards is 363.805 	Total timesteps: 2769600 	 Percentage complete 69.240
Average episode rewards is 384.139 	Total timesteps: 2769600 	 Percentage complete 69.240
Average episode rewards is 351.009 	Total timesteps: 2777600 	 Percentage complete 69.440
Average episode rewards is 379.205 	Total timesteps: 2777600 	 Percentage complete 69.440
Average episode rewards is 350.228 	Total timesteps: 2785600 	 Percentage complete 69.640
Average episode rewards is 387.736 	Total timesteps: 2785600 	 Percentage complete 69.640
Average episode rewards is 363.331 	Total timesteps: 2793600 	 Percentage complete 69.840
Average episode rewards is 378.578 	Total timesteps: 2793600 	 Percentage complete 69.840
Average episode rewards is 352.596 	Total timesteps: 2801600 	 Percentage complete 70.040
Average episode rewards is 367.984 	Total timesteps: 2801600 	 Percentage complete 70.040
Average episode rewards is 349.677 	Total timesteps: 2809600 	 Percentage complete 70.240
Average episode rewards is 370.024 	Total timesteps: 2809600 	 Percentage complete 70.240
Average episode rewards is 361.896 	Total timesteps: 2817600 	 Percentage complete 70.440
Average episode rewards is 360.151 	Total timesteps: 2817600 	 Percentage complete 70.440
Average episode rewards is 360.799 	Total timesteps: 2825600 	 Percentage complete 70.640
Average episode rewards is 371.652 	Total timesteps: 2825600 	 Percentage complete 70.640
Average episode rewards is 369.178 	Total timesteps: 2833600 	 Percentage complete 70.840
Average episode rewards is 380.878 	Total timesteps: 2833600 	 Percentage complete 70.840
Average episode rewards is 360.352 	Total timesteps: 2841600 	 Percentage complete 71.040
Average episode rewards is 360.506 	Total timesteps: 2841600 	 Percentage complete 71.040
Average episode rewards is 360.639 	Total timesteps: 2849600 	 Percentage complete 71.240
Average episode rewards is 343.947 	Total timesteps: 2849600 	 Percentage complete 71.240
Average episode rewards is 353.564 	Total timesteps: 2857600 	 Percentage complete 71.440
Average episode rewards is 344.053 	Total timesteps: 2857600 	 Percentage complete 71.440
Average episode rewards is 337.749 	Total timesteps: 2865600 	 Percentage complete 71.640
Average episode rewards is 354.452 	Total timesteps: 2865600 	 Percentage complete 71.640
Average episode rewards is 357.485 	Total timesteps: 2873600 	 Percentage complete 71.840
Average episode rewards is 359.540 	Total timesteps: 2873600 	 Percentage complete 71.840
Average episode rewards is 367.867 	Total timesteps: 2881600 	 Percentage complete 72.040
Average episode rewards is 353.925 	Total timesteps: 2881600 	 Percentage complete 72.040
Average episode rewards is 356.473 	Total timesteps: 2889600 	 Percentage complete 72.240
Average episode rewards is 372.629 	Total timesteps: 2889600 	 Percentage complete 72.240
Average episode rewards is 379.099 	Total timesteps: 2897600 	 Percentage complete 72.440
Average episode rewards is 360.507 	Total timesteps: 2897600 	 Percentage complete 72.440
Average episode rewards is 370.145 	Total timesteps: 2905600 	 Percentage complete 72.640
Average episode rewards is 341.888 	Total timesteps: 2905600 	 Percentage complete 72.640
Average episode rewards is 385.437 	Total timesteps: 2913600 	 Percentage complete 72.840
Average episode rewards is 345.023 	Total timesteps: 2913600 	 Percentage complete 72.840
Average episode rewards is 377.747 	Total timesteps: 2921600 	 Percentage complete 73.040
Average episode rewards is 352.441 	Total timesteps: 2921600 	 Percentage complete 73.040
Average episode rewards is 365.788 	Total timesteps: 2929600 	 Percentage complete 73.240
Average episode rewards is 360.880 	Total timesteps: 2929600 	 Percentage complete 73.240
Average episode rewards is 366.516 	Total timesteps: 2937600 	 Percentage complete 73.440
Average episode rewards is 371.006 	Total timesteps: 2937600 	 Percentage complete 73.440
Average episode rewards is 370.560 	Total timesteps: 2945600 	 Percentage complete 73.640
Average episode rewards is 363.120 	Total timesteps: 2945600 	 Percentage complete 73.640
Average episode rewards is 366.822 	Total timesteps: 2953600 	 Percentage complete 73.840
Average episode rewards is 358.313 	Total timesteps: 2953600 	 Percentage complete 73.840
Average episode rewards is 369.125 	Total timesteps: 2961600 	 Percentage complete 74.040
Average episode rewards is 376.555 	Total timesteps: 2961600 	 Percentage complete 74.040
Average episode rewards is 374.767 	Total timesteps: 2969600 	 Percentage complete 74.240
Average episode rewards is 382.330 	Total timesteps: 2969600 	 Percentage complete 74.240
Average episode rewards is 378.145 	Total timesteps: 2977600 	 Percentage complete 74.440
Average episode rewards is 358.634 	Total timesteps: 2977600 	 Percentage complete 74.440
Average episode rewards is 345.243 	Total timesteps: 2985600 	 Percentage complete 74.640
Average episode rewards is 353.759 	Total timesteps: 2985600 	 Percentage complete 74.640
Average episode rewards is 375.711 	Total timesteps: 2993600 	 Percentage complete 74.840
Average episode rewards is 349.635 	Total timesteps: 2993600 	 Percentage complete 74.840
Average episode rewards is 362.966 	Total timesteps: 3001600 	 Percentage complete 75.040
Average episode rewards is 369.102 	Total timesteps: 3001600 	 Percentage complete 75.040
Average episode rewards is 382.021 	Total timesteps: 3009600 	 Percentage complete 75.240
Average episode rewards is 362.051 	Total timesteps: 3009600 	 Percentage complete 75.240
Average episode rewards is 380.817 	Total timesteps: 3017600 	 Percentage complete 75.440
Average episode rewards is 372.801 	Total timesteps: 3017600 	 Percentage complete 75.440
Average episode rewards is 371.576 	Total timesteps: 3025600 	 Percentage complete 75.640
Average episode rewards is 364.171 	Total timesteps: 3025600 	 Percentage complete 75.640
Average episode rewards is 385.321 	Total timesteps: 3033600 	 Percentage complete 75.840
Average episode rewards is 356.623 	Total timesteps: 3033600 	 Percentage complete 75.840
Average episode rewards is 374.929 	Total timesteps: 3041600 	 Percentage complete 76.040
Average episode rewards is 368.725 	Total timesteps: 3041600 	 Percentage complete 76.040
Average episode rewards is 369.341 	Total timesteps: 3049600 	 Percentage complete 76.240
Average episode rewards is 368.998 	Total timesteps: 3049600 	 Percentage complete 76.240
Average episode rewards is 351.464 	Total timesteps: 3057600 	 Percentage complete 76.440
Average episode rewards is 365.455 	Total timesteps: 3057600 	 Percentage complete 76.440
Average episode rewards is 354.847 	Total timesteps: 3065600 	 Percentage complete 76.640
Average episode rewards is 382.920 	Total timesteps: 3065600 	 Percentage complete 76.640
Average episode rewards is 368.371 	Total timesteps: 3073600 	 Percentage complete 76.840
Average episode rewards is 368.239 	Total timesteps: 3073600 	 Percentage complete 76.840
Average episode rewards is 386.654 	Total timesteps: 3081600 	 Percentage complete 77.040
Average episode rewards is 371.783 	Total timesteps: 3081600 	 Percentage complete 77.040
Average episode rewards is 382.469 	Total timesteps: 3089600 	 Percentage complete 77.240
Average episode rewards is 399.947 	Total timesteps: 3089600 	 Percentage complete 77.240
Average episode rewards is 371.345 	Total timesteps: 3097600 	 Percentage complete 77.440
Average episode rewards is 374.722 	Total timesteps: 3097600 	 Percentage complete 77.440
Average episode rewards is 370.234 	Total timesteps: 3105600 	 Percentage complete 77.640
Average episode rewards is 377.470 	Total timesteps: 3105600 	 Percentage complete 77.640
Average episode rewards is 371.125 	Total timesteps: 3113600 	 Percentage complete 77.840
Average episode rewards is 376.273 	Total timesteps: 3113600 	 Percentage complete 77.840
Average episode rewards is 364.379 	Total timesteps: 3121600 	 Percentage complete 78.040
Average episode rewards is 376.554 	Total timesteps: 3121600 	 Percentage complete 78.040
Average episode rewards is 388.992 	Total timesteps: 3129600 	 Percentage complete 78.240
Average episode rewards is 396.872 	Total timesteps: 3129600 	 Percentage complete 78.240
Average episode rewards is 362.665 	Total timesteps: 3137600 	 Percentage complete 78.440
Average episode rewards is 391.331 	Total timesteps: 3137600 	 Percentage complete 78.440
Average episode rewards is 365.014 	Total timesteps: 3145600 	 Percentage complete 78.640
Average episode rewards is 381.898 	Total timesteps: 3145600 	 Percentage complete 78.640
Average episode rewards is 379.339 	Total timesteps: 3153600 	 Percentage complete 78.840
Average episode rewards is 366.306 	Total timesteps: 3153600 	 Percentage complete 78.840
Average episode rewards is 360.447 	Total timesteps: 3161600 	 Percentage complete 79.040
Average episode rewards is 382.106 	Total timesteps: 3161600 	 Percentage complete 79.040
Average episode rewards is 391.368 	Total timesteps: 3169600 	 Percentage complete 79.240
Average episode rewards is 385.338 	Total timesteps: 3169600 	 Percentage complete 79.240
Average episode rewards is 377.115 	Total timesteps: 3177600 	 Percentage complete 79.440
Average episode rewards is 398.436 	Total timesteps: 3177600 	 Percentage complete 79.440
Average episode rewards is 368.905 	Total timesteps: 3185600 	 Percentage complete 79.640
Average episode rewards is 399.654 	Total timesteps: 3185600 	 Percentage complete 79.640
Average episode rewards is 371.271 	Total timesteps: 3193600 	 Percentage complete 79.840
Average episode rewards is 396.466 	Total timesteps: 3193600 	 Percentage complete 79.840
Average episode rewards is 378.935 	Total timesteps: 3201600 	 Percentage complete 80.040
Average episode rewards is 395.196 	Total timesteps: 3201600 	 Percentage complete 80.040
Average episode rewards is 381.119 	Total timesteps: 3209600 	 Percentage complete 80.240
Average episode rewards is 393.593 	Total timesteps: 3209600 	 Percentage complete 80.240
Average episode rewards is 360.679 	Total timesteps: 3217600 	 Percentage complete 80.440
Average episode rewards is 389.942 	Total timesteps: 3217600 	 Percentage complete 80.440
Average episode rewards is 365.952 	Total timesteps: 3225600 	 Percentage complete 80.640
Average episode rewards is 385.449 	Total timesteps: 3225600 	 Percentage complete 80.640
Average episode rewards is 359.549 	Total timesteps: 3233600 	 Percentage complete 80.840
Average episode rewards is 406.029 	Total timesteps: 3233600 	 Percentage complete 80.840
Average episode rewards is 372.136 	Total timesteps: 3241600 	 Percentage complete 81.040
Average episode rewards is 376.435 	Total timesteps: 3241600 	 Percentage complete 81.040
Average episode rewards is 364.820 	Total timesteps: 3249600 	 Percentage complete 81.240
Average episode rewards is 361.645 	Total timesteps: 3249600 	 Percentage complete 81.240
Average episode rewards is 358.818 	Total timesteps: 3257600 	 Percentage complete 81.440
Average episode rewards is 366.304 	Total timesteps: 3257600 	 Percentage complete 81.440
Average episode rewards is 353.602 	Total timesteps: 3265600 	 Percentage complete 81.640
Average episode rewards is 379.076 	Total timesteps: 3265600 	 Percentage complete 81.640
Average episode rewards is 357.546 	Total timesteps: 3273600 	 Percentage complete 81.840
Average episode rewards is 381.009 	Total timesteps: 3273600 	 Percentage complete 81.840
Average episode rewards is 363.080 	Total timesteps: 3281600 	 Percentage complete 82.040
Average episode rewards is 375.081 	Total timesteps: 3281600 	 Percentage complete 82.040
Average episode rewards is 366.227 	Total timesteps: 3289600 	 Percentage complete 82.240
Average episode rewards is 370.026 	Total timesteps: 3289600 	 Percentage complete 82.240
Average episode rewards is 364.232 	Total timesteps: 3297600 	 Percentage complete 82.440
Average episode rewards is 377.008 	Total timesteps: 3297600 	 Percentage complete 82.440
Average episode rewards is 375.640 	Total timesteps: 3305600 	 Percentage complete 82.640
Average episode rewards is 340.900 	Total timesteps: 3305600 	 Percentage complete 82.640
Average episode rewards is 356.401 	Total timesteps: 3313600 	 Percentage complete 82.840
Average episode rewards is 340.167 	Total timesteps: 3313600 	 Percentage complete 82.840
Average episode rewards is 376.753 	Total timesteps: 3321600 	 Percentage complete 83.040
Average episode rewards is 359.957 	Total timesteps: 3321600 	 Percentage complete 83.040
Average episode rewards is 377.324 	Total timesteps: 3329600 	 Percentage complete 83.240
Average episode rewards is 368.328 	Total timesteps: 3329600 	 Percentage complete 83.240
Average episode rewards is 367.497 	Total timesteps: 3337600 	 Percentage complete 83.440
Average episode rewards is 365.170 	Total timesteps: 3337600 	 Percentage complete 83.440
Average episode rewards is 362.573 	Total timesteps: 3345600 	 Percentage complete 83.640
Average episode rewards is 361.345 	Total timesteps: 3345600 	 Percentage complete 83.640
Average episode rewards is 370.028 	Total timesteps: 3353600 	 Percentage complete 83.840
Average episode rewards is 363.199 	Total timesteps: 3353600 	 Percentage complete 83.840
Average episode rewards is 370.322 	Total timesteps: 3361600 	 Percentage complete 84.040
Average episode rewards is 346.321 	Total timesteps: 3361600 	 Percentage complete 84.040
Average episode rewards is 341.730 	Total timesteps: 3369600 	 Percentage complete 84.240
Average episode rewards is 349.080 	Total timesteps: 3369600 	 Percentage complete 84.240
Average episode rewards is 346.216 	Total timesteps: 3377600 	 Percentage complete 84.440
Average episode rewards is 375.122 	Total timesteps: 3377600 	 Percentage complete 84.440
Average episode rewards is 353.245 	Total timesteps: 3385600 	 Percentage complete 84.640
Average episode rewards is 363.866 	Total timesteps: 3385600 	 Percentage complete 84.640
Average episode rewards is 348.779 	Total timesteps: 3393600 	 Percentage complete 84.840
Average episode rewards is 355.257 	Total timesteps: 3393600 	 Percentage complete 84.840
Average episode rewards is 356.158 	Total timesteps: 3401600 	 Percentage complete 85.040
Average episode rewards is 356.682 	Total timesteps: 3401600 	 Percentage complete 85.040
Average episode rewards is 365.319 	Total timesteps: 3409600 	 Percentage complete 85.240
Average episode rewards is 366.589 	Total timesteps: 3409600 	 Percentage complete 85.240
Average episode rewards is 394.054 	Total timesteps: 3417600 	 Percentage complete 85.440
Average episode rewards is 361.615 	Total timesteps: 3417600 	 Percentage complete 85.440
Average episode rewards is 361.723 	Total timesteps: 3425600 	 Percentage complete 85.640
Average episode rewards is 350.054 	Total timesteps: 3425600 	 Percentage complete 85.640
Average episode rewards is 376.787 	Total timesteps: 3433600 	 Percentage complete 85.840
Average episode rewards is 346.821 	Total timesteps: 3433600 	 Percentage complete 85.840
Average episode rewards is 356.414 	Total timesteps: 3441600 	 Percentage complete 86.040
Average episode rewards is 358.172 	Total timesteps: 3441600 	 Percentage complete 86.040
Average episode rewards is 372.327 	Total timesteps: 3449600 	 Percentage complete 86.240
Average episode rewards is 367.335 	Total timesteps: 3449600 	 Percentage complete 86.240
Average episode rewards is 363.173 	Total timesteps: 3457600 	 Percentage complete 86.440
Average episode rewards is 347.651 	Total timesteps: 3457600 	 Percentage complete 86.440
Average episode rewards is 379.733 	Total timesteps: 3465600 	 Percentage complete 86.640
Average episode rewards is 369.646 	Total timesteps: 3465600 	 Percentage complete 86.640
Average episode rewards is 392.605 	Total timesteps: 3473600 	 Percentage complete 86.840
Average episode rewards is 376.224 	Total timesteps: 3473600 	 Percentage complete 86.840
Average episode rewards is 386.553 	Total timesteps: 3481600 	 Percentage complete 87.040
Average episode rewards is 359.033 	Total timesteps: 3481600 	 Percentage complete 87.040
Average episode rewards is 362.925 	Total timesteps: 3489600 	 Percentage complete 87.240
Average episode rewards is 377.649 	Total timesteps: 3489600 	 Percentage complete 87.240
Average episode rewards is 372.005 	Total timesteps: 3497600 	 Percentage complete 87.440
Average episode rewards is 395.622 	Total timesteps: 3497600 	 Percentage complete 87.440
Average episode rewards is 387.441 	Total timesteps: 3505600 	 Percentage complete 87.640
Average episode rewards is 369.170 	Total timesteps: 3505600 	 Percentage complete 87.640
Average episode rewards is 413.861 	Total timesteps: 3513600 	 Percentage complete 87.840
Average episode rewards is 393.283 	Total timesteps: 3513600 	 Percentage complete 87.840
Average episode rewards is 387.770 	Total timesteps: 3521600 	 Percentage complete 88.040
Average episode rewards is 382.659 	Total timesteps: 3521600 	 Percentage complete 88.040
Average episode rewards is 413.876 	Total timesteps: 3529600 	 Percentage complete 88.240
Average episode rewards is 379.563 	Total timesteps: 3529600 	 Percentage complete 88.240
Average episode rewards is 387.702 	Total timesteps: 3537600 	 Percentage complete 88.440
Average episode rewards is 399.920 	Total timesteps: 3537600 	 Percentage complete 88.440
Average episode rewards is 379.943 	Total timesteps: 3545600 	 Percentage complete 88.640
Average episode rewards is 388.065 	Total timesteps: 3545600 	 Percentage complete 88.640
Average episode rewards is 377.022 	Total timesteps: 3553600 	 Percentage complete 88.840
Average episode rewards is 359.757 	Total timesteps: 3553600 	 Percentage complete 88.840
Average episode rewards is 377.165 	Total timesteps: 3561600 	 Percentage complete 89.040
Average episode rewards is 385.223 	Total timesteps: 3561600 	 Percentage complete 89.040
Average episode rewards is 376.514 	Total timesteps: 3569600 	 Percentage complete 89.240
Average episode rewards is 387.375 	Total timesteps: 3569600 	 Percentage complete 89.240
Average episode rewards is 386.935 	Total timesteps: 3577600 	 Percentage complete 89.440
Average episode rewards is 417.290 	Total timesteps: 3577600 	 Percentage complete 89.440
Average episode rewards is 397.517 	Total timesteps: 3585600 	 Percentage complete 89.640
Average episode rewards is 412.253 	Total timesteps: 3585600 	 Percentage complete 89.640
Average episode rewards is 387.134 	Total timesteps: 3593600 	 Percentage complete 89.840
Average episode rewards is 421.779 	Total timesteps: 3593600 	 Percentage complete 89.840
Average episode rewards is 389.883 	Total timesteps: 3601600 	 Percentage complete 90.040
Average episode rewards is 408.546 	Total timesteps: 3601600 	 Percentage complete 90.040
Average episode rewards is 405.757 	Total timesteps: 3609600 	 Percentage complete 90.240
Average episode rewards is 417.966 	Total timesteps: 3609600 	 Percentage complete 90.240
Average episode rewards is 403.038 	Total timesteps: 3617600 	 Percentage complete 90.440
Average episode rewards is 430.462 	Total timesteps: 3617600 	 Percentage complete 90.440
Average episode rewards is 373.238 	Total timesteps: 3625600 	 Percentage complete 90.640
Average episode rewards is 386.621 	Total timesteps: 3625600 	 Percentage complete 90.640
Average episode rewards is 372.923 	Total timesteps: 3633600 	 Percentage complete 90.840
Average episode rewards is 387.008 	Total timesteps: 3633600 	 Percentage complete 90.840
Average episode rewards is 387.126 	Total timesteps: 3641600 	 Percentage complete 91.040
Average episode rewards is 371.950 	Total timesteps: 3641600 	 Percentage complete 91.040
Average episode rewards is 394.604 	Total timesteps: 3649600 	 Percentage complete 91.240
Average episode rewards is 399.438 	Total timesteps: 3649600 	 Percentage complete 91.240
Average episode rewards is 383.021 	Total timesteps: 3657600 	 Percentage complete 91.440
Average episode rewards is 378.066 	Total timesteps: 3657600 	 Percentage complete 91.440
Average episode rewards is 409.649 	Total timesteps: 3665600 	 Percentage complete 91.640
Average episode rewards is 402.729 	Total timesteps: 3665600 	 Percentage complete 91.640
Average episode rewards is 382.550 	Total timesteps: 3673600 	 Percentage complete 91.840
Average episode rewards is 402.469 	Total timesteps: 3673600 	 Percentage complete 91.840
Average episode rewards is 389.865 	Total timesteps: 3681600 	 Percentage complete 92.040
Average episode rewards is 387.147 	Total timesteps: 3681600 	 Percentage complete 92.040
Average episode rewards is 420.332 	Total timesteps: 3689600 	 Percentage complete 92.240
Average episode rewards is 393.207 	Total timesteps: 3689600 	 Percentage complete 92.240
Average episode rewards is 415.877 	Total timesteps: 3697600 	 Percentage complete 92.440
Average episode rewards is 376.847 	Total timesteps: 3697600 	 Percentage complete 92.440
Average episode rewards is 403.607 	Total timesteps: 3705600 	 Percentage complete 92.640
Average episode rewards is 386.489 	Total timesteps: 3705600 	 Percentage complete 92.640
Average episode rewards is 397.234 	Total timesteps: 3713600 	 Percentage complete 92.840
Average episode rewards is 376.579 	Total timesteps: 3713600 	 Percentage complete 92.840
Average episode rewards is 397.392 	Total timesteps: 3721600 	 Percentage complete 93.040
Average episode rewards is 393.951 	Total timesteps: 3721600 	 Percentage complete 93.040
Average episode rewards is 388.940 	Total timesteps: 3729600 	 Percentage complete 93.240
Average episode rewards is 383.410 	Total timesteps: 3729600 	 Percentage complete 93.240
Average episode rewards is 407.017 	Total timesteps: 3737600 	 Percentage complete 93.440
Average episode rewards is 377.393 	Total timesteps: 3737600 	 Percentage complete 93.440
Average episode rewards is 402.801 	Total timesteps: 3745600 	 Percentage complete 93.640
Average episode rewards is 377.987 	Total timesteps: 3745600 	 Percentage complete 93.640
Average episode rewards is 389.531 	Total timesteps: 3753600 	 Percentage complete 93.840
Average episode rewards is 358.678 	Total timesteps: 3753600 	 Percentage complete 93.840
Average episode rewards is 388.430 	Total timesteps: 3761600 	 Percentage complete 94.040
Average episode rewards is 364.032 	Total timesteps: 3761600 	 Percentage complete 94.040
Average episode rewards is 384.983 	Total timesteps: 3769600 	 Percentage complete 94.240
Average episode rewards is 361.290 	Total timesteps: 3769600 	 Percentage complete 94.240
Average episode rewards is 392.519 	Total timesteps: 3777600 	 Percentage complete 94.440
Average episode rewards is 386.364 	Total timesteps: 3777600 	 Percentage complete 94.440
Average episode rewards is 401.536 	Total timesteps: 3785600 	 Percentage complete 94.640
Average episode rewards is 389.275 	Total timesteps: 3785600 	 Percentage complete 94.640
Average episode rewards is 404.155 	Total timesteps: 3793600 	 Percentage complete 94.840
Average episode rewards is 411.939 	Total timesteps: 3793600 	 Percentage complete 94.840
Average episode rewards is 398.231 	Total timesteps: 3801600 	 Percentage complete 95.040
Average episode rewards is 406.828 	Total timesteps: 3801600 	 Percentage complete 95.040
Average episode rewards is 404.873 	Total timesteps: 3809600 	 Percentage complete 95.240
Average episode rewards is 418.424 	Total timesteps: 3809600 	 Percentage complete 95.240
Average episode rewards is 406.435 	Total timesteps: 3817600 	 Percentage complete 95.440
Average episode rewards is 404.368 	Total timesteps: 3817600 	 Percentage complete 95.440
Average episode rewards is 383.358 	Total timesteps: 3825600 	 Percentage complete 95.640
Average episode rewards is 409.969 	Total timesteps: 3825600 	 Percentage complete 95.640
Average episode rewards is 408.955 	Total timesteps: 3833600 	 Percentage complete 95.840
Average episode rewards is 395.605 	Total timesteps: 3833600 	 Percentage complete 95.840
Average episode rewards is 413.106 	Total timesteps: 3841600 	 Percentage complete 96.040
Average episode rewards is 407.509 	Total timesteps: 3841600 	 Percentage complete 96.040
Average episode rewards is 396.883 	Total timesteps: 3849600 	 Percentage complete 96.240
Average episode rewards is 412.664 	Total timesteps: 3849600 	 Percentage complete 96.240
Average episode rewards is 412.459 	Total timesteps: 3857600 	 Percentage complete 96.440
Average episode rewards is 392.886 	Total timesteps: 3857600 	 Percentage complete 96.440
Average episode rewards is 376.908 	Total timesteps: 3865600 	 Percentage complete 96.640
Average episode rewards is 368.800 	Total timesteps: 3865600 	 Percentage complete 96.640
Average episode rewards is 391.647 	Total timesteps: 3873600 	 Percentage complete 96.840
Average episode rewards is 396.645 	Total timesteps: 3873600 	 Percentage complete 96.840
Average episode rewards is 411.087 	Total timesteps: 3881600 	 Percentage complete 97.040
Average episode rewards is 398.418 	Total timesteps: 3881600 	 Percentage complete 97.040
Average episode rewards is 403.071 	Total timesteps: 3889600 	 Percentage complete 97.240
Average episode rewards is 369.507 	Total timesteps: 3889600 	 Percentage complete 97.240
Average episode rewards is 388.918 	Total timesteps: 3897600 	 Percentage complete 97.440
Average episode rewards is 421.391 	Total timesteps: 3897600 	 Percentage complete 97.440
Average episode rewards is 394.757 	Total timesteps: 3905600 	 Percentage complete 97.640
Average episode rewards is 398.711 	Total timesteps: 3905600 	 Percentage complete 97.640
Average episode rewards is 384.186 	Total timesteps: 3913600 	 Percentage complete 97.840
Average episode rewards is 392.960 	Total timesteps: 3913600 	 Percentage complete 97.840
Average episode rewards is 411.448 	Total timesteps: 3921600 	 Percentage complete 98.040
Average episode rewards is 392.903 	Total timesteps: 3921600 	 Percentage complete 98.040
Average episode rewards is 395.083 	Total timesteps: 3929600 	 Percentage complete 98.240
Average episode rewards is 400.540 	Total timesteps: 3929600 	 Percentage complete 98.240
Average episode rewards is 385.113 	Total timesteps: 3937600 	 Percentage complete 98.440
Average episode rewards is 411.679 	Total timesteps: 3937600 	 Percentage complete 98.440
Average episode rewards is 401.569 	Total timesteps: 3945600 	 Percentage complete 98.640
Average episode rewards is 410.298 	Total timesteps: 3945600 	 Percentage complete 98.640
Average episode rewards is 400.931 	Total timesteps: 3953600 	 Percentage complete 98.840
Average episode rewards is 396.393 	Total timesteps: 3953600 	 Percentage complete 98.840
Average episode rewards is 385.115 	Total timesteps: 3961600 	 Percentage complete 99.040
Average episode rewards is 387.860 	Total timesteps: 3961600 	 Percentage complete 99.040
Average episode rewards is 388.912 	Total timesteps: 3969600 	 Percentage complete 99.240
Average episode rewards is 378.139 	Total timesteps: 3969600 	 Percentage complete 99.240
Average episode rewards is 376.520 	Total timesteps: 3977600 	 Percentage complete 99.440
Average episode rewards is 393.113 	Total timesteps: 3977600 	 Percentage complete 99.440
Average episode rewards is 383.731 	Total timesteps: 3985600 	 Percentage complete 99.640
Average episode rewards is 404.555 	Total timesteps: 3985600 	 Percentage complete 99.640
Average episode rewards is 395.447 	Total timesteps: 3993600 	 Percentage complete 99.840
Average episode rewards is 387.893 	Total timesteps: 3993600 	 Percentage complete 99.840
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: \ 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: | 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: / 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                actor_grad_norm â–â–‚â–ƒâ–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–„â–…â–…â–…â–†â–…â–„â–…â–…â–…â–…â–„â–ˆâ–…â–‡
wandb:            agent0/dist_to_goal â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–â–â–â–‚â–â–â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:      agent0/individual_rewards â–â–ƒâ–…â–‡â–‡â–…â–†â–…â–…â–…â–†â–†â–…â–…â–„â–„â–„â–†â–†â–…â–„â–ˆâ–…â–…â–…â–…â–†â–†â–…â–†â–†â–‡â–†â–†â–‡â–‡â–ˆâ–‡â–†â–†
wandb:        agent0/min_time_to_goal â–„â–„â–…â–†â–‡â–ƒâ–ƒâ–…â–„â–‡â–„â–ƒâ–…â–â–„â–…â–†â–…â–†â–†â–ƒâ–ƒâ–ˆâ–†â–ƒâ–…â–…â–‚â–…â–ƒâ–…â–‡â–…â–ƒâ–…â–‡â–‡â–†â–„â–…
wandb:    agent0/num_agent_collisions â–ƒâ–‚â–„â–ƒâ–â–ƒâ–ƒâ–„â–â–…â–‚â–â–‚â–†â–ƒâ–ƒâ–ƒâ–â–…â–†â–ƒâ–…â–ƒâ–ƒâ–…â–…â–ˆâ–‚â–ƒâ–†â–ƒâ–†â–„â–…â–„â–„â–„â–„â–ƒâ–
wandb: agent0/num_obstacle_collisions â–…â–†â–ƒâ–…â–†â–…â–„â–‡â–†â–†â–…â–„â–ƒâ–ƒâ–†â–…â–…â–…â–„â–ƒâ–ƒâ–…â–…â–…â–‡â–…â–â–„â–ˆâ–‡â–‡â–‡â–†â–‚â–†â–„â–ƒâ–…â–†â–‡
wandb:            agent0/time_to_goal â–ˆâ–†â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–‚â–â–‚â–â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:            agent1/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      agent1/individual_rewards â–â–ƒâ–†â–‡â–‡â–…â–†â–…â–‡â–†â–†â–†â–…â–‡â–„â–…â–…â–‡â–‡â–†â–…â–†â–†â–…â–‡â–‡â–†â–‡â–†â–†â–‡â–ˆâ–†â–‡â–‡â–‡â–ˆâ–‡â–‡â–†
wandb:        agent1/min_time_to_goal â–ƒâ–„â–…â–†â–ƒâ–…â–†â–ƒâ–ƒâ–„â–â–‡â–‚â–‡â–‡â–†â–†â–ƒâ–„â–…â–„â–ƒâ–„â–†â–‡â–ˆâ–„â–†â–…â–ƒâ–ƒâ–ˆâ–…â–â–„â–â–†â–ˆâ–ƒâ–†
wandb:    agent1/num_agent_collisions â–‚â–„â–…â–â–ˆâ–…â–…â–ƒâ–‡â–†â–ƒâ–…â–…â–…â–…â–…â–ˆâ–‚â–‡â–‡â–…â–…â–â–ƒâ–„â–…â–ˆâ–„â–ƒâ–ƒâ–…â–‚â–…â–ƒâ–…â–ƒâ–„â–…â–ƒâ–‡
wandb: agent1/num_obstacle_collisions â–‡â–†â–†â–„â–†â–„â–…â–‡â–â–„â–…â–„â–…â–„â–…â–„â–„â–„â–ƒâ–†â–„â–†â–‡â–„â–„â–†â–…â–ƒâ–„â–†â–†â–ƒâ–…â–ˆâ–ƒâ–†â–†â–„â–‚â–„
wandb:            agent1/time_to_goal â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–‚â–‚â–â–‚
wandb:            agent2/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      agent2/individual_rewards â–â–ƒâ–†â–†â–‡â–†â–†â–†â–†â–†â–…â–†â–…â–†â–…â–„â–„â–‡â–†â–†â–…â–‡â–†â–…â–†â–†â–†â–†â–…â–†â–…â–‡â–‡â–†â–†â–ˆâ–‡â–ˆâ–‡â–‡
wandb:        agent2/min_time_to_goal â–‚â–„â–â–†â–ƒâ–†â–…â–‡â–„â–„â–†â–ƒâ–ˆâ–ƒâ–…â–…â–ƒâ–…â–‡â–…â–„â–ƒâ–„â–…â–ˆâ–ƒâ–ƒâ–ˆâ–…â–ƒâ–…â–†â–†â–ƒâ–‡â–„â–†â–†â–…â–…
wandb:    agent2/num_agent_collisions â–„â–ƒâ–…â–„â–„â–‡â–†â–ƒâ–„â–†â–„â–…â–ƒâ–„â–„â–ˆâ–„â–‚â–…â–ƒâ–„â–…â–ƒâ–‚â–„â–ƒâ–ˆâ–„â–â–‡â–„â–ƒâ–â–…â–„â–†â–†â–‚â–†â–„
wandb: agent2/num_obstacle_collisions â–„â–„â–ƒâ–ƒâ–…â–‚â–â–…â–‚â–…â–‚â–„â–„â–‚â–ƒâ–ˆâ–ƒâ–…â–†â–…â–ƒâ–ƒâ–‚â–…â–„â–ƒâ–‡â–…â–‚â–…â–ƒâ–†â–â–ƒâ–â–ƒâ–ƒâ–‚â–ƒâ–‚
wandb:            agent2/time_to_goal â–ˆâ–†â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            agent3/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      agent3/individual_rewards â–â–ƒâ–‡â–‡â–ˆâ–…â–†â–†â–†â–…â–†â–†â–…â–…â–†â–„â–„â–†â–†â–…â–…â–†â–†â–…â–†â–…â–†â–…â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–ˆâ–‡â–†â–‡
wandb:        agent3/min_time_to_goal â–‚â–†â–‡â–…â–…â–â–„â–„â–„â–†â–„â–…â–…â–…â–…â–ƒâ–ˆâ–„â–…â–ƒâ–‡â–„â–ˆâ–ƒâ–†â–‡â–…â–„â–†â–„â–…â–…â–†â–…â–…â–‚â–…â–…â–„â–†
wandb:    agent3/num_agent_collisions â–„â–†â–…â–ƒâ–‚â–…â–„â–„â–†â–†â–„â–ƒâ–‚â–…â–‡â–…â–ˆâ–â–ƒâ–…â–‚â–ƒâ–…â–…â–ƒâ–‡â–ˆâ–â–„â–ƒâ–…â–†â–„â–„â–‚â–ˆâ–‚â–„â–â–ƒ
wandb: agent3/num_obstacle_collisions â–„â–…â–…â–„â–…â–…â–„â–ƒâ–…â–„â–†â–‚â–‚â–ƒâ–…â–…â–ƒâ–ƒâ–„â–‚â–†â–ƒâ–„â–…â–â–‡â–‚â–ˆâ–‚â–‡â–…â–…â–‡â–„â–…â–„â–‚â–…â–‚â–„
wandb:            agent3/time_to_goal â–ˆâ–†â–ƒâ–‚â–‚â–â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–‚â–â–â–â–‚â–â–‚â–â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–‚â–â–‚â–â–â–‚
wandb:            agent4/dist_to_goal â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      agent4/individual_rewards â–â–„â–‡â–‡â–ˆâ–†â–†â–…â–‡â–…â–†â–‡â–…â–†â–†â–…â–…â–…â–‡â–†â–†â–‡â–†â–…â–†â–†â–‡â–†â–†â–†â–…â–‡â–†â–†â–ˆâ–ˆâ–‡â–‡â–‡â–‡
wandb:        agent4/min_time_to_goal â–†â–‚â–‚â–†â–†â–„â–…â–‡â–â–‡â–ˆâ–ƒâ–ƒâ–„â–‡â–…â–…â–‡â–„â–„â–‡â–„â–„â–„â–„â–…â–„â–ƒâ–â–ˆâ–„â–„â–„â–…â–ƒâ–…â–ƒâ–ƒâ–„â–„
wandb:    agent4/num_agent_collisions â–‚â–„â–â–„â–ƒâ–„â–„â–…â–„â–ˆâ–…â–ƒâ–„â–„â–…â–…â–„â–â–ƒâ–ƒâ–…â–â–â–â–‚â–…â–ƒâ–ƒâ–‚â–„â–…â–â–„â–‚â–‚â–…â–…â–…â–ƒâ–…
wandb: agent4/num_obstacle_collisions â–†â–‡â–…â–„â–…â–ˆâ–ƒâ–†â–ˆâ–„â–‚â–„â–ƒâ–ƒâ–„â–‡â–†â–„â–â–ˆâ–‡â–„â–…â–‚â–‚â–†â–„â–‡â–„â–ˆâ–„â–ƒâ–„â–ƒâ–„â–‚â–†â–„â–†â–„
wandb:            agent4/time_to_goal â–ˆâ–†â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–‚â–â–â–‚â–â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:            agent5/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      agent5/individual_rewards â–â–„â–†â–ˆâ–‡â–†â–†â–†â–‡â–†â–†â–‡â–…â–…â–…â–…â–…â–‡â–†â–‡â–…â–†â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡
wandb:        agent5/min_time_to_goal â–„â–â–ƒâ–…â–„â–â–ƒâ–ƒâ–„â–‡â–ˆâ–…â–„â–†â–†â–†â–„â–ƒâ–†â–…â–ƒâ–†â–„â–…â–ƒâ–â–ƒâ–…â–„â–„â–ƒâ–‡â–…â–†â–‡â–„â–‚â–‡â–†â–ƒ
wandb:    agent5/num_agent_collisions â–ˆâ–ƒâ–ƒâ–â–ƒâ–‚â–ƒâ–„â–ƒâ–„â–…â–‡â–ƒâ–†â–‚â–ˆâ–…â–â–â–…â–ƒâ–„â–ƒâ–ƒâ–„â–„â–†â–…â–ƒâ–ƒâ–„â–„â–…â–„â–ƒâ–‚â–…â–„â–†â–
wandb: agent5/num_obstacle_collisions â–ˆâ–â–ƒâ–ƒâ–„â–„â–ƒâ–…â–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–„â–ƒâ–…â–â–ƒâ–‚â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–†â–ƒâ–‚â–‚â–â–…â–„â–†â–…â–ƒâ–ƒâ–‚â–„â–ƒ
wandb:            agent5/time_to_goal â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–
wandb:            agent6/dist_to_goal â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:      agent6/individual_rewards â–â–‚â–‡â–ˆâ–‡â–†â–†â–†â–†â–‡â–†â–†â–†â–†â–†â–…â–„â–†â–‡â–†â–…â–‡â–†â–…â–‡â–†â–†â–†â–†â–‡â–†â–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡
wandb:        agent6/min_time_to_goal â–„â–ˆâ–†â–†â–‚â–†â–…â–…â–…â–ƒâ–…â–„â–…â–‡â–„â–…â–†â–†â–…â–…â–â–ˆâ–†â–ƒâ–„â–‡â–„â–‡â–…â–…â–ˆâ–…â–…â–†â–„â–†â–…â–…â–…â–„
wandb:    agent6/num_agent_collisions â–†â–†â–†â–„â–ƒâ–†â–…â–†â–ƒâ–…â–ƒâ–†â–‚â–ˆâ–„â–‡â–†â–„â–ƒâ–ƒâ–ˆâ–â–ƒâ–ƒâ–‚â–…â–„â–„â–‚â–„â–…â–„â–…â–ƒâ–ƒâ–„â–…â–…â–ƒâ–…
wandb: agent6/num_obstacle_collisions â–…â–…â–„â–„â–ƒâ–ƒâ–„â–ƒâ–‚â–…â–†â–…â–ƒâ–…â–„â–ƒâ–‡â–…â–ƒâ–ƒâ–…â–†â–ƒâ–ƒâ–„â–†â–„â–…â–â–ˆâ–†â–ƒâ–…â–†â–„â–†â–†â–…â–‚â–‡
wandb:            agent6/time_to_goal â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–‚â–â–‚â–â–â–‚â–â–‚â–‚â–â–‚â–â–â–â–
wandb:        average_episode_rewards â–â–ƒâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:               critic_grad_norm â–ˆâ–„â–‚â–ƒâ–â–ƒâ–â–â–‚â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                   dist_entropy â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    policy_loss â–ˆâ–ƒâ–â–‚â–‚â–ƒâ–…â–…â–…â–…â–…â–†â–†â–…â–…â–†â–…â–†â–‡â–†â–…â–†â–…â–„â–†â–„â–ˆâ–†â–†â–‡â–†â–†â–„â–„â–‡â–†â–†â–†â–†â–†
wandb:                          ratio â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–„â–ˆâ–‚â–„â–ƒâ–â–‚â–â–â–â–â–ƒâ–‚â–‚â–‚â–â–â–â–‚â–‚â–â–‚â–‚â–ƒâ–â–
wandb:                     value_loss â–…â–…â–‡â–ˆâ–…â–„â–ƒâ–ƒâ–…â–ƒâ–ƒâ–„â–„â–ƒâ–‚â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:                actor_grad_norm 0.64949
wandb:            agent0/dist_to_goal 0.05367
wandb:      agent0/individual_rewards 2.85459
wandb:        agent0/min_time_to_goal 0.46584
wandb:    agent0/num_agent_collisions 0.89062
wandb: agent0/num_obstacle_collisions 0.29688
wandb:            agent0/time_to_goal 0.97813
wandb:            agent1/dist_to_goal 0.04627
wandb:      agent1/individual_rewards 3.17643
wandb:        agent1/min_time_to_goal 0.42629
wandb:    agent1/num_agent_collisions 0.84375
wandb: agent1/num_obstacle_collisions 0.35938
wandb:            agent1/time_to_goal 0.90313
wandb:            agent2/dist_to_goal 0.04763
wandb:      agent2/individual_rewards 3.33148
wandb:        agent2/min_time_to_goal 0.51584
wandb:    agent2/num_agent_collisions 0.82812
wandb: agent2/num_obstacle_collisions 0.34375
wandb:            agent2/time_to_goal 1.02656
wandb:            agent3/dist_to_goal 0.04621
wandb:      agent3/individual_rewards 3.56891
wandb:        agent3/min_time_to_goal 0.47053
wandb:    agent3/num_agent_collisions 0.53125
wandb: agent3/num_obstacle_collisions 0.39062
wandb:            agent3/time_to_goal 0.95938
wandb:            agent4/dist_to_goal 0.04329
wandb:      agent4/individual_rewards 3.96307
wandb:        agent4/min_time_to_goal 0.49508
wandb:    agent4/num_agent_collisions 0.64062
wandb: agent4/num_obstacle_collisions 0.3125
wandb:            agent4/time_to_goal 1.01719
wandb:            agent5/dist_to_goal 0.05624
wandb:      agent5/individual_rewards 2.92781
wandb:        agent5/min_time_to_goal 0.44066
wandb:    agent5/num_agent_collisions 0.78125
wandb: agent5/num_obstacle_collisions 0.26562
wandb:            agent5/time_to_goal 0.93906
wandb:            agent6/dist_to_goal 0.05435
wandb:      agent6/individual_rewards 2.69646
wandb:        agent6/min_time_to_goal 0.43664
wandb:    agent6/num_agent_collisions 0.82812
wandb: agent6/num_obstacle_collisions 0.3125
wandb:            agent6/time_to_goal 0.89375
wandb:        average_episode_rewards 395.44656
wandb:               critic_grad_norm 0.05585
wandb:                   dist_entropy 0.33199
wandb:                    policy_loss -0.00478
wandb:                          ratio 0.99635
wandb:                     value_loss 0.05154
wandb: 
wandb: ðŸš€ View run rmappo_informarl_seed200 at: https://wandb.ai/golde/enemy/runs/9egtea5h
wandb: ï¸âš¡ View job at https://wandb.ai/golde/enemy/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjM2MjUwMjIwOQ==/version_details/v2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)
wandb: Find logs at: ./onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240812_083746-9egtea5h/logs
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)wandb: \ 0.009 MB of 0.459 MB uploaded (0.000 MB deduped)wandb: | 0.459 MB of 0.459 MB uploaded (0.000 MB deduped)wandb: / 0.459 MB of 0.459 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                actor_grad_norm â–â–…â–ˆâ–†â–…â–„â–‡â–„â–…â–†â–…â–…â–†â–…â–…â–†â–‡â–…â–†â–†â–‡â–†â–†â–…â–†â–‡â–‡â–‡â–…â–†â–†â–†â–†â–‡â–†â–ˆâ–ˆâ–‡â–‡â–‡
wandb:            agent0/dist_to_goal â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–‚â–â–‚â–â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:      agent0/individual_rewards â–â–‡â–ˆâ–‡â–†â–…â–†â–†â–†â–†â–…â–†â–†â–†â–„â–†â–…â–†â–†â–…â–†â–†â–…â–…â–‡â–†â–‡â–…â–‡â–‡â–‡â–†â–…â–…â–†â–ˆâ–ˆâ–†â–‡â–‡
wandb:        agent0/min_time_to_goal â–ƒâ–â–ˆâ–ˆâ–ƒâ–„â–†â–„â–„â–„â–‚â–„â–…â–†â–ˆâ–ƒâ–…â–†â–„â–†â–ƒâ–ƒâ–…â–†â–ƒâ–„â–ƒâ–ƒâ–†â–‚â–„â–…â–ƒâ–‡â–…â–ƒâ–ƒâ–„â–‚â–‡
wandb:    agent0/num_agent_collisions â–ƒâ–‡â–…â–„â–ˆâ–„â–‡â–…â–†â–…â–„â–†â–„â–„â–†â–…â–†â–ƒâ–…â–ƒâ–„â–„â–„â–„â–ƒâ–†â–ƒâ–‚â–†â–‚â–ƒâ–…â–…â–…â–„â–‚â–â–ƒâ–…â–…
wandb: agent0/num_obstacle_collisions â–…â–„â–…â–„â–…â–â–ƒâ–ƒâ–„â–ƒâ–â–‡â–ƒâ–ˆâ–…â–ƒâ–…â–†â–…â–†â–‚â–ƒâ–ƒâ–…â–„â–…â–‡â–…â–„â–ƒâ–ƒâ–„â–â–‚â–â–ƒâ–ˆâ–‚â–ƒâ–ƒ
wandb:            agent0/time_to_goal â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–‚
wandb:            agent1/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–â–â–‚â–‚â–â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:      agent1/individual_rewards â–â–…â–‡â–‡â–†â–†â–…â–†â–†â–†â–†â–…â–…â–…â–„â–†â–…â–…â–…â–…â–†â–†â–†â–…â–†â–‡â–†â–†â–‡â–‡â–†â–†â–†â–†â–†â–ˆâ–‡â–…â–†â–‡
wandb:        agent1/min_time_to_goal â–ƒâ–…â–…â–„â–ƒâ–‚â–†â–…â–„â–„â–â–â–…â–…â–„â–ˆâ–‚â–â–â–‚â–â–…â–„â–‚â–„â–‚â–„â–†â–†â–…â–ƒâ–…â–‚â–†â–„â–‚â–„â–…â–ƒâ–…
wandb:    agent1/num_agent_collisions â–‚â–â–‚â–…â–‚â–„â–ƒâ–†â–…â–†â–‡â–†â–ƒâ–ƒâ–ˆâ–†â–„â–ƒâ–„â–„â–„â–‡â–…â–†â–‚â–ƒâ–‚â–„â–…â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–„â–‡â–†â–‚
wandb: agent1/num_obstacle_collisions â–„â–â–â–…â–„â–‡â–…â–„â–„â–ˆâ–‚â–„â–†â–„â–„â–…â–…â–„â–†â–…â–„â–ƒâ–‚â–…â–ƒâ–â–‚â–‚â–„â–‚â–ƒâ–ƒâ–„â–‚â–„â–…â–‚â–…â–ƒâ–…
wandb:            agent1/time_to_goal â–ˆâ–…â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–‚
wandb:            agent2/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–‚â–â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:      agent2/individual_rewards â–â–…â–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–†â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–ˆâ–†â–‡â–‡â–†â–†â–†â–„â–†â–ˆâ–‡â–‡â–‡â–‡
wandb:        agent2/min_time_to_goal â–ƒâ–†â–ƒâ–„â–â–ƒâ–‚â–…â–…â–„â–„â–â–„â–ƒâ–ƒâ–„â–†â–…â–…â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ˆâ–…â–„â–ƒâ–„â–†â–ƒâ–ƒâ–ƒâ–†â–ƒâ–‚â–…â–ƒâ–ƒ
wandb:    agent2/num_agent_collisions â–â–†â–ƒâ–„â–‡â–‚â–†â–‡â–ˆâ–…â–ˆâ–ƒâ–„â–†â–…â–‚â–…â–…â–„â–‡â–‡â–…â–…â–†â–…â–‡â–…â–…â–„â–†â–ƒâ–ƒâ–ƒâ–ƒâ–‡â–…â–ƒâ–ƒâ–‡â–ƒ
wandb: agent2/num_obstacle_collisions â–†â–‡â–‡â–‡â–†â–†â–„â–…â–‚â–†â–…â–„â–„â–ˆâ–†â–„â–„â–„â–ƒâ–‚â–ƒâ–ƒâ–†â–„â–…â–„â–‚â–ˆâ–ƒâ–‚â–…â–†â–…â–„â–†â–‚â–…â–â–‡â–
wandb:            agent2/time_to_goal â–ˆâ–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–‚â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–
wandb:            agent3/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–â–â–‚â–‚â–â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:      agent3/individual_rewards â–â–…â–‡â–‡â–†â–†â–†â–‡â–…â–…â–†â–†â–…â–…â–„â–„â–…â–„â–†â–„â–†â–…â–†â–…â–…â–‡â–†â–…â–‡â–‡â–…â–‡â–‡â–…â–†â–ˆâ–‡â–†â–‡â–†
wandb:        agent3/min_time_to_goal â–â–…â–„â–ƒâ–†â–ƒâ–ƒâ–„â–…â–†â–„â–†â–†â–‡â–†â–ƒâ–…â–…â–„â–ƒâ–…â–ƒâ–…â–ƒâ–†â–‚â–â–‡â–…â–ƒâ–…â–„â–…â–‚â–ƒâ–‚â–…â–„â–ˆâ–‚
wandb:    agent3/num_agent_collisions â–â–…â–ƒâ–ˆâ–‡â–„â–ƒâ–â–ƒâ–†â–†â–„â–‡â–„â–…â–†â–„â–ƒâ–‚â–„â–‡â–ƒâ–…â–„â–…â–†â–„â–ƒâ–‡â–„â–„â–„â–…â–†â–‚â–ƒâ–ƒâ–†â–†â–„
wandb: agent3/num_obstacle_collisions â–ƒâ–…â–ƒâ–ƒâ–„â–‚â–†â–‡â–‚â–„â–…â–‡â–†â–†â–„â–‡â–‡â–‡â–ƒâ–†â–†â–„â–„â–„â–†â–ƒâ–ƒâ–â–…â–ƒâ–ˆâ–„â–ƒâ–â–…â–‚â–…â–„â–†â–ƒ
wandb:            agent3/time_to_goal â–ˆâ–…â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–‚â–â–‚â–â–â–‚â–‚â–â–‚â–â–â–â–â–â–‚â–‚â–‚â–
wandb:            agent4/dist_to_goal â–ˆâ–‚â–â–â–â–‚â–â–â–â–‚â–â–â–‚â–â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:      agent4/individual_rewards â–â–†â–‡â–‡â–†â–†â–†â–‡â–…â–„â–†â–†â–†â–†â–„â–…â–„â–†â–…â–…â–…â–„â–†â–…â–†â–†â–†â–†â–‡â–‡â–†â–‡â–‡â–…â–‡â–ˆâ–‡â–‡â–ˆâ–‡
wandb:        agent4/min_time_to_goal â–‡â–‚â–†â–„â–…â–‚â–…â–†â–‚â–ƒâ–ˆâ–‚â–‡â–†â–ƒâ–†â–ƒâ–…â–ƒâ–†â–…â–‚â–…â–†â–…â–„â–…â–…â–„â–ƒâ–ƒâ–‚â–„â–„â–†â–â–†â–ƒâ–†â–
wandb:    agent4/num_agent_collisions â–‚â–„â–ƒâ–†â–‡â–‚â–†â–‡â–…â–…â–…â–‚â–…â–‚â–…â–‡â–…â–„â–„â–ˆâ–‡â–ƒâ–ƒâ–‡â–ƒâ–…â–ƒâ–†â–‡â–…â–‚â–ƒâ–ƒâ–ƒâ–„â–‚â–â–ˆâ–„â–‚
wandb: agent4/num_obstacle_collisions â–‡â–„â–ƒâ–…â–„â–„â–…â–‡â–ƒâ–…â–ƒâ–…â–‚â–‡â–…â–ƒâ–…â–†â–ˆâ–…â–„â–…â–…â–†â–…â–‡â–‡â–ƒâ–‚â–†â–ƒâ–†â–â–…â–„â–ƒâ–„â–…â–†â–…
wandb:            agent4/time_to_goal â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–‚â–â–‚â–â–‚â–
wandb:            agent5/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:      agent5/individual_rewards â–â–…â–†â–ˆâ–†â–†â–…â–†â–…â–„â–†â–†â–…â–†â–„â–…â–„â–…â–…â–…â–…â–„â–…â–…â–…â–‡â–‡â–†â–‡â–†â–†â–†â–†â–…â–†â–ˆâ–‡â–‡â–‡â–†
wandb:        agent5/min_time_to_goal â–ƒâ–‚â–‚â–‡â–‚â–ƒâ–…â–…â–ƒâ–ƒâ–„â–ƒâ–‚â–…â–‚â–‚â–ˆâ–‚â–ƒâ–â–„â–‚â–…â–„â–†â–…â–†â–ƒâ–„â–…â–ƒâ–…â–„â–ƒâ–ƒâ–…â–…â–ƒâ–†â–
wandb:    agent5/num_agent_collisions â–ƒâ–‡â–ƒâ–‡â–„â–„â–…â–ˆâ–ˆâ–…â–‡â–„â–…â–„â–†â–ƒâ–…â–†â–†â–ˆâ–†â–„â–…â–…â–…â–„â–†â–â–†â–ƒâ–â–„â–„â–„â–…â–ƒâ–„â–†â–…â–ƒ
wandb: agent5/num_obstacle_collisions â–ƒâ–„â–„â–„â–ƒâ–‚â–…â–…â–†â–ƒâ–‚â–‚â–†â–„â–†â–†â–â–ˆâ–…â–ƒâ–…â–„â–†â–…â–„â–‡â–ƒâ–‚â–„â–…â–‚â–…â–‡â–†â–„â–ƒâ–„â–„â–…â–„
wandb:            agent5/time_to_goal â–ˆâ–„â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–
wandb:            agent6/dist_to_goal â–ˆâ–‚â–â–â–â–â–â–â–‚â–‚â–â–â–â–‚â–‚â–â–‚â–â–‚â–‚â–â–‚â–â–‚â–‚â–â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:      agent6/individual_rewards â–â–…â–‡â–ˆâ–‡â–†â–…â–…â–„â–†â–†â–†â–…â–„â–„â–…â–…â–†â–…â–„â–‡â–„â–…â–…â–†â–‡â–†â–…â–†â–‡â–‡â–†â–†â–…â–†â–ˆâ–‡â–…â–‡â–‡
wandb:        agent6/min_time_to_goal â–‚â–‚â–‚â–ƒâ–…â–„â–†â–ƒâ–„â–ƒâ–…â–…â–‡â–…â–‚â–…â–ˆâ–â–‚â–â–ƒâ–â–…â–ƒâ–ƒâ–ƒâ–‡â–‚â–„â–ƒâ–‚â–‚â–„â–„â–ƒâ–ƒâ–ƒâ–†â–ˆâ–‚
wandb:    agent6/num_agent_collisions â–â–„â–ƒâ–„â–ƒâ–ƒâ–ˆâ–†â–„â–„â–„â–…â–‡â–…â–„â–…â–‡â–„â–„â–„â–†â–†â–…â–„â–‡â–„â–‚â–‚â–ˆâ–‚â–ƒâ–…â–†â–„â–…â–ƒâ–‚â–…â–†â–„
wandb: agent6/num_obstacle_collisions â–…â–„â–„â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–„â–„â–‚â–„â–ƒâ–‚â–…â–„â–…â–…â–„â–„â–‚â–…â–ˆâ–ƒâ–„â–†â–â–…â–†â–ƒâ–…â–‡â–‚â–…â–…â–â–‚â–ƒâ–„
wandb:            agent6/time_to_goal â–ˆâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–‚â–‚â–
wandb:        average_episode_rewards â–â–„â–†â–†â–†â–‡â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆ
wandb:               critic_grad_norm â–ˆâ–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                   dist_entropy â–ˆâ–„â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                    policy_loss â–ˆâ–â–â–‚â–„â–„â–…â–†â–†â–†â–‡â–‡â–†â–†â–†â–†â–‡â–‡â–†â–ˆâ–‡â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–ˆâ–ˆâ–‡â–†â–†
wandb:                          ratio â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–…â–ƒâ–„â–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–‚â–…â–â–ˆâ–ƒâ–‚â–„â–â–‚â–‚â–„â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–â–ƒâ–â–ƒ
wandb:                     value_loss â–ƒâ–ˆâ–ˆâ–„â–ƒâ–„â–ƒâ–ƒâ–„â–„â–…â–„â–„â–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–
wandb: 
wandb: Run summary:
wandb:                actor_grad_norm 0.51879
wandb:            agent0/dist_to_goal 0.0461
wandb:      agent0/individual_rewards 2.78619
wandb:        agent0/min_time_to_goal 0.44649
wandb:    agent0/num_agent_collisions 1.28125
wandb: agent0/num_obstacle_collisions 0.29688
wandb:            agent0/time_to_goal 0.93125
wandb:            agent1/dist_to_goal 0.04012
wandb:      agent1/individual_rewards 3.4172
wandb:        agent1/min_time_to_goal 0.48968
wandb:    agent1/num_agent_collisions 0.96875
wandb: agent1/num_obstacle_collisions 0.26562
wandb:            agent1/time_to_goal 1.01562
wandb:            agent2/dist_to_goal 0.04483
wandb:      agent2/individual_rewards 2.78521
wandb:        agent2/min_time_to_goal 0.46467
wandb:    agent2/num_agent_collisions 1.125
wandb: agent2/num_obstacle_collisions 0.42188
wandb:            agent2/time_to_goal 0.93594
wandb:            agent3/dist_to_goal 0.04201
wandb:      agent3/individual_rewards 3.25849
wandb:        agent3/min_time_to_goal 0.45388
wandb:    agent3/num_agent_collisions 1.03125
wandb: agent3/num_obstacle_collisions 0.17188
wandb:            agent3/time_to_goal 0.91094
wandb:            agent4/dist_to_goal 0.04785
wandb:      agent4/individual_rewards 2.85957
wandb:        agent4/min_time_to_goal 0.45277
wandb:    agent4/num_agent_collisions 0.82812
wandb: agent4/num_obstacle_collisions 0.54688
wandb:            agent4/time_to_goal 0.92344
wandb:            agent5/dist_to_goal 0.04662
wandb:      agent5/individual_rewards 2.86235
wandb:        agent5/min_time_to_goal 0.45825
wandb:    agent5/num_agent_collisions 0.875
wandb: agent5/num_obstacle_collisions 0.17188
wandb:            agent5/time_to_goal 0.92813
wandb:            agent6/dist_to_goal 0.04465
wandb:      agent6/individual_rewards 3.41443
wandb:        agent6/min_time_to_goal 0.52142
wandb:    agent6/num_agent_collisions 1.01562
wandb: agent6/num_obstacle_collisions 0.25
wandb:            agent6/time_to_goal 1.02188
wandb:        average_episode_rewards 387.89279
wandb:               critic_grad_norm 0.08466
wandb:                   dist_entropy 0.36246
wandb:                    policy_loss -0.00491
wandb:                          ratio 1.00171
wandb:                     value_loss 0.05107
wandb: 
wandb: ðŸš€ View run rmappo_informarl_seed400 at: https://wandb.ai/golde/enemy/runs/fama9fwe
wandb: ï¸âš¡ View job at https://wandb.ai/golde/enemy/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjM2MjUwMjIwOQ==/version_details/v2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)
wandb: Find logs at: ./onpolicy/results/GraphMPE/navigation_graph/rmappo/informarl/wandb/run-20240812_083855-fama9fwe/logs
